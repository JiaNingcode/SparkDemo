                      
本文总共分为九章，分别为java 、数据库、hadoop相关、hive、数仓、spark相关、kafka、flink、数据结构与算法。可能有的题目模棱两可，所以难免有分类不严格之处，希望大家把更多精力放在题目本身之上，也希望大家提出更多宝贵意见，我们第一时间择优采纳。

本文很多题目都是主观题，所以并无统一标准答案，同时我们尽力提供参考答案，以供大家参考，如有更好的答案，可整理并讨论。

本文更多偏向于笔试题或者编程题，不排除可能有场景题，如果需要场景题，参考“千锋-大数据场景练习.pdf",建议准备笔试或者编程的同学查阅准备。

最后非常感谢参与该文档编写、更新、矫正等贡献的所有老师和同学！！！



编写规则:

1、题目必须是文字或者图片。

2、参考答案必须在代码块中，如参考答案是图片，也在代码块中注明答案如下图，如果图片嵌套在答案中，则先代码块，再图片，再代码块等。

3、题目需要标出难度等级，5*标识。

4、图片必须是相对路径、标题必须严格按照规定编写，同时图片名称统一按照章节编号拼接0001来递增即可，如：第三章第5张图片名称为30005。

5、每个章节下应该配有说明，说明该章节具体搜集的题目范围，总体难度、重要程度等信息。

6、每个大章节或者小节下可以具体再划分，划分适合即可。

7、建议丰富hive、数仓、spark、kafka等相关章节题目。

8、答案不能使用markdown的分级语法，比如答案带5级目录。

9、可以使用不同颜色或者字体标注答案中的核心或者要点等信息。



**目录**

[TOC]



#  第一章	java  #
### 1、java底层 hashmap扩容怎么实现 hashtable和currenthashmap的原理 ###

```
##### 第一:java底层 hashmap扩容怎么实现

答:可是当哈希表接近装满时,因为数组的扩容问题,性能较低(转移到更大的哈希表中).

Java默认的散列单元大小全部都是2的幂，初始值为16（2的4次幂）。假如16条链表中的75%链接有数据的时候，则认为加载因子达到默认的0.75。HahSet开始重新散列，也就是将原来的散列结构全部抛弃，重新开辟一个散列单元大小为32（2的5次幂）的散列结果，并重新计算各个数据的存储位置。以此类推下去.....

负载(加载)因子:0.75.-->hash表提供的空间是16 也就是说当到达12的时候就扩容

##### 第二:hashtable和currenthashmap的原理

答:HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下，HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。

###### hashtable实现:

底层数组+链表实现，无论key还是value都**不能为null**，线程**安全**，实现线程安全的方式是在修改数据时锁住整个HashTable，效率低，ConcurrentHashMap做了相关优化

初始size为**11**，扩容：newsize = olesize*2+1

计算index的方法：index = (hash & 0x7FFFFFFF) % tab.length

###### Java5提供了ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。concurrentHashMap的原理:

底层采用分段的数组+链表实现，线程**安全**

通过把整个Map分为N个Segment，可以提供相同的线程安全，但是效率提升N倍，默认提升16倍。(读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。)

Hashtable的synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术

有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁

扩容：段内扩容（段内元素超过该段对应Entry数组长度的75%触发扩容，不会对整个Map进行扩容），插入前检测需不需要扩容，有效避免无效扩容

###### concurrentMap和hashtable比较:

ConcurrentHashMap是使用了锁分段技术来保证线程安全的。

**锁分段技术**：首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 

ConcurrentHashMap提供了与Hashtable和SynchronizedMap不同的锁机制。Hashtable中采用的锁机制是一次锁住整个hash表，从而在同一时刻只能由一个线程对其进行操作；而ConcurrentHashMap中则是一次锁住一个桶。

ConcurrentHashMap默认将hash表分为16个桶，诸如get、put、remove等常用操作只锁住当前需要用到的桶。这样，原来只能一个线程进入，现在却能同时有16个写线程执行，并发性能的提升是显而易见的。
```

### 2、nio和bio的区别 为啥nio好 ###

```
同步阻塞IO（JAVA BIO/Blocking IO ）：  同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销.

Java NIO(Non-Blocking IO ) ： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。NIO的优点在于首先基于缓存读写文件，能够批量操作，然后用channel双向读写数据，减少每次打开断开流的资源消耗。引入selecore的概念，用一个线程管理多个通道，大大减少线程开销。

Java AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理，NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持I/O属于底层操作，需要操作系统支持，并发也需要操作系统的支持，所以性能方面不同操作系统差异会比较明显。另外NIO的非阻塞，需要一直轮询，也是一个比较耗资源的。所以出现AIO
```



### 3、网络如何通信 ###

```
这个问题回答起来比较复杂.设计到硬件和软件的相关知识.

所有的一切,都依赖于一套网络协议,协议是通信双方约定好的通信法则.计算机也要遵循协议,来实现计算机的通信.计算机的协议从低到高分成多层,在底层,两台计算机只能通过0或1的二进制信号通话.信息在向高层协议翻译的过程,信息越来越容易被人理解.

从底层到高层依次是物理层,数据链路层,网络层,传输层,会话层,表示层,应用层.在每一层上都有主要的协议.我们通过这些协议就可以实现通信.比如:

我们常见的IP协议在网络层,TCP,UDP在传输层,http,ftp,smtp等在应用层.应用层是离我们最近的层,实现与其它计算机进行通讯的一个应用，它是对应应用程序的通信服务的.
```



### 4、threadlocal原理 ###

```
ThreadLocal就是一种以**空间换时间**的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了

4.1实际通过ThreadLocal创建的副本是存储在每个线程自己的threadLocals中的；

4.2.为何threadLocals的类型ThreadLocalMap的键值为ThreadLocal对象，因为每个线程中可有多个threadLocal变量；

4.3.在进行get之前，必须先set，否则会报空指针异常；因为在上面的代码分析过程中，我们发现如果没有先set的话，即在map中查找不到对应的存储，则会通过调用setInitialValue方法返回i，而在setInitialValue方法中，有一个语句是T value = initialValue()， 而默认情况下，initialValue方法返回的是null。

4.4 如果想在get之前不需要调用set就能正常访问的话，必须重写initialValue()方法。　
```



### 5、arrayList和LinkedList的区别 ###

```
Arraylist：底层是基于动态数组，根据下表随机访问数组元素的效率高，向数组尾部添加元素的效率高；但是，删除数组中的数据以及向数组中间添加数据效率低，因为需要移动数组。

Linkedlist基于链表的动态数组，数据添加删除效率高，只需要改变指针指向即可，但是访问数据的平均效率低，需要对链表进行遍历。

总结：对于随机访问get和set，ArrayList优于LinkedList，因为LinkedList要移动指针。  对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。
```



### 6、单利模式是什么，线程安全吗 ###

```
单例是java中一种典型的设计模式,定义在功能实现时一个类只能有一个对象,建立一个全局的访问点提供出去供大家使用.也就是说通过单例我们可以实现数据的全局访问,还可以再全局实现功能的调用.单例分成懒汉式和饿汉式,对于懒汉式会有线程安全问题,需要进行同步处理,对于饿汉式不会有线程安全问题,不需要同步.
```



### 7、Vector是什么，线程安全吗？一直是安全的吗 ###

```
Vector 可实现自动增长的对象数组。 java.util.vector提供了向量类(Vector)以实现类似动态数组的功能。 创建了一个向量类的对象后，可以往其中随意插入不同类的对象，即不需顾及类型也不需预先选定向量的容量，并可以方便地进行查找。对于预先不知或者不愿预先定义数组大小，并且需要频繁地进行查找，插入，删除工作的情况，可以考虑使用向量类。

Vector是Collection中List的一种,vector的单个操作时原子性的，也就是线程安全的。但是如果两个原子操作复合而来，这个组合的方法是非线程安全的，需要使用锁来保证线程安全。
```



### 8、Synchronized和lock说一下区别 ###

```
1.首先synchronized是java内置关键字，在jvm层面，Lock是个java接口；

2.synchronized无法判断是否获取锁的状态，Lock可以判断是否获取到锁；

3.synchronized会自动释放锁(a 线程执行完同步代码会释放锁 ；b 线程执行过程中发生异常会释放锁)，Lock需在finally中手工释放锁（unlock()方法释放锁），否则容易造成线程死锁；

4.synchronized需要使用Object的wait,notify等方法实现唤醒等待,而lock通过面向对象的Condition实现.

5.synchronized的锁可重入、不可中断、非公平，而Lock锁可重入、可判断、可公平（两者皆可）

6.Lock锁适合大量同步的代码的同步问题，synchronized锁适合代码少量的同步问题。

7.Lock被称为显式锁,出现在jdk1.5,,synchronized称为隐式锁,出现在jdk1.0,lock的效率更高.
```



### 9、ConcurrentHashMap了解吗 ###

```
首先Map是接口，一般而言concurrentHashMap是线程安全的，具体实现 

在1.7采取的segment分段锁，有点类似于16个线程安全的hashtable组合成了一个concurrenthashmap，不同分段操作不需要上锁，同一个分段才需要上锁，读不上锁，写上锁。锁的粒度更加精细,而在1.8中而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap,而原有的Segment的数据结构虽保留了，但是已经简化了属性，只是为了兼容旧版本.
```



### 10、说一下你们堆外内存和堆内存是如何分配的？ ###

```
堆内内存（on-heap memory）完全遵守JVM虚拟机的内存管理机制，堆内内存 = 新生代+老年代+持久代,我们采用垃圾回收器（GC）统一进行内存管理，平时GC会去频繁的回收新生代的对象,也就是minor GC.然后GC会在某些特定的时间点进行一次彻底回收，也就是Full GC，GC会对所有分配的堆内内存进行扫描，在这个过程中会对JAVA应用程序的性能造成一定影响，还可能会产生Stop The World。我们在jvm参数中只要使用-Xms，-Xmx等参数就可以设置堆的大小和最大值,

和堆内内存相对应，堆外内存就是把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够在一定程度上减少垃圾回收对应用程序造成的影响。

我们经常用java.nio.DirectByteBuffer对象进行堆外内存的管理和使用，它会在对象创建的时候就分配堆外内存.DirectByteBuffer类是在Java Heap外分配内存，对堆外内存的申请主要是通过成员变量unsafe来操作
```



### 11、常见的gc策略了解吗？有哪些gc策略？你们的gc策略是什么？说一下你们的gc策略的实现？（让我说英文名字） ###

```
GC是分代收集算法,频繁收集Young区,较少收集Old区,基本不动Perm区 ,JVM在进行GC时，并非每次都对上面三个内存区域一起回收的，大部分时候回收的都是指新生代。 因此GC按照回 收的区域又分了两种类型，一种是普通GC(minor GC)，一种是全局GC(major GC or Full GC)， 普通 GC(minor GC):只针对新生代区域的GC。 全局GC(major GC or Full GC):针对年老代的GC，偶尔伴随对 新生代的GC以及对永久代的GC。 

GC常用算法 1.引用计数法(了解) 2.复制算法(Copying) 3.标记清除(Mark-Sweep) 4.标记压缩(Mark-Compact) 5.标记清除压缩(Mark-Sweep-Compact) 

算法没有最好的,只能找最合适的,我们使用的是分代收集算法(相对联合的应用)

年轻代(Young Gen)
 年轻代特点是区域相对老年代较小，对像存活率低。 

这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对像大小有关，因而很适用于年轻代 的回收。而复制算法内存利用率不高的问题，通过hotspot中的两个survivor的设计得到缓解。 

老年代(Tenure Gen) 老年代的特点是区域较大，对像存活率高。 

这种情况，存在大量存活率高的对像，复制算法明显变得不合适。一般是由标记清除或者是标记清除与标记整理的
混合实现。

Mark阶段的开销与存活对像的数量成正比，这点上说来，对于老年代，标记清除或者标记整理有一些不符，但可 以通过多核/线程利用，对并发、并行的形式提标记效率。 

Sweep阶段的开销与所管理区域的大小形正相关，但Sweep“就地处决”的特点，回收的过程没有对像的移动。使其 相对其它有对像移动步骤的回收算法，仍然是效率最好的。但是需要解决内存碎片问题。 

Compact阶段的开销与存活对像的数据成开比，如上一条所描述，对于大量对像的移动是很大开销的，做为老年代 的第一选择并不合适。 

基于上面的考虑，老年代一般是由标记清除或者是标记清除与标记整理的混合实现。以hotspot中的CMS回收器为 例，CMS是基于Mark-Sweep实现的，对于对像的回收效率很高，而对于碎片问题，CMS采用基于Mark-Compact 算法的Serial Old回收器做为补偿措施:当内存回收不佳(碎片导致的Concurrent Mode Failure时)，将采用 Serial Old执行Full GC以达到对老年代内存的整理。 
```



### 12、多线程了解吗？线程池呢？有几种线程池？线程池的构造方法的参数有哪几个？ ###

```
进程process是操作系统中运行的一个任务，占有一定的内存资源；线程thread是进程中包含的一个或多个执行单元，归属于进程,多线程就是在一个进程中同时存在一个以上的线程.当一个程序需要同时完成多个任务时或者多个线程效率更高的情况下，比如下载可以使用多线程.

对于线程池:java给我们提供了Executor类,Executor作为灵活且强大的异步执行框架，其支持多种不同类型的任务执行策略，提供了一种标准的方法将任务的提交过程和执行过程解耦开发，基于生产者-消费者模式，其提交任务的线程相当于生产者，执行任务的线程相当于消费者，并用Runnable来表示任务，Executor的实现还提供了对生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。

Java通过Executors提供四种线程池，分别为：

 newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。

newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 

newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。

newWorkStealingPool jdk8增加了newWorkStealingPool(int parall)，增加并行处理任务的线程池，不能保证处理的顺序。

主要参数:

​```
corePoolSize：线程池的大小。线程池创建之后不会立即去创建线程，而是等待线程的到来。当当前执行的线程数大于改值是，线程会加入到缓冲队列；
maximumPoolSize：线程池中创建的最大线程数；
keepAliveTime：空闲的线程多久时间后被销毁。默认情况下，改值在线程数大于corePoolSize时，对超出corePoolSize值得这些线程起作用。
unit：TimeUnit枚举类型的值，代表keepAliveTime时间单位，可以取下列值：
TimeUnit.DAYS; //天
　　TimeUnit.HOURS; //小时
　　TimeUnit.MINUTES; //分钟
　　TimeUnit.SECONDS; //秒
　　TimeUnit.MILLISECONDS; //毫秒
　　TimeUnit.MICROSECONDS; //微妙
　　TimeUnit.NANOSECONDS; //纳秒
workQueue：阻塞队列，用来存储等待执行的任务，决定了线程池的排队策略，有以下取值：
　　ArrayBlockingQueue;
　　LinkedBlockingQueue;
　　SynchronousQueue;
　　threadFactory：线程工厂，是用来创建线程的。默认new Executors.DefaultThreadFactory();
handler:线程拒绝策略。当创建的线程超出maximumPoolSize，且缓冲队列已满时，新任务会拒绝，有以下取值：
　　ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 
　　ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 
　　ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）
　　ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务
​```
```



### 13、说一下hashmap，是如何实现的？ ###

```
HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，计算并返回的hashCode是用于找到Map数组的bucket位置来储存Node 对象。这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Node 。
```

![20190103132811368-6636693](img/ajava/图1.png)

```
以下是HashMap初始化 ，简单模拟数据结构**Node[] table=new Node[16]** 散列桶初始化，tableclass Node {hash;//hash值key;//键　value;//值　node next;//用于指向链表的下一层（产生冲突，用拉链法）}

以下是具体的put过程（JDK1.8版）

1、对Key求Hash值，然后再计算下标

2、如果没有碰撞，直接放入桶中（碰撞的意思是计算得到的Hash值相同，需要放到同一个bucket中）

3、如果碰撞了，以链表的方式链接到后面

4、如果链表长度超过阀值( TREEIFY THRESHOLD==8)，就把链表转成红黑树，链表长度低于6，就把红黑树转回链表

5、如果节点已经存在就替换旧值

6、如果桶满了(容量16*加载因子0.75)，就需要 resize（扩容2倍后重排）

以下是具体get过程(考虑特殊情况如果两个键的hashcode相同，你如何获取值对象？)当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。
```

![20190103132811368-6636693](img/ajava/图2.png)



### 14、lambda架构是什么？ ###

![20190103132811368-6636693](img/ajava/20190103132811368.png)

```
lambda架构从这点出发, 有两套解决办法, 正如图上的两条分支, 一条叫Speed Layer 顾名思义 快速的处理实时数据以供查询, 而另一条分支, 又分作两层(Batch Layer & Serving Layer) 处理那些对时效性要求不高的数据。

Speed Layer处理实时数据 代价是对计算资源要求很高, 而且逻辑复杂度也会很高， 通常采用的技术比如 Redis，Storm，Kafka，Spark Streaming等。而另外两层使用的典型技术比如MR或Spark，Hive。这条路线处理延迟比较大， 结果逻辑相对简单，往往把它的处理叫做“离线处理”， 与Speed Layer的“实时处理”相对应。这种设计被称作：Complexity Isolation（复杂度分离）。

两者其实是相辅相成的, Batch Layer会持续地吸收增量数据加以处理（比如渐变维度，增加索引，划分分区，预计算聚合值等操作）, 当新增数据被Batch Layer处理完成后, 它们的分析就不再由Speed Layer处理了（交由Serving Layer处理），所以保证了Speed Layer处理的历史数据量永远不会太大，毕竟对于Speed Layer来说 “快” 是关键。
```



# 第二章 数据库 #
### 1、你们用redis是用来干嘛的 ###
### 2、redis支持的数据结构 ###
### 3、redis的缓存有哪些问题？一致性？击穿？雪崩等是如何解决的？说出具体的解决方法？ ###
### 4、mysql的引擎是什么？innodb的底层是什么？ ###
### 5、你们mysql的单表数据量大小有多少？ ###
### 6、mysql的优化了解哪些？ ###
### 7、布隆过滤器的底层？你在项目中是如何使用的？ ###
### 8、 ###
![](./img/20001.jpg)
![](./img/20002.jpg)

### 9、Left Join跟Right Join，Inner Join的区别 ###
### 10、行转列 ###
### 11、mysql的索引类型，什么是b+树 ###
### 12、mysql中有100万条数据，b+树对应的高度是多少了？ ###

> sql题
其中
courseid
1 对应语文
2 对应数学
3 对应英语



> 将
stuid	courseid	score
> 
1		1			50


> 1		2			90	


> 1		3			60


> 2		1			30
。。。。


> 变为


> stuid	chinesescore mathscore	englishscore



> 1 		50 			 90 		60
。。。。
### 13、mysql事务，在删除、写入数据时怎么保证事务 ###
### 14、mysql设计学生表课程表成绩表，sql实现求各科成绩在第六名到第九名的学生信息； ###



# 第三章 Linux #
### 1、Linux中、如何调整文件最大打开数

```
修改linux的软硬件限制文件/etc/security/limits.conf. 

在文件尾部添加如下代码： 
* soft nofile 65536
* hard nofile 131072

修改完成后可以使用ulimit查看即可。

注:
*  代表任何用户
soft 代表软件
nofile 最大文件
noproc 最大进程
65536 数量
```



### 2、请列举几个常用的Linux命令

```
man :帮助命令

sudo : 获取root权限

jps : 查看java进程

ps -ef  : 查看进程

ps -aux : 查看进程

netstat -nltcp : 监听端口

free : 内存查看

top :动态 查看服务器资源信息

uptime : 查看系统运行时长和负载率

w : 查看系统运行时长和负载率

df : 查看磁盘情况

df -h /home : 查看home的使用情况

iostat :  查看i/o情况

tar :

zip :

unzip :

gzip:

gunzip:
rpm :

yum :

等等。
```



### 3、linux如何查看系统负载,内存.硬盘使用情况

```
系统负载命令:uptime、w、top
[root@node242 ~]# uptime
 12:06:18 up 6 days, 12:51,  3 users,  load average: 0.00, 0.01, 0.05


[root@node242 ~]# w
 12:06:42 up 6 days, 12:51,  3 users,  load average: 0.00, 0.01, 0.05
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
root     pts/0    10.0.151.251     08:53    3:11m  4:38   0.03s -bash
root     pts/2    10.0.157.236     11:39   27:03   0.01s  0.01s -bash
root     pts/3    10.0.151.253     11:47    2.00s  0.05s  0.01s w

[root@node242 ~]# top
top - 12:07:29 up 6 days, 12:52,  3 users,  load average: 0.00, 0.01, 0.05
Tasks: 249 total,   1 running, 248 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 98824672 total, 73614424 free, 21389376 used,  3820868 buff/cache
KiB Swap:  4194300 total,  4194300 free,        0 used. 76497328 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 7706 root      20   0   10.4g   1.1g  15900 S   1.0  1.1  76:26.86 java
 5461 root      20   0 3129584 459292  19332 S   0.7  0.5  55:37.83 java
......

load average分别对应于过去1分钟，5分钟，15分钟的负载平均值。

内存命令:free、top
[root@node242 ~]# free
              total        used        free      shared  buff/cache   available
Mem:       98824668    21388812    73435172        9616     4000684    76496944
Swap:       4194300           0     4194300


磁盘命令:df
[root@node242 ~]# df
Filesystem               1K-blocks     Used  Available Use% Mounted on
/dev/mapper/centos-root   52403200  7870980   44532220  16% /
devtmpfs                  49398856        0   49398856   0% /dev

[root@node242 ~]#
[root@node242 ~]# df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   50G  7.6G   43G  16% /
devtmpfs                  48G     0   48G   0% /dev

[root@node242 ~]# df -h /home
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-home  1.8T   48G  1.8T   3% /home


查看io资源命令：iostat  (需要安装)
```



### 4、linux  >  和 >> 的区别,常用查看日志命令

```
> : 覆盖文件，如果文件没有将会创建，并且将内容写到文件中，可以用于删除数据重新写入数据场景。
>> : 在文件末尾追加，如果文件没有将会创建，用于原文件数据不能丢失场景。
```

### 5、用shell脚本怎么替换字符串(现场写) ###

```
vi /home/test/rep.sh
#!/bin/bash

ori_str=$1
rep_str=$2
grep_word=$3
file_dir=$4
sed -i "s/${ori_str}/${rep_str}/g" `grep "$grep_word" -rl $4`  #数据源未目录或者文件即可

测试：
[root@hadoop01 test]# chmod a+x /home/test/rep.sh
[root@hadoop01 test]# ./rep.sh abc 123 abc /home/test/
```



### 6、HDFS原理是什么，我们Linux中文件的原理是什么，区别是什么 ###

```
hdfs原理：
使用多台廉价服务器来构建分布式文件存储系统，主要是一个master/slave架构，2.x版本可以实现多(推荐2)主多从来实现HA机制。同时该分布式文件系统也使用副本机制来最大限度保障文件的安全性能。
linxu原理：
在LINUX系统中有一个重要的概念：一切都是文件。 其实这是UNIX哲学的一个体现，而Linux是重写UNIX而来，所以这个概念也就传承了下来。在UNIX系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。
文件系统这一层相信大家都再熟悉不过了，目前大多Linux发行版本默认使用的文件系统一般是ext4，另外，新一代的btrfs也呼之欲出，不管什么样的文件系统，都是由一系列的mkfs.xxx命令来创建，如：
  mkfs.ext4 /dev/sda
  mkfs.btrfs /dev/sdb
  内核所支持的文件系统类型，可以通过内核目录树 fs 目录中的内容来查看。
  
共同：
1、都是文件系统，都可以存储文件或者目录等。
2、都是以块的形式存储的。

不同点:
1、块:linux的块直接对应于物理磁盘的block，而hdfs的块对应于linux中的文件。
2、块大小：linux的块大小要看磁盘类型，有1k、4k等。而hdfs中的块大小很轻松自行设置，默认hadoop1.x为64M，hadoop2.x为128M，hadoop3.x为256M，远大于linux的block大小。
3、文件:一个文件有多个block组成，而hdfs中一个文件底层还是由多个文件组成。
4、元数据:有操作系统上的inode记录其文件存储的数据区的block指针，而hdfs的由namenode来进行维护。
5、应用：linux主要用于应用服务和数据存储(不强调海量和分布式)，而hdfs强调的是规模数据分布式的存储。
```



### 7、shell脚本编程：将select max(id) from table的值赋值到一个变量中； ###

```
vi /home/test/my.sh
#!/bin/bash

mysql_var=`mysql --skip-column-names -uroot -proot -e "use test;select max(id) from stu"`
echo $mysql_var

测试:
chmod a+x /home/test/my.sh
/home/test/my.sh  运行脚本
```



### 8、shell脚本编程：将mysql数据库中五个库中的五张表合并到一张表中，表的结构都相同； ###

```
vi /home/test/mm.sh
#!/bin/bash

`mysql --skip-column-names -uroot -proot -e "insert into test.a1(id,name) select from ( select id,name from test1.a1 union all select id,name from test2.a1 union all select id,name from test3.a1 union all select id,name from test4.a1 union all select id,name from test5.a1)"`


测试(未测试):
chmod a+x /home/test/mm.sh
/home/test/mm.sh  运行脚本
```



### 9、Linux的系统了解吗，讲解一下Linux的系统 ###

```
linux是一款开源的类unix操作系统，一般有4个主要部分组成，分别为：内核、shell、文件系统和应用程序。内核、shell和文件系统一起形成了基本的操作系统结构，它们使得用户可以运行程序、管理文件并使用系统。结构图如下：
```

![](./img/3-9.01.png)

```
内核是操作系统的核心，具有很多最基本功能，它负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。Linux 内核由如下几部分组成：内存管理、进程管理、设备驱动程序、文件系统和网络管理等。内核架构如下图:
```

![](./img/3-9.02.png)

```
其他说辞结合该图即可。

该答案引用:https://blog.csdn.net/kai_zone/article/details/80444872
```



# 第四章 hadoop #
### 1、mapreduce 二次排序 ###
> 待排序的数据具有多个字段，首先对第一个字段进行排序，第一个字段相同的情况下，再按照第二个字段进行排序，第二次排序不会破坏第一次排序的结果。这个过程称之为二次排序。

原始数据如下：  
![](./img/4-1.01.jpg)

输出结果数据如下：  
![](./img/4-1.02.jpg)

#### 第一种方式：简答粗暴 ####
> 第一个字段在规约到reduce端的reduce函数之前排好序。而我们只需要在进入reduce函数后，对第二个字段进行再次排序即可。如下代码：

	 @Override
	 public void reduce(Text key, Iterable<IntWritable> values, Context context)
	         throws IOException, InterruptedException {
	
	      List<Integer> valuesList = new ArrayList<Integer>();
	
	      // 取出value
	      for(IntWritable value : values) {
	          valuesList.add(value.get());
	      }
	      // 进行排序
	      Collections.sort(valuesList);
	
	      for(Integer value : valuesList) {
	         context.write(key, new IntWritable(value));
	      }
	
	 }

> 但是，如果把排序工作都放到reduce端完成，当values序列长度非常大时，回对cpu和内存造成极大的负载。

#### 第二种方式： ####
> 将map端输出的<key,value>中的key和value组合成一个新的key（称为newKey），value值不变。这里就变成<(key,value),value>，在针对newKey排序的时候，如果key相同，就再对value进行排序。  
> 需要自定义的地方  
　　1. 自定义数据类型实现组合key  
　　　 实现方式：继承WritableComparable  
　　2. 自定义partioner，形成newKey后保持分区规则仍然按照key进行。保证不打乱原来的分区。  
　　　　实现方式：继承partitioner  
　　3. 自定义分组，保持分组规则仍然按照key进行。不打乱原来的分组
　　　　实现方式：继承RawComparator



### 2、yarn原理 ###
#### 为什么要使用YARN ####

	为了提升集群的利用率、资源统一管理， 使用YARN为上层应用提供统一的资源管理和调度的平台。  

#### YARN的优势 ####

1. 资源的统一管理和调度：     集群中所有节点的资源(内存、CPU、磁盘、网络等)抽象为Container。计算框架需要资源进行运算任务时需要向YARN申请Container， YARN按照特定的策略对资源进行调度进行Container的分配。    
2. 资源隔离：     YARN使用了轻量级资源隔离机制Cgroups进行资源隔离以避免相互干扰，一旦Container使用的资源量超过事先定义的上限值，就将其杀死。  
3. YARN的几个专用名词：
	- Resource Manager：全局资源管理器，一个集群只有一个RM。负责和AM(Application Master)交互，资源调度、资源分配等工作。
	- Application Master：应用程序的管理器，类似项目经理，一个应用程序只有一个AM。负责任务开始时找RM要资源,任务完成时向RM注销自己，释放资源；与NM通信以启动/停止任务；接收NM同步的任务进度信息。
	- Node Manager：一台机器上的管理者，类似于部门经理。管理着本机上若干小弟Containers的生命周期、监视资源和跟踪节点健康并定时上报给RM；接收并处理来自AM的Container启动/停止等各种请求。
	- Container：一台机器上具体提供运算资源，将设备上的内存、CPU、磁盘、网络等资源封装在一起的抽象概念——“资源容器”，Container是一个动态资源分配单位，为了限定每个任务使用的资源量。
	- Attempt：提交到Yarn中的应用程序被称为Application，它可能会尝试运行多次，每次的尝试运行称为“Application Attempt”，如果一次尝试运行失败，则由RMApp创建另一个继续运行，直至达到失败次数的上限。
	![](./img/4-2.01.png)
4. 执行过程:参考上图
		1. 用户向YARN提交程序，以Map Reduce程序为例，Resource Manager(资源管理器)接收到客户端程序的运行请求
		2. Resource Manager分配一个Container(资源)用来启动Application Master(程序管理员)，并告知Node Manager(节点管理员)，要求它在这个Container下启动Application Master
		3. Application Master启动后，向Resource Manager发起注册请求
		4. Application Master向Resource Manager申请资源
		5. 取得资源后，根据资源，向相关的Node Manager通信，要求其启动程序
		6. Node Manager（多个）启动MR（每个MR任务都是一个job，可以在job日志中查看程序运行日志）
		7. Node Manager不断汇报MR状态和进展给Application Master
		8. 当MR全部完成时，Application Master向Resource Manager汇报任务完成，并注销自己


### 3、什么是MapReduce ###
1. MapReduce是Hadoop的一个核心技术、是一个基于分布式的对大数据集进行并行处理的一个计算框架。
2. 核心思想是移动计算而非数据。
3. 整个计算流程分为两个阶段，一个是map阶段，一个是reduce阶段
	- map阶段  
			一个mapreduce作业在map阶段会先对数据进行逻辑分片处理，一个逻辑分片对应一个MapTask。每一个MapTask在处理数据的时候，都会将每一行数据解析成键值对<k1，v1>的形式，作为输入。输入给map函数。map函数处理后，也会以键值对<k2，v2>的形式作为输出。之后会对数据进行分区，排序等处理
	- reduce阶段 
           在进入reduce阶段之前。reduceTask会将自己要处理的分区数据fetch到reduceTask所在的机器节点上,然后以键值对<k2,list<v2>>作为reduce函数的输入，经过函数处理后，再以键值对<k3,v3>的形式作为输出。最终结果可以保存到hdfs系统上。
### 4、getSplit怎么分片的，分片的大小 ###
1. split是逻辑分片，再mapTask任务开始前，将文件按照指定的大小进行逻辑切分。每一个部分称之为一个split。默认情况下，split的大小与block的大小相等。均为128M.
2. 可以参考FileInputForamt类的getSplits()源码
	1. 会先获取三个参数的值，minSize,maxSize,blockSize
	2. 然后创建一个分片集合用于存储分片数据
	3. 获取文件的所有块信息，进行遍历
	4. 得到一个块的状态信息，然后判断是否可以切分
	5. 然后根据三个参数获取分片大小
	6. 循环判断文件剩余部分是否大于切片大小的1.1倍，
		1. 大于的话就调用makeSplit方法创建当前块的逻辑分片
		2. 不大于的话，就将文件剩余的部分创建一个唯一的最后一个分片。
	7. 将每一个逻辑分片添加到分片集合中，等待被使用

3. 分片的大小由minSize,maxSize,blockSize三个参数决定
	
		算法如下：

		Math.max(minSize,Math.min(maxSize, blockSize))，其中maxSize是取得longValueMax的值
		
		1.如果blockSize小于maxSize && blockSize 大于 minSize之间，那么split就是blockSize；
		
		2.如果blockSize小于maxSize && blockSize 小于 minSize之间，那么split就是minSize；
		
		3.如果blockSize大于maxSize && maxSize   大于 minSize之间，那么split就是maxSize；
		
		4.如果blockSize大于maxSize && maxSize   小于 minSize之间，那么split就是maxSize（不存在这种关系）。

### 5、快排、归并怎么实现的？时间复杂度？ ###
1. 快速排序
	1. 是一个优秀的排序算法，O(n²)和Ω(nlgn)，期望运行时间：θ(nlgn)且常数因子较小。
	2. 快速排序采用了分治的思想
		- 分：将数组划分成两个部分(核心，partition)
		- 治：递归的对划分的两个子数组进行排序
		![](./img/4-5.1.jpg)
2. 归并排序
	1. 归并排序（英语：Merge sort，或mergesort），是创建在归并操作上的一种有效的排序算法，效率为O(n log n)。1945年由约翰·冯·诺伊曼首次提出。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。
	2. 让左右两部分的元素先有序，然后把两个有序的部分合并为一个有序的过程。那么如何让左边的部分和右边的部分有序呢？继续把左边的部分分为两部分，然后排序。然后再把右边的部分分为两部分，再排序。这是一个递归的过程
	3. ![](./img/4-5.2.jpg)
	4. 代码如下图  
	   ![](./img/4-5.3.jpg)
### 6、shuffle流程 ###
1. map函数的输出 先写入环形缓冲区内（默认100M），当达到阈值80%，会进行溢出写到本地文件系统内。
2. 在溢出写之前，会先根据reduceTask任务数进行相应的分区(partition)
3. 分区后，再再内存中进行排序（快排）
4. 如果指定了combiner函数，排序后，会进行调用combiner函数
5. 然后开始溢出写，产生溢写文件。如果文件块较大，可能会产生多个溢写文件
6. 当有多个溢写文件时，会merge成一个大的溢写文件。merge期间同一个区间的合并到一起并排序，如果指定了combiner,会调用（前提是溢写文件数>3）。map阶段完成
7. reduceTask会通过http协议将属于它所处理的分区的数据fetch到运行reduceTask的节点上。
8. 如果数据较小，直接在内存中merge。如果数据较大，会先溢写到磁盘中，最后再统一merge到内存中（同样，溢写文件数>3会调用combiner函数）
9. 然后将内存中的数据输入给reduce函数。shuffle流程结束。
### 7、 ###
![](./img/40001.jpg)
![](./img/40002.jpg)
### 8、高可用时两个namenode的元数据不相同怎么处理（原因+处理方式） ###
### 9、环形缓冲区详细讲解（源码级） ###


> 1. 环形缓冲区其实是一个字节数组，在MapTask.MapOutputBuffer中定义的。数组中存放着key、value的序列化数据和key、value的元数据信息  
> 2. key/value的元数据存储的格式是int类型，每个key/value对应一个元数据，元数据由4个int组成，第一个int存放value的起始位置，第二个存放key的起始位置，第三个存放partition，最后一个存放value的长度。  
> 3. key/value序列化的数据和元数据在环形缓冲区中的存储是由equator分隔的，key/value按照索引递增的方向存储，meta则按照索引递减的方向存储，将其数组抽象为一个环形结构之后，以equator为界，key/value顺时针存储，meta逆时针存储。
> 4. 环形缓冲区:参考下图
![](./img/4-9.1.jpg)
> 5.我们可以将之抽象成环形状态：如下图
![](./img/4-9.2.jpg)

> 6.当数据量达到阈值(数组的80%）开始进行溢写，溢写后，释放空间，重置equator点。
> 重置时，如果已经存在数据(20%的空间会继续写进来数据)，一般不会移动kv对，而是移动元数据。

### 10、基于yarn的任务运行时报错，用Linux命令行查看错误日志信息 ###
> 1、查看某个job的日志，例如：
yarn logs -applicationId application_1529513682598_0009


> 2、查看某个job的状态，例如：
yarn application -status application_1529513682598_0009



> 3、终止某个job    注意：一般不要直接在UI界面或者是终端kill掉任务，该任务可能还会继续执行下去。
>  正确操作方法：停止job的执行命令如下：yarn application -kill application_1515118561637_0439

### 11、Hadoop三种任务调度 ###

1. FIFO Scheduler

   ```
   将所有的Applications放到队列中，先按照作业的优先级高低、再按照到达时间的先后，为每个app分配资源。如果第一个app需要的资源被满足了，如果还剩下了资源并且满足第二个app需要的资源，那么就为第二个app分配资源，and so on。  
   优点：简单，不需要配置。  
   缺点：不适合共享集群。如果有大的app需要很多资源，那么其他app可能会一直等待。  
   ```

2. Capacity Scheduler

   ```
   CapacityScheduler用于一个集群（集群被多个组织共享）中运行多个Application的情况，目标是最大化吞吐量和集群利用率。
   
   CapacityScheduler允许将整个集群的资源分成多个部分，每个组织使用其中的一部分，即每个组织有一个专门的队列，每个组织的队列还可以进一步划分成层次结构（**Hierarchical Queues**），从而允许组织内部的不同用户组的使用。
   每个队列内部，按照FIFO的方式调度Applications。当某个队列的资源空闲时，可以将它的剩余资源共享给其他队列。
   ```

3. Fair Scheduler　

   ```
   FairScheduler允许应用在一个集群中公平地共享资源。默认情况下FairScheduler的公平调度只基于内存，也可以配置成基于memory and CPU。当集群中只有一个app时，它独占集群资源。当有新的app提交时，空闲的资源被新的app使用，这样最终每个app就会得到大约相同的资源。可以为不同的app设置优先级，决定每个app占用的资源百分比。FairScheduler可以让短的作业在合理的时间内完成，而不必一直等待长作业的完成。
   ```

   

### 12、数据存储在hdfs格式,使用的什么压缩方式,压缩比多少 ###

目前在Hadoop中用得比较多的有lzo，gzip，snappy，bzip2这4种压缩格式，笔者根据实践经验介绍一下这4种压缩格式的优缺点和应用场景，以便大家在实践中根据实际情况选择不同的压缩格式。

1. gzip压缩
   - 优点：
     - 压缩率比较高，而且压缩/解压速度也比较快；
     - hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；
     - 有hadoop native库；
     - 大部分linux系统都自带gzip命令，使用方便。
   - 缺点：不支持split。
   - 应用场景：
     当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。譬如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。
     hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。  
2. lzo压缩
   - 优点：
     - 压缩/解压速度也比较快，合理的压缩率；
     - 支持split，是hadoop中最流行的压缩格式；
     - 支持hadoop native库；
     - 可以在linux系统下安装lzop命令，使用方便。
   - 缺点：
     - 压缩率比gzip要低一些；
     - hadoop本身不支持，需要安装；
     - 在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。
   - 应用场景：
     一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越明显。  
3. snappy压缩
   - 优点：
     - 高速压缩速度和合理的压缩率；	
     - 支持hadoop native库。
   - 缺点：
     - 不支持split；
     - 压缩率比gzip要低；
     - hadoop本身不支持，需要安装；
     - linux系统下没有对应的命令。
   - 应用场景：
     当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输入。  
4. bzip2压缩
   - 优点：
     - 支持split；
     - 具有很高的压缩率，比gzip压缩率都高；
     - hadoop本身支持，但不支持native；
     - 在linux系统下自带bzip2命令，使用方便。
   - 缺点：
     - 压缩/解压速度慢；
     - 不支持native。
   - 应用场景：
     适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；
     或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；
     或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。

### 13、了解zookeeper吗，如何进行容错机制 ###

1. 容错概念
   所谓的zookeeper容错是指，当宕掉几个zookeeper服务器之后，剩下的个数必须大于宕掉的个数，也就是剩下的服务数必须大于n/2，zookeeper才可以继续使用，无论奇偶数都可以选举leader。5台机器最多宕掉2台，还可以继续使用，因为剩下3台大于5/2。说为什么最好为奇数个，是在以最大容错服务器个数的条件下，会节省资源，比如，最大容错为2的情况下，对应的zookeeper服务数，奇数为5，而偶数为6，也就是6个zookeeper服务的情况下最多能宕掉2个服务，所以从节约资源的角度看，没必要部署6（偶数）个zookeeper服务。
2. 通过选举制度来选出一个leader，
   假设目前有5台服务器，每台服务器均没有数据，它们的编号分别是1,2,3,4,5,按编号依次启动，它们的选择举过程如下：
   - 服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking。
   - 服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。
   - 服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为领导者，服务器1,2成为小弟。
   - 服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为小弟。
   - 服务器5启动，后面的逻辑同服务器4成为小弟。

### 14、高可用的集群中namenode宕机了，怎么恢复的，数据如何转移 ###

```
首先进入安全模式: 
hdfs dfsadmin -safemode enter 
然后刷一下active节点的log到image 
hdfs dfsadmin -saveNamespace 
然后将active节点的image文件全部拷贝到故障节点的相应目录下 
然后重启故障namenode 
最后hdfs namenode -bootstrapStandby 
到此，故障解决。 
后来还解决过一次hdfs的block丢失的问题，也是将原先的image全部拷贝回来搞定的。 
所以说，即便有ha，定期备份image文件还是很重要的。
```

### 15、hdfs的瓶颈 ###

1. fsimage加载阶段，主要耗时的操作:
   1.1）FSDirectory.addToParent(),功能是根据路径path生成节点INode，并加入目录树中，占加载时间的73%；
     1.2）FSImage.readString和PermissionStatus.read操作是从fsimage的文件流中读取数据（分别是读取String和short）的操作，分别占加载时间的15%和8%；
   优化方案：对fsimage的持久化操作采用多线程技术，为目录列表中的每个目录存储开辟一个线程，用来存储fsimage文件。主线程等待所有存储的子线程完毕后完成对fsimage加载。这样，存储时间将取决于存储最慢的那个线程，达到了提高fsimage加载速度的目的，从而在一定程度上提升了NameNode启动速度。
2. blockReport阶段，DataNode在启动时通过RPC调用NameNode.blockReport()方法， 主要耗时操作：
   2.1）FSNamesystem.addStoredBlock调用，对每一个汇报上来的block，将其于汇报上来的datanode的对应关系初始化到namenode内存中的BlocksMap表中，占用时间比为77%；此方法中主要耗时的两个阶段分别是FSNamesystem.countNode和DatanodeDescriptor.addBlock，后者是java中的插表操作，而FSNamesystem.countNode调用的目的是为了统计在BlocksMap中，每一个block对应的各副本中，有几个是live状态，几个是decommission状态，几个是Corrupt状态。
     2.2）DatanodeDescriptor.reportDiff，主要是为了将该datanode汇报上来的blocks跟namenode内存中的BlocksMap中进行对比，以决定那个哪些是需要添加到BlocksMap中的block，哪些是需要添加到toRemove队列中的block，以及哪些是添加到toValidate队列中的block，占用时间比为20%；
3. 锁的竞争成为性能瓶颈
   优化方案：其中锁内耗时的操作有FSEditLog.logXXX方法，可以考虑将FSEditLog的logXXX操作放到写锁外记录，但会引起记录的顺序不一致，可以通过在写锁内生成SerialNumber，在锁外按顺序输出来解决；
4. UTF8/Unicode转码优化成为性能瓶颈
   优化方案：用SIMD指令的JVM Intrinsic转码实现，32bytes比目前Hadoop内pure Java实现快4倍,64bytes快十几倍。
5. RPC框架中，N个Reader将封装的Call对象放入一个BlockingQueue，N个Handler从一个BlockingQueue中提取Call对象，使得此BlockingQueue成为性能瓶颈
   优化方案：采用多队列，一个Handler一个BlockingQueue，按callid分配队列。
   6.sendHeartbeat阶段，在DataNode调用namenode.sendHeartbeat()方法时会DF和DU两个类，它们通过Shell类的runComamnd方法来执行系统命令，以获取当前目录的 df, du 值，在runComamnd方法中实质是通过java.lang.ProcessBuilder类来执行系统命令的。该类由JVM通过Linux 内核来fork 子进程，子进程当然会完全继承父进程的所有内存句柄，jstack看到JVM此时线程状态大部分处于WAITING， 这个过程会影响DFSClient写入超时，或关闭流出错。

### 16、hdfs存放文件量过大怎么优化 ###

1. 那么可以通过调用 HDFS 的 sync() 方法(和 append 方法结合使用)，每隔一定时间生成一个大文件。或者，可以通过写一个 MapReduce 程序来来合并这些小文件。

2. HAR File  
   Hadoop Archives （HAR files）是在 0.18.0 版本中引入到 HDFS 中的，它的出现就是为了缓解大量小文件消耗 NameNode 内存的问题。HAR 文件是通过在 HDFS 上构建一个分层文件系统来工作。HAR 文件通过 hadoop archive 命令来创建，而这个命令实际上是运行 MapReduce 作业来将小文件打包成少量的 HDFS 文件（译者注：将小文件进行合并成几个大文件）。对于客户端来说，使用 HAR 文件系统没有任何的变化：所有原始文件都可见以及可以访问（只是使用 har://URL，而不是 hdfs://URL），但是在 HDFS 中中文件个数却减少了。

   读取 HAR 文件不如读取 HDFS 文件更有效，并且实际上可能更慢，因为每个 HAR 文件访问需要读取两个索引文件以及还要读取数据文件本。

   尽管 HAR 文件可以用作 MapReduce 的输入，但是 Map 没有办法直接对共同驻留在 HDFS 块上的 HAR 所有文件操作。可以考虑通过创建一种 input format，充分利用 HAR 文件的局部性优势，但是目前还没有这种input format。需要注意的是：MultiFileInputSplit，即使在 HADOOP-4565 进行了改进，选择节点本地分割中的文件，但始终还是需要每个小文件的搜索。在目前看来，HAR 可能最好仅用于存储文档。

3. SequenceFile  
   通常解决”小文件问题”的回应是：使用 SequenceFile。这种方法的思路是，使用文件名作为 key，文件内容作为 value

   在实践中这种方式非常有效。我们回到10,000个100KB大小的小文件问题上，你可以编写一个程序将合并为一个 SequenceFile，然后你可以以流式方式处理（直接处理或使用 MapReduce） SequenceFile。这样会带来两个优势：

   SequenceFiles 是可拆分的，因此 MapReduce 可以将它们分成块，分别对每个块进行操作；
   与 HAR 不同，它们支持压缩。在大多数情况下，块压缩是最好的选择，因为它直接对几个记录组成的块进行压缩，而不是对每一个记录进行压缩。
   将现有数据转换为 SequenceFile 可能会很慢。但是，完全可以并行创建一个一个的 SequenceFile 文件。Stuart Sierra 写了一篇关于将 tar 文件转换为 SequenceFile 的文章，像这样的工具是非常有用的，我们应该多看看。向前看，最好设计好数据管道，如果可能的话，将源数据直接写入 SequenceFile，而不是作为中间步骤写入小文件。

   与 HAR 文件不同，没有办法列出 SequenceFile 中的所有键，所以不能读取整个文件。Map File，类似于对键进行排序的 SequenceFile，维护部分索引，所以他们也不能列出所有的键

### 17、 ###
![](./img/40003.jpg)
![](./img/40004.jpg)
![](./img/40005.jpg)
![](./img/40006.jpg)
![](./img/40007.jpg)

1. ![](img/40003.jpg)

   1. 数据分片
      参考第4题

   2. 数据完整性

      Hadoop 用户肯定都不希望系统在存储和处理数据时不会丢失或者损坏任何数据。接下来，我们来考究一下 HDFS 在为了保证数据完整性，所做的工作。

      总的来说，HDFS 会对写入的数据计算校验和，并在读取数据时验证校验和。

      datanode 负责收到数据后存储该数据及其校验和。datanode 的数据来源可分为两种，其一为是从客户端收到的数据，其二为从其他 datanode 复制来的数据。还有一种情况，正在些数据的客户端将数据及其校验和发送到由一系列 datanode 组成的管线，管线中最后一个 datanode 负责验证校验和。

      客户端从 datanode 读取数据时，也会验证校验和，将他们与 datanode 中存储的校验和进行比较。每个 datanode 都持久保存一个用于验证的校验和日志，所以会知道每个数据块的最后一次验证时间。客户端成功验证一个数据块后，会告诉这个 datanode 来更新日志。对于检测损坏的磁盘很有价值。

      不只是客户端读取数据库时会验证校验和，每个 datanode 也会在一个后台进程中运行一个 DataBlockScanner ，从而定期验证存储在这个 datanode 的所有数据库。该措施是解决物理存储媒体上位损坏的有力措施。

      HDFS 会存储每个数据块的复本，可以通过数据复本来修复损坏的数据块。 客户端在读取数据块时，如果检测到错误首先向 namenode 报告已损坏的数据块及其正在尝试读取操作的这个 datanode 。namenode 会将这个数据块标记为已损坏，对这个数据块的请求会被 namenode 安排到另一个复本上。之后，它安排这个数据块的另一个复本复制到另一个 datanode 上，如此，数据块的复本因子又回到期望水平。此后，已损坏的数据块复本会被删除。

      在读取数据时，也可以禁止校验，把已损坏的数据在删除前尝试看看是否能够恢复数据。

   3. map的数量 map的数量通常是由hadoop集群的DFS块大小确定的，也就是输入文件的总块数，正常的map数量的并行规模大致是每一个Node是10~100个，对于CPU消耗较小的作业可以设置Map数量为300个左右，但是由于hadoop的每一个任务在初始化时需要一定的时间，因此比较合理的情况是每个map执行的时间至少超过1分钟。具体的数据分片是这样的，InputFormat在默认情况下会根据hadoop集群的DFS块大小进行分片，每一个分片会由一个map任务来进行处理，当然用户还是可以通过参数mapred.min.split.size参数在作业提交客户端进行自定义设置。还有一个重要参数就是mapred.map.tasks，这个参数设置的map数量仅仅是一个提示，只有当InputFormat 决定了map任务的个数比mapred.map.tasks值小时才起作用。同样，Map任务的个数也能通过使用JobConf 的conf.setNumMapTasks(int num)方法来手动地设置。这个方法能够用来增加map任务的个数，但是不能设定任务的个数小于Hadoop系统通过分割输入数据得到的值。当然为了提高集群的并发效率，可以设置一个默认的map数量，当用户的map数量较小或者比本身自动分割的值还小时可以使用一个相对交大的默认值，从而提高整体hadoop集群的效率。 2 reduece的数量 reduce在运行时往往需要从相关map端复制数据到reduce节点来处理，因此相比于map任务。reduce节点资源是相对比较缺少的，同时相对运行较慢，正确的reduce任务的个数应该是0.95或者1.75 *（节点数 ×mapred.tasktracker.tasks.maximum参数值）。如果任务数是节点个数的0.95倍，那么所有的reduce任务能够在 map任务的输出传输结束后同时开始运行。如果任务数是节点个数的1.75倍，那么高速的节点会在完成他们第一批reduce任务计算之后开始计算第二批 reduce任务，这样的情况更有利于负载均衡。同时需要注意增加reduce的数量虽然会增加系统的资源开销，但是可以改善负载匀衡，降低任务失败带来的负面影响。同样，Reduce任务也能够与 map任务一样，通过设定JobConf 的conf.setNumReduceTasks(int num)方法来增加任务个数。 3 reduce数量为0 有些作业不需要进行归约进行处理，那么就可以设置reduce的数量为0来进行处理，这种情况下用户的作业运行速度相对较高，map的输出会直接写入到 SetOutputPath(path)设置的输出目录，而不是作为中间结果写到本地。同时Hadoop框架在写入文件系统前并不对之进行排序。

2. ![](img/40004.jpg)

```
1. Row_Number,Rank，Dense_Rank   这三个窗口函数的使用场景非常多 
	
	row_number():从1开始，按照顺序，生成分组内记录的序列,row_number()的值不会存在重复,当排序的值相同时,按照表中记录的顺序进行排列;通常用于获取分组内排序第一的记录;获取一个session中的第一条refer等。
	rank()：生成数据项在分组中的排名，排名相等会在名次中留下空位。
	dense_rank():生成数据项在分组中的排名，排名相等会在名次中不会留下空位。

2. 2.SUM、AVG、MIN、MAX
	
	首先理解下什么是WINDOW子句
	
	PRECEDING：往前
	FOLLOWING：往后
	CURRENT ROW：当前行
	UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点
	UNBOUNDED FOLLOWING：表示到后面的终点
3. NTILE  
	NTILE(n) 用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。NTILE不支持ROWS BETWEEN
4. LEAD,LAG,FIRST_VALUE,LAST_VALUE

	lag与lead函数可以返回上下行的数据
	
	LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值
	第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）
	
	LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值
	第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）
	FIRST_VALUE:取分组内排序后，截止到当前行，第一个值

	LAST_VALUE:取分组内排序后，截止到当前行,最后一个值
5. CUME_DIST，PERCENT_RANK

	这两个序列分析函数不是很常用，这里也介绍下，他不支持window子句

	–CUME_DIST 小于等于当前值的行数/分组内总行数	
	–比如，统计小于等于当前薪水的人数，所占总人数的比例

6. Hive Map Join
	1. 什么是MapJoin?
		MapJoin顾名思义，就是在Map阶段进行表之间的连接。而不需要进入到Reduce阶段才进行连接。这样就节省了在Shuffle阶段时要进行的大量数据传输。从而起到了优化作业的作用。
```

​		

```
	2. MapJoin的原理：
	
		通常情况下，要连接的各个表里面的数据会分布在不同的Map中进行处理。即同一个Key对应的Value可能存在不同的Map中。这样就必须等到Reduce中去连接。
		
		要使MapJoin能够顺利进行，那就必须满足这样的条件：除了一份表的数据分布在不同的Map中外，其他连接的表的数据必须在每个Map中有完整的拷贝。
```

​		

```
	3. MapJoin适用的场景：
```

​		

```
		通过上面分析你会发现，并不是所有的场景都适合用MapJoin. 它通常会用在如下的一些情景：在二个要连接的表中，有一个很大，有一个很小，这个小表可以存放在内存中而不影响性能。
		
		这样我们就把小表文件复制到每一个Map任务的本地，再让Map把文件读到内存中待用。
	
	4. MapJoin的实现方法：
	
	     1）在Map-Reduce的驱动程序中使用静态方法DistributedCache.addCacheFile()增加要拷贝的小表文件，。JobTracker在作业启动之前会获取这个URI列表，并将相应的文件拷贝到各个TaskTracker的本地磁盘上。
	 
	     2）在Map类的setup方法中使用DistributedCache.getLocalCacheFiles()方法获取文件目录，并使用标准的文件读写API读取相应的文件。
	 
	5. Hive内置提供的优化机制之一就包括MapJoin。
		在Hive v0.7之前，需要使用hint提示 /*+ mapjoin(table) */才会执行MapJoin  。Hive v0.7之后的版本已经不需要给出MapJoin的指示就进行优化。它是通过如下配置参数来控制的：
		 
		hive> set hive.auto.convert.join=true;
		  
		Hive还提供另外一个参数--表文件的大小作为开启和关闭MapJoin的阈值。
		 
		hive.mapjoin.smalltable.filesize=25000000 即25M

3. 动态分区

	1. 动态分区属性设置及示例
		
		动态分区的属性： hive.exec.dynamic.partition=true hive.exec.dynamic.partition.mode=strict/nonstrict hive.exec.max.dynamic.partitions=1000 hive.exec.max.dynamic.partitions.pernode=100
	2. 创建动态分区表
			
		create table dy_part1( id int, name string ) partitioned by (dt string) row format delimited fields terminated by ',' ;
	3. 动态分区加载数据不能使用load方式加载
		
		load data local inpath '/hivedata/user.txt' into table dy_part1 partition(dt);
	4. 动态分区使用insert into的方式加载数据
		``` 先创建临时表： create table temp_part( id int, name string, dt string ) row format delimited fields terminated by ',' ;
```

3. ![](img/40005.jpg)  

   1. 数据丢失的原因
      Kafka 消息发送分同步 (sync)、异步 (async) 两种方式，默认使用同步方式，可通过 producer.type 属性进行配置；

      通过 request.required.acks 属性进行配置：值可设为 0, 1, -1(all)    -1 和 all 等同

      0 代表：不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据；

      1 代表：producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据；

      -1 代表：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才返回 ack，数据一般不会丢失，延迟时间长但是可靠性高；但是这样也不能保证数据不丢失，比如当 ISR 中只有 leader 时( ISR 中的成员由于某些情况会增加也会减少，最少就只剩一个 leader)，这样就变成了 acks = 1 的情况；

      另外一个就是使用高级消费者存在数据丢失的隐患: 消费者读取完成，高级消费者 API 的 offset 已经提交，但是还没有处理完成Spark Streaming 挂掉，此时 offset 已经更新，无法再消费之前丢失的数据. 解决办法使用低级消费者

   2. 数据丢失的解决办法
      设置同步模式, producer.type = sync, Request.required.acks =  -1, replication.factor >= 2 且 min.insync.replicas >= 2

4. ![](img/40006.jpg)

   1. 在 ES 中,文档归属于一种 类型 (type) ,而这些类型存在于索引 (index) 中,类比传统关系型数据库

   2. DB -> Databases -> Tables -> Rows -> Columns
      关系型数据库

      - 表
      - 行
      - 列

   3. ES -> Indices -> Types -> Documents -> Fields

      - 索引
      - 类型
      - 文档
      - 域(字段)

   4. 倒排索引的原理

      Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。

      ```
      例如，假设我们有两个文档，每个文档的 content 域包含如下内容：
      
      The quick brown fox jumped over the lazy dog
      
      Quick brown foxes leap over lazy dogs in summer
      
      为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：
      
      Term      Doc_1  Doc_2
      -------------------------
      Quick   |       |  X
      The     |   X   |
      brown   |   X   |  X
      dog     |   X   |
      dogs    |       |  X
      fox     |   X   |
      foxes   |       |  X
      in      |       |  X
      jumped  |   X   |
      lazy    |   X   |  X
      leap    |       |  X
      over    |   X   |  X
      quick   |   X   |
      summer  |       |  X
      the     |   X   |
      ------------------------
      现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档：
      
      Term      Doc_1  Doc_2
      -------------------------
      brown   |   X   |  X
      quick   |   X   |
      ------------------------
      Total   |   2   |  1
      两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。
      
      但是，我们目前的倒排索引有一些问题：
      
      Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。
      
      fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。
      
      jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。
      
      使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。
      
      我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。
      
      如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如：
      
      Quick 可以小写化为 quick 。
      
      foxes 可以 词干提取 --变为词根的格式-- 为 fox 。类似的， dogs 可以为提取为 dog 。
      
      jumped 和 leap 是同义词，可以索引为相同的单词 jump 。
      
      现在索引看上去像这样：
      
      Term      Doc_1  Doc_2
      -------------------------
      brown   |   X   |  X
      dog     |   X   |  X
      fox     |   X   |  X
      in      |       |  X
      jump    |   X   |  X
      lazy    |   X   |  X
      over    |   X   |  X
      quick   |   X   |  X
      summer  |       |  X
      the     |   X   |  X
      ------------------------
      这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！
      ```

5. ![](img/40007.jpg)

   1. Hbase的存储特点：
      1. 容量大：
         传统关系型数据库，单表不会超过五百万，超过要做分表分库，不会超过30列
         Hbase单表可以有百亿行、百万列，数据矩阵横向和纵向两个维度所支持的数据量级都非常具有弹性
      2. 面向列：
         面向列的存储和权限控制，并支持独立检索，可以动态增加列，即，可单独对列进行各方面的操作
         列式存储，其数据在表中是按照某列存储的，这样在查询只需要少数几个字段的时候，能大大减少读取的数量
      3. 多版本：
         Hbase的每一个列的数据存储有多个Version，比如住址列，可能有多个变更，所以该列可以有多个version
      4. 稀疏性：
         为空的列并不占用存储空间，表可以设计的非常稀疏。
         不必像关系型数据库那样需要预先知道所有列名然后再进行null填充
      5. 拓展性：
         底层依赖HDFS，当磁盘空间不足的时候，只需要动态增加datanode节点服务(机器)就可以了
      6. 高可靠性：
         WAL机制，保证数据写入的时候不会因为集群异常而导致写入数据丢失
         Replication机制，保证了在集群出现严重的问题时候，数据不会发生丢失或者损坏
         Hbase底层使用HDFS，本身也有备份。
      7. 高性能：
         底层的LSM数据结构和RowKey有序排列等架构上的独特设计，使得Hbase写入性能非常高。
         Region切分、主键索引、缓存机制使得Hbase在海量数据下具备一定的随机读取性能，该性能针对Rowkey的查询能够到达毫秒级别
         LSM树，树形结构，最末端的子节点是以内存的方式进行存储的，内存中的小树会flush到磁盘中（当子节点达到一定阈值以后，会放到磁盘中，且存入的过程会进行实时merge成一个主节点，然后磁盘中的树定期会做merge操作，合并成一棵大树，以优化读性能。）
   2. 使用场景
      1. 交通方面：
         船舶GPS信息，全长江的船舶GPS信息，每天有1千万左右的数据存储。
      2. 金融方面：
         消费信息，贷款信息，信用卡还款信息等
      3. 电商：
         淘宝的交易信息等，物流信息，浏览信息等
      4. 移动：
         通话信息等，都是基于HBase的存储。

### 18、container的生命周期是什么，是整个job运行完成，还是说container上的任务完成后 ###

1. Container启动过程主要经历三个阶段：资源本地化、启动并运行container、资源回收，其中，资源本地化指创建container工作目录，从HDFS下载运行container所需的各种资源（jar包、可执行文件等）等，而资源回收则是资源本地化的逆过程，它负责清理各种资源，它们均由ResourceLocalizationService服务完成的。启动container是由ContainersLauncher服务完成的，而运行container是由插拔式组件ContainerExecutor完成的，YARN提供了两种ContainerExecutor实现，一种是 DefaultContainerExecutor，另一种是LinuxContainerExecutor

2. container的生命周期是这样的：

   nm先去申请资源，然后是locallizing-downloading-localized->running-exit with failure(success)—>kill->clearnup

### 19、sqoop拉取的数据量 ###

实现方法：
利用--where参数对关系型数据库数据进行筛选，将结果导入非关系型数据库。根据特殊字段，将日期作为一个查询条件对源数据进行匹配，将符合条件的记录作为结果采集到非关系型数据库中

格式：
import --connect jdbcurl --username xxxx--password ****** --table tablename  --where "data_insert like '2018-08-16%' " --target-dir /user/scapp/tablename --null-non-string \\N --null-string \\N --lines-terminated-by \n  --hive-import --hive-overwrite --hive-table hivedb.hivetable --delete-target-dir -m 1 --fields-terminated-by \001 --hive-drop-import-delims

参数解析：
--connect :连接的关系数据库的jdbc信息

--username:关系数据库的登录账户

--password:关系数据库的登录密码

--table: 关系数据库表名

 --hive-import: 从关系型数据库向非关系型数据库中导入数据的标志

--where：从关系型数据库导入数据时的查询条件,接where的查询条件。例如：‘–where id<100’
--table:关系数据库表名
注意事项：
1.参数解析未尽之处可参照Mysql导入数据到Hive 的可选参数内容

2.按日增量采集的前提是历史数据未做变更，只需将当日新增数据插入非关系型数据库中

3.本质上只是按照一定查询条件对源数据进行查询后采集，所以查询条件并没有固定格式，查询的字段不限于日期格式的字段，也可以是字符串型或数值型的字段。查询方式是精确查询，也可以是模糊查询。

4.对于字符串型字段，匹配时，一般是使用模糊查询的方式进行匹配 colname like 'yyyy-mm-dd%',注意匹配合适的日期格式

5.一般而言日期格式的字段时可以使用>,<,=符号进行查询，也可以将日期格式的字段值使用函数转化为字符串，参照字符串型字段的方法进行匹配

6.数值型的字段可以使用使用>,<,=符号进行查询

7.该方法不限于获取当天的数据，也可以是某个指定时间段的数据，取决于筛选条件的匹配

8.在工作流中，可以将日期作为一个工作流变量，通过引用该变量，动态的变化查询条件，定时的抽取指定日期的数据

### 20、sqoop是T+1还是当天 ###
### 21、sqoop导入数据时怎么分段 ###

Sqoop通过--split-by指定切分的字段，--m设置mapper的数量。通过这两个参数分解生成m个where子句，进行分段查询。因此sqoop的split可以理解为where子句的切分。

第一步，获取切分字段的MIN()和MAX()
为了根据mapper的个数切分table,sqoop首先会执行一个sql，用于获取table中该字段的最小值和最大值，源码片段为org.apache.sqoop.mapreduce.DataDrivenImportJob 224行。大体为：

```
		`private String buildBoundaryQuery(String col, String query) {
	    ....
	    return "SELECT MIN(" + qualifiedName + "), MAX(" + qualifiedName + ") "
	        + "FROM (" + query + ") AS " + alias;
	  }`
```

获取到最大值和最小值，就可以根据不同的字段类型进行切分。

第二步，根据MIN和MAX不同的类型采用不同的切分方式
支持有Date,Text,Float,Integer，Boolean,NText,BigDecimal等等。

数字都是一个套路，就是步长=（最大值-最小值）/mapper个数

生成的区间为
		

```
	[最小值，最小值+步长)
	[最小值+2*步长，最小值+3*步长)
	...
	[最大值-步长，最大值]
```

可以参考下面的代码片段org.apache.sqoop.mapreduce.db.FloatSplitter 43行：

```
		`    List<InputSplit> splits = new ArrayList<InputSplit>();
		    ...
		    int numSplits = ConfigurationHelper.getConfNumMaps(conf);
		    double splitSize = (maxVal - minVal) / (double) numSplits;
		...
		    double curLower = minVal;
		    double curUpper = curLower + splitSize;
		    while (curUpper < maxVal) {
		        splits.add(new DataDrivenDBInputFormat.DataDrivenDBInputSplit(
		          lowClausePrefix + Double.toString(curLower),
		          highClausePrefix + Double.toString(curUpper)));
		        curLower = curUpper;
		        curUpper += splitSize;
		    }`
```

这样最后每个mapper会执行自己的sql语句，比如第一个mapper执行：

select * from t where splitcol >= min and splitcol < min+splitsize

第二个mapper又会执行

select * from t where splitcol >= min+splitsize and splitcol < min+2*splitsize

### 22、sqoop和cdh是不是要装在一个机器上，如不是，sqoop命令如何在cdh节点上执行？ ###
### 23、cdh平台如果发生故障启动不了了，如何处理？ ###
### 24、如何将已使用原生的Hadoop集群与新建的cdh平台进行一个融合？ ###
### 25、任务调度时如何查看各个job之间的依赖关系？ ###

### 

```
需要获取mapreduce的运行信息,比如运行状态,map,reduce的执行进度.
```

hadoop 50030端口提供web ui服务,没找到提供json或者xml的服务方式.
于是,查找hadoop 50030的加载:
\org\apache\hadoop\mapred\JobTracker.java:
JobTracker(final JobConf conf, String identifier, Clock clock, QueueManager qm)
-->
private void createInstrumentation()
-->

```
		    `...
		    String infoAddr = 
		      NetUtils.getServerAddress(conf, "mapred.job.tracker.info.bindAddress",
		                                "mapred.job.tracker.info.port",
		                                "mapred.job.tracker.http.address");
		    InetSocketAddress infoSocAddr = NetUtils.createSocketAddr(infoAddr);
		    String infoBindAddress = infoSocAddr.getHostName();
		    int tmpInfoPort = infoSocAddr.getPort();
		    this.startTime = clock.getTime();
		    infoServer = new HttpServer("job", infoBindAddress, tmpInfoPort, 
		        tmpInfoPort == 0, conf, aclsManager.getAdminsAcl());
		    infoServer.setAttribute("job.tracker", this); 
		...`
```

在这里JobTracker启了一个提供http服务的Jetty Server,并且设置了这个jetty实例infoServer的application属性job.tracker为 jobtracker本身(this)

在JobTracker运行节点上有jsp页面:
$HADOOP_HOME/webapps/job/jobtracker.jsp

```
		`...
		  JobTracker tracker = (JobTracker) application.getAttribute("job.tracker");
		  ClusterStatus status = tracker.getClusterStatus();
		  ClusterMetrics metrics = tracker.getClusterMetrics();
		  String trackerName =
		           StringUtils.simpleHostname(tracker.getJobTrackerMachine());
		  JobQueueInfo[] queues = tracker.getQueues();
		  Vector<JobInProgress> runningJobs = tracker.runningJobs();
		  Vector<JobInProgress> completedJobs = tracker.completedJobs();
		  Vector<JobInProgress> failedJobs = tracker.failedJobs();
		...
```

`

从这里获取了jobtracker对象,对此对象操作可获取到job的执行信息.

所以:
尝试仿照jobtracker.jsp页面写满足自己需求的jobtracker_1.jsp
直接拷贝一个测试一下.
访问: :50030/jobtracker_1.jsp 不成功.
设置WEB-INF/web.xml加入对url和servlet的对应关系,发现需要编译出org.apache.hadoop.mapred.jobtracker_1_jsp的类
而且,系统自带的这些jsp文件都以_jsp.class的形式存在于hadoop-core.jar里了.

jsp编译为servlet一遍是中间件(tomcat.resin)直接做的事情,而且每种中间件编译出来的类包是不同的,比如一般放到:
org.apache.jsp包下. 如何放到prg.apache.hadoop.mapred下呢?

先试一下,自己写一个jsp页面让resin,tomcat编译为class,再在web,xml中配置为他org.apache.jsp.xxxx可否.

在写jsp的时候发现,要获取JobTracker的信息.里面有很多变量诸如jobtracker.conf,这些变量都是包外不可见的.所以还不能把这个jsp对应的servlet编译为别的包下的类,只能编译到org.apache.hadoop.mapred包里.

搜索如何手动编译jsp为servlet,参考如下文章:
http://blog.csdn.net/codolio/article/details/5177236
通过org.apache.jasper.JspC来手工编译jsp为servlet.

java -cp /opt/jars/ant.jar:/opt/hadoop-1.0.4/lib/commons-logging-1.1.1.jar:/opt/hadoop-1.0.4/hadoop-ant-1.0.4.jar:/opt/hadoop-1.0.4/lib/commons-el-1.0.jar:/opt/hadoop-1.0.4/lib/jasper-compiler-5.5.12.jar:/opt/hadoop-1.0.4/lib/jasper-runtime-5.5.12.jar:/opt/hadoop-1.0.4/lib/servlet-api-2.5-20081211.jar:/opt/hadoop-1.0.4/lib/jsp-2.1/jsp-api-2.1.jar:/opt/hadoop-1.0.4/lib/commons-io-2.1.jar org.apache.jasper.JspC-classpath /opt/hadoop-1.0.4/hadoop-core-1.0.4.jar:/opt/hadoop-1.0.4/hadoop-ant-1.0.4.jar:/opt/hadoop-1.0.4/lib/commons-logging-1.1.1.jar:/opt/hadoop-1.0.4/lib/commons-logging-api-1.0.4.jar:/opt/hadoop-1.0.4/lib/log4j-1.2.15.jar:/opt/hadoop-1.0.4/lib/commons-io-2.1.jar -p org.apache.hadoop.mapred -compile -v -d dist -uriroot ./ -webxml dist/web.xml xxxxx.jsp
注意:
-p参数可以指定编译出来的servlet的package

通过这种方法应该能得到.class和.java,但我却只得到了.java,我理解可能是因为我引用的jar包不全无法通过编译,没有报错信息,很奇怪.

于是基于这个.java文件自己编译为class打成jar包,放到了$HADOOP_HOME/lib/下.
配置job/WEB-INF/web.xml文件为自己编译的类

### 26、zk的作用，应用的场景 ###

1. 服务器节点动态上下线
2. 服务器主从协调
3. 统一配置管理



# 第五章	hive  #
### 1、hive运行的原理

![1-1](img/ahive/1-1.png)



### 2、用两种方法写出求topn得方法

```
排名函数的方法实现，整体分一个组，按照要排序的字段进行排序，然后条件控制输出topn
order by的实现，order by整体排序用limit限制输出数量
```

### 3、udf你写什么，写过udaf吗udtf呢

```
udf继承UDF类，重写evaluate()方法，将具体内容定义在方法中。然后将程序打包上传到服务器，在hive中创建函数指定我们上传的jar包。

```

### 4、hive、impala的原理和实现

### 5、hive的调度方式？

```
使用crontab定时调度hive脚本
使用azkaban、oozie等调度工具进行调度
```

### 6、hive中存储为什么格式,有几张hive表,有什么字段、 压没压缩

```
orc的格式，压缩格式为snappy
```

### 7、点击执行怎么能知道查询哪张hive表

### 8、t+1的方式拉取数据到hive分区表，同一条数据可能在多个分区（按天分区）中，这样怎么保证拿到的数据是最新的

### 9、两个表A 和B ，均有key 和value 两个字段，写一个SQL语句，将B表中的value值置成A表中相同key值对应的value值

### 10、Hive代码运行过慢，说一下子你是怎么去解决的

#### 11、hive中的sql语句写过最复杂的语句,处理数据时候的数据处理时间,进行优化后处理数据的时间是多少?怎么优化的?

### 12、Mysql数据没有时间戳同步到Hive中，不知道那些是当天的数据，不能增量拉取，有没有好的方法

```
可以拉取mysql的日志进行数据的拉取
```

### 13、hsql语句怎么优化数据随着时间推移越来越大，hsql计算耗时超过1小时怎么优化？

### 14、用过哪些开窗函数

```
sum(col) over() : 分组对col累计求和，over() 中的语法如下 
count(col) over() : 分组对col累计，over() 中的语法如下 
min(col) over() : 分组对col求最小 
max(col) over() : 分组求col的最大值 
avg(col) over() : 分组求col列的平均值first_value(col) over() : 某分区排序后的第一个col值 
last_value(col) over() : 某分区排序后的最后一个col值 
lag(col,n,DEFAULT) : 统计往前n行的col值，n可选，默认为1，DEFAULT当往上第n行为NULL时候， 
取默认值，如不指定，则为NULL 
lead(col,n,DEFAULT) : 统计往后n行的col值，n可选，默认为1，DEFAULT当往下第n行为NULL时候， 
取默认值，如不指定，则为NULL 
ntile(n) : 用于将分组数据按照顺序切分成n片，返回当前切片值。注意：n必须为int类型。 
排名函数： 
row_number() over() : 排名函数，不会重复，适合于生成主键或者不并列排名 
rank() over() : 排名函数，有并列名次，名次不连续。如:1,1,3 
dense_rank() over() : 排名函数，有并列名次，名次连续。如：1，1，2 
```

### 15、hive中两个表join转换成mr程序,怎么转换的,原理是什么

​	在map的输出value中为不同表的数据打上tag标记，在reduce阶段根据tag判断数据来源。MapReduce的过程如下

![join](img/ahive/join.png)

### 16、写7日留存率 （顺便说了下redis的bitmap实现）

### 17、每条数据只有时间戳和当日收入，求环比。 

### 18、sort by 和order by的区别

```
	order by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，然而只有一个Reducer，会导致当输入规模较大时，消耗较长的计算时间。这样很可能会超过单个节点的磁盘和内存存储能力导致任务失败。
	sort by的数据只能保证在同一个reduce中的数据可以按指定字段排序。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定)，对输出的数据再执行归并排序
```

### 19、hive的引擎

```shell
可以通过配置去设置hive的引擎
1、配置mapreduce计算引擎
set hive.execution.engine=mr;
2、配置spark计算引擎
set hive.execution.engine=spark;
3、配置tez 计算引擎
set hive.execution.engine=tez;
```


#  第六章 数仓  #
### 1、如果让你处理hbase 怎么保证数据的安全性可靠性 不需要具体的设置 要一套方案 ###

```
有关数据安全及可靠我们认为大体上分为存储安全和使用安全
1 数据存储安全
hbase是基于hdfs的一种数据存储解决方案，所以有关数据的安全性可靠性可以利用hdfs自身的副本机制保障。另外原生的hbase（1.x）并没有提供数据备份机制，目前还是依赖于企业自身的研发保障，如阿里的云hbase进行数据备份。另外在实际操作过程会存在不小心产生的数据误操作，这样备份同样具有实际应用的意义。

2 数据使用安全
示例用户通过接口直接访问HBase数据库，这种情况下存在安全隐患的概率会比较大。一种可能性是黑客会通过黑客技术入侵数据库，对用户的数据进行肆意的“操作”，造成用户数据无法访问，然后进行勒索。必然会要求一种安全机制进行任务守护
```



### 2、hbase的架构 ###

![](img\hbase_arth.png)

```
HBase中的存储包括HMaster、HRegionSever、HRegion、HLog、Store、MemStore、StoreFile、HFile等角色构成，具体如下
```

HMaster的作用

```
HBase中的每张表都通过键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理

1.为HRegionServer分配HRegion
2.负责HRegionServer的负载均衡
3.发现失效的HRegionServer并重新分配
4.HDFS上的垃圾文件回收
5.处理Schema更新请求
```



HRegionServer的作用

```
1.维护HMaster分配给它的HRegion，处理对这些HRegion的IO请求
2.负责切分正在运行过程中变得过大的HRegion可以看到，Client访问HBase上的数据并不需要HMaster参与，寻址访问ZooKeeper和HRegionServer，数据读写访问HRegionServer，HMaster仅仅维护Table和Region的元数据信息，Table的元数据信息保存在ZooKeeper上，负载很低。HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列簇创建一个Store对象，每个Store都会有一个MemStore和0或多个StoreFile与之对应，每个StoreFile都会对应一个HFile，HFile就是实际的存储文件。因此，一个HRegion有多少列簇就有多少个Store。
3 一个HRegionServer会有多个HRegion和一个HLog。
```



HRegion的作用

```
	Table在行的方向上分割为多个HRegion，HRegion是HBase中分布式存储和负载均衡的最小单元，即不同的HRegion可以分别在不同的HRegionServer上，但同一个HRegion是不会拆分到多个HRegionServer上的。HRegion按大小分割，每个表一般只有一个HRegion，随着数据不断插入表，HRegion不断增大，当HRegion的某个列簇达到一个阀值（默认256M）时就会分成两个新的HRegion。

1、<表名，StartRowKey, 创建时间>
2、由目录表(-ROOT-和.META.)记录该Region的EndRowKey
HRegion定位：HRegion被分配给哪个HRegionServer是完全动态的，所以需要机制来定位HRegion具体在哪个HRegionServer，HBase使用三层结构来定位HRegion：

1、通过zk里的文件/hbase/rs得到-ROOT-表的位置。-ROOT-表只有一个region。
2、通过-ROOT-表查找.META.表的第一个表中相应的HRegion位置。其实-ROOT-表是.META.表的第一个region；
.META.表中的每一个Region在-ROOT-表中都是一行记录。
3、通过.META.表找到所要的用户表HRegion的位置。用户表的每个HRegion在.META.表中都是一行记录。
-ROOT-表永远不会被分隔为多个HRegion，保证了最多需要三次跳转，就能定位到任意的region。Client会将查询的位置信息保存缓存起来，缓存不会主动失效，因此如果Client上的缓存全部失效，则需要进行6次网络来回，才能定位到正确的HRegion，其中三次用来发现缓存失效，另外三次用来获取位置信息。
```



Store相关

```
Store
每一个HRegion由一个或多个Store组成，至少是一个Store，HBase会把一起访问的数据放在一个Store里面，即为每个ColumnFamily建一个Store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个MemStore和0或者多个StoreFile组成。 HBase以Store的大小来判断是否需要切分HRegion。

MemStore
MemStore 是放在内存里的，保存修改的数据即keyValues。当MemStore的大小达到一个阀值（默认64MB）时，MemStore会被Flush到文件，
即生成一个快照。目前HBase会有一个线程来负责MemStore的Flush操作。

StoreFile
MemStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。
```



HFile

```
HBase中KeyValue数据的存储格式，是Hadoop的二进制格式文件。 首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。
```



### 3、hbase的优化 ###

写入数据方面

```
Auto Flash
通过调用HTable.setAutoFlushTo(false)方法可以将HTable写客户端自动flush关闭，这样可以批量写入数据到HBase，而不是有一条put就执行一次更新，只有当put填满客户端写缓存的时候，才会向HBase服务端发起写请求。默认情况下auto flush是开启的。

Write Buffer
通过调用HTable.setWriteBufferSize(writeBufferSize)方法可以设置HTable客户端的写buffer大小，如果新设置的buffer小于当前写buffer中的数据时，buffer将会被flush到服务端。其中，writeBufferSize的单位是byte字节数，可以根基实际写入数据量的多少来设置该值。

WAL Flag
在HBase中，客户端向集群中的RegionServer提交数据时（Put/Delete操作），首先会写到WAL（Write Ahead Log）日志，即HLog，一个RegionServer上的所有Region共享一个HLog，只有当WAL日志写成功后，再接着写MemStore，然后客户端被通知提交数据成功，如果写WAL日志失败，客户端被告知提交失败，这样做的好处是可以做到RegionServer宕机后的数据恢复。
对于不太重要的数据，可以在Put/Delete操作时，通过调用Put.setWriteToWAL(false)或Delete.setWriteToWAL(false)函数，放弃写WAL日志，以提高数据写入的性能。
注：如果关闭WAL日志，一旦RegionServer宕机，Put/Delete的数据将会无法根据WAL日志进行恢复。

Compression 压缩
数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，性价比最高。
HColumnDescriptor hcd = new HColumnDescriptor(familyName);   
hcd.setCompressionType(Algorithm.SNAPPY);  

批量写
通过调用HTable.put(Put)方法可以将一个指定的row key记录写入HBase，同样HBase提供了另一个方法：通过调用HTable.put(List<Put>)方法可以将指定的row key列表，批量写入多行记录，这样做的好处是批量执行，只需要一次网络I/O开销，这对于对数据实时性要求高，网络传输RTT高的情景下可能带来明显的性能提升。

多线程并发写
在客户端开启多个 HTable 写线程，每个写线程负责一个 HTable 对象的 flush 操作，这样结合定时 flush 和写 buffer（writeBufferSize），可以既保证在数据量小的时候，数据可以在较短时间内被 flush（如1秒内），同时又保证在数据量大的时候，写 buffer 一满就及时进行 flush。

```



读数据方面

```
批量读
通过调用 HTable.get(Get) 方法可以根据一个指定的 row key 获取一行记录，同样 HBase 提供了另一个方法：通过调用 HTable.get(List) 方法可以根据一个指定的 row key 列表，批量获取多行记录，这样做的好处是批量执行，只需要一次网络 I/O 开销，这对于对数据实时性要求高而且网络传输 RTT 高的情景下可能带来明显的性能提升。
缓存查询结果
对于频繁查询 HBase 的应用场景，可以考虑在应用程序中做缓存，当有新的查询请求时，首先在缓存中查找，如果存在则直接返回，不再查询 HBase；否则对 HBase 发起读请求查询，然后在应用程序中将查询结果缓存起来。至于缓存的替换策略，可以考虑 LRU 等常用的策略。
```



数据及集群管理

```
预分区
默认情况下，在创建HBase表的时候会自动创建一个Region分区，当导入数据的时候，所有的HBase客户端都向Region写数据，知道这个Region足够大才进行切分，一种可以加快批量写入速度的方法是通过预先创建一些空的Regions，这样当数据写入HBase的时候，会按照Region分区情况，在进群内做数据的负载均衡。
Rowkey优化
rowkey是按照字典存储，因此设置rowkey时，要充分利用排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放到一块。
rowkey若是递增生成的，建议不要使用正序直接写入，可以使用字符串反转方式写入，使得rowkey大致均衡分布，这样设计的好处是能将RegionServer的负载均衡，否则容易产生所有新数据都在集中在一个RegionServer上堆积的现象，这一点还可以结合table的与分区设计。
减少Column Family数量
不要在一张表中定义太多的column family。目前HBase并不能很好的处理超过2-3个column family的表，因为某个column family在flush的时候，它临近的column family也会因关联效应被触发flush，最终导致系统产生更过的I/O;
设置最大版本数
创建表的时候，可以通过 HColumnDescriptor.setMaxVersions(int maxVersions) 设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置 setMaxVersions(1)。
缓存策略（setCaching）
创建表的时候，可以通过HColumnDEscriptor.setInMemory(true)将表放到RegionServer的缓存中，保证在读取的时候被cache命中。
设置存储生命期
创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命周期，过期数据将自动被删除
磁盘配置
每台RegionServer管理10-1000个Regions。每个Region在1-2G，则每台server最少要10G，最大要1000*2G=2TB，考虑3备份，需要6TB。方案1是3块2TB磁盘，2是12块500G磁盘，带宽足够时，后者能提供更大的吞吐率，更细力度的冗余备份，更快速的单盘故障恢复。
分配何时的内存给RegionServer
在不影响其他服务的情况下，越大越好。在HBase的conf目录下的hbase-env.sh的最后添加export HBASE_REGIONSERVER_OPTS="- Xmx16000m $HBASE_REGIONSERVER_OPTS"
其中16000m为分配给REgionServer的内存大小。
写数据的备份数
备份数与读性能是成正比，与写性能成反比，且备份数影响高可用性。有两种配置方式，一种是将hdfs-site.xml拷贝到hbase的conf目录下，然后在其中添加或修改配置项dfs.replication的值为要设置的备份数，这种修改所有的HBase用户都生效。另一种方式是改写HBase代码，让HBase支持针对列族设置备份数，在创建表时，设置列族备份数，默认为3，此种备份数支队设置的列族生效。
客户端一次从服务器拉取的数量
通过配置一次拉取较大的数据量可以减少客户端获取数据的时间，但是他会占用客户端的内存，有三个地方可以进行配置

在HBase的conf配置文件中进行配置hbase.client.scanner.caching;
通过调用HTble.setScannerCaching(int scannerCaching)进行配置；
通过调用Sacn.setCaching(int caching)进行配置，三者的优先级越来越高。

客户端拉取的时候指定列族
scan是指定需要column family，可以减少网络传输数据量，否则默认scan操作会返回整行所有column family的数据
拉取完数据之后关闭ResultScanner
通过 scan 取完数据后，记得要关闭 ResultScanner，否则 RegionServer 可能会出现问题（对应的 Server 资源无法释放）。
RegionServer的请求处理IO线程数
较少的IO线程适用于处理单次请求内存消耗较高的Big Put场景（大容量单词Put或设置了较大cache的scan，均数据Big Put）或RegionServer的内存比较紧张的场景。
较多的IO线程，适用于单次请求内存消耗低，TPS要求（每次事务处理量）非常高的场景。这只该值的时候，以监控内存为主要参考
在hbase-site.xml配置文件中配置项为hbase.regionserver.handle.count
Region大小设置
配置项hbase.hregion.max.filesize，所属配置文件为hbase-site.xml，默认大小是256m。
在当前RegionServer上单个Region的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的Region。小Region对split和compaction友好，因为拆分Region或compact小Region里的StoreFile速度非常快，内存占用低。缺点是split和compaction会很频繁，特别是数量较多的小Region不同的split，compaction，会导致集群响应时间波动很大，Region数量太多不仅给管理上带来麻烦，设置会引起一些HBase个bug。一般 512M 以下的都算小 Region。大 Region 则不太适合经常 split 和 compaction，因为做一次 compact 和 split 会产生较长时间的停顿，对应用的读写性能冲击非常大。
此外，大 Region 意味着较大的 StoreFile，compaction 时对内存也是一个挑战。如果你的应用场景中，某个时间点的访问量较低，那么在此时做 compact 和 split，既能顺利完成 split 和 compaction，又能保证绝大多数时间平稳的读写性能。compaction 是无法避免的，split 可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如 100G，就可以间接禁用自动 split(RegionServer 不会对未到达 100G 的 Region 做 split)。再配合 RegionSplitter 这个工具，在需要 split 时，手动 split。手动 split 在灵活性和稳定性上比起自动 split 要高很多，而且管理成本增加不多，比较推荐 online 实时系统使用。内存方面，小 Region 在设置 memstore 的大小值上比较灵活，大 Region 则过大过小都不行，过大会导致 flush 时 app 的 IO wait 增高，过小则因 StoreFile 过多影响读性能。
```



### 4、hbase的集群搭建 ###

hbase集群搭建之前首先完成hadoop集群搭建

```
环境搭建
wget http://mirror.bit.edu.cn/apache/hbase/1.x.y/hbase-1.x.y-bin.tar.gz
#解压
tar -xzvf hbase-1.x.y-bin.tar.gz  -C /usr/local/
#重命名 
mv hbase-1.x.y hbase


配置环境变量vim /etc/profile
#内容
export HBASE_HOME=/usr/local/hbase
export PATH=$HBASE_HOME/bin:$PATH
#使立即生效
source /etc/profile


修改系统变量ulimit
ulimit -n 10240

配置文件
hbase 相关的配置主要包括hbase-env.sh、hbase-site.xml、regionservers三个文件，都在 /usr/local/hbase/conf目录下面：配置hbase-env.sh
vim hbase-env.sh
#内容
export JAVA_HOME=/usr/lib/jvm/jre-1.7.0-openjdk.x86_64
export HBASE_CLASSPATH=/usr/local/hbase/conf
# 此配置信息，设置由hbase自己管理zookeeper，不需要单独的zookeeper。
export HBASE_MANAGES_ZK=true
export HBASE_HOME=/usr/local/hbase
export HADOOP_HOME=/usr/local/hadoop
#Hbase日志目录
export HBASE_LOG_DIR=/usr/local/hbase/logs

配置 hbase-site.xml
<configuration>
	<property>
		<name>hbase.rootdir</name>
		<value>hdfs://hadoop-master:9000/hbase</value>
	</property>
	<property>
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>
	<property>
		<name>hbase.master</name>
		<value>hadoop-master:60000</value>
	</property>
	<property>
		<name>hbase.zookeeper.quorum</name>
		<value>hadoop-master,hadoop-slave1,hadoop-slave2,hadoop-slave3</value>
	</property>
</configuration>

配置regionservers
vim /usr/local/hbase/conf/regionservers
hadoop-master
hadoop-slave1
hadoop-slave2
hadoop-slave3

复制hbase到从节点中
scp -r /usr/local/hbase hadoop-slave1:/usr/local/
scp -r /usr/local/hbase hadoop-slave2:/usr/local/
scp -r /usr/local/hbase hadoop-slave3:/usr/local/

hbase启动
~/hbase/bin/start-hbase.sh
```



### 5、hbase的管理你有什么看法 ###

```
运维任务
regionserver添加/删除节点 master备份
1 添加新节点 
复制hbase目录并进行配置文件修改（regionserver增加新节点）并保持配置文件在全集群一致，在新节点上启动相关进程如hbase-daemon.sh start regionserver命令

2 删除节点
停止相关进程（例如regionserver hbase-daemon.sh stop regionserver），下线节点需注意
(2-1) 如果负载均衡进程正在执行，请先停止，因为其可能会和master转移region产生竞争
(2-2) 如果该节点上的数据量很大，移动region的过程可能很漫长

3 master备份
(3-1) 修改配置文件conf/backup-master文件
(3-2) 启动相关进程 hbase-daemon.sh start master

数据迁移
(1) 导入导出
HBase的jar包中包含了两个以MapReduce作业形式来导入导出数据的工具，如hadoop jar ${hbase.jar} export ${tablename} ${outputdir
该工具其余的参数可以做到增量导出、控制导出的数据版本等功能.

(2)

```



### 6、hbase的一些错误解决方案 ###

```

```



### 7、你用数仓呀，说一下数仓的你们的数据处理，数据流转 ###

```
具体数仓甚至数据治理方面可以参考下图
按照数仓分层思想，分为ods贴源层、dw主题层、mid维表层、dm集市层、app应该层
过程如下：
1 数据通过采集或同步落地基于HDFS存储的ods层
2 主题抽取确认
3 如果有此需求，构建基于主题数据的微聚合结果
4 构建维表层数据，如时间、地区、产品类别等数据
5 进行数据集市构建如统计结果、用户画像、TopN热门数据
6 进行集市数据的输出到app进行BI可视化展示
```

![](img\QF_数据治理.jpg)



### 8、你怎么看dws和dm ###

```
dws是基于主题数据做的微聚合，对下游的dm集市数据聚合起到提高计算效率的优化，另外对于其他如用户画像标签表可以做到数据复用的目的。
dm是集市数据层，主要是针对app应用数据层，包括了统计报表类的结果数据、用户标签表数据及TopN的热门数据（如商品、音乐、聊天话题等）
```



### 9、数据库中表在变化，因为是业务表，我现在需要对他进行一个查询(类似于产生报表)但是数据量巨大，有没有什么办法(可不可以用什么东西维护一份镜像做查询) ###

```

```



### 10、现在我有一个需求，数据库中有80张表，字段也很多，有million到billion级别的数据，数据库无法做复杂查询如group by key,distinct等，有什么解决方案 ###

```

```



### 11、你参与到了数仓的搭建吗？etl呢？谈谈你的理解 ###

```

```



### 12、数仓的DW层,面向什么主题的,在DW层数据做了什么处理,举一个例子 ###

```

```



### 13、数仓架构、怎么分层 ###

```

```



### 14、hive+hbase的双重查询了解吗? ###

```

```



### 15、数据都是Nginx发过来的吗?有其他的吗? ###

```

```



### 16、hbase数据如何基于hbase进行计算? ###

```

```



### 17、数仓中每一层到每一层的具体主题,具体表,具体模型都是怎么做的 ###

```

```



### 18、如果用Hbase进行实时计算，有没有什么思路 ###

```

```



### 19、如果用Hbase进行实时查询，有没有什么思路 ###

```

```



### 20、数仓数据导入到mysql怎么实现。mysql中有原来的数据，有些数据没有，有的会改变，怎么和新数据进行整合。 ###

```

```



### 21、缓慢变化维问题 ###
### 22、rowkey设计原理,hbase的架构,假设有一百万条数据怎么写入hbase中 ###
### 23、标签为什么存在hbase中 ###
### 24、介绍一下hbase的lsm树 ###
### 25、你们数仓是怎么规划的，分为那些主题，主题下又分为那些业务线，业务线又有那些事实表和维度表？数仓怎么分层的 ###
### 26、详细讲下DWD层数据模型，用的什么方法建模的 ###
### 27、数仓CIF架构和总线式的架构有什么区别 ###
### 28、用Kylin+Hbase做OLAP聚合分析，遇到最大的问题是什么？cube时候会产生数据膨胀怎么优化的 ###
### 30、设计一条完整的离线etl链路 ###

```

```



### 31、网络访问日志中储存了访问用户的ip和访问时间，当五分钟内访问次数超过100次时表示存在恶意访问，请找出恶意访问的用户ip（lag窗口函数） ###

```

```



### 32、什么是维度表？什么是事实表？ ###

```

```



### 33、维度表多少个？事实表多少个？ ###

```

```



# 第七章 flume #
### 1、flume的source的类型,flume是否高可用 ###
### 2、为什么用flume拉取日志，别的不行吗？ ###
### 3、为什么用flume从kafka拉取日志到HDFS。 ###
### 4、flume的官网看过吗?简单说一下 ###
### 5、Flume怎么sink到两个下沉地 ###
### 6、Flume拉取的文件有多少个 ###
### 7、flume如何配置 ###

### 8、两个数据库同步，中间用flume行不行

### 9、flume获取数据是用什么方式

### 10、你的flume源码是如何修改的？



# 第八章	spark相关 #
### 1、spark core运行的原理 ###
### 2、spark sql运行原理 ###
### 3、spark版本2.x 与1.6的区别 ###

### 4、ds的特点 和df的区别 ###
### 5、hashmanager和sortedmanager的区别 为什么性能好 钨丝为啥好  ###
### 6、说一下spark_sql和hive区别	 ###
### 7、说一下常见的spark调优策略？ ###
### 8、在实际操作spark的时候，自己调节过的关于内存的参数有哪些？ ###
### 9、 数据倾斜是问题吗？这种就不叫问题，写在简历上有啥用？既然你写了，说出数据倾斜的解决方案你了解多少？ ###
### 10、spark session的accumulator为什么要用字符串 ###
### 11、task信息怎么从前台传到spark的 ###
### 12、用sparkcore,不用sparksql怎么写入数据库 ###
### 13、spark on yarn的两种模式的区别 ###
### 14、spark sql中缓存方式有哪几种 ###
### 15、registerTempTable是action类型的么,发生不发生缓存 ###
### 16、groupByKey,reduceByKey和combineByKey的区别 ###
### 17、foreach和foreachPartition的区别 ###
### 18、map与mapPartitions的区别 ###
### 19、spark core往mysql中写数据是在哪个算子里进行操作的（foreachPartition） ###
### 20、spark造数据倾斜的原因，怎么去查看哪个executor数据倾斜？executor分配数据的默认分配逻辑是什么？ ###
### 21、SparkStreaming一个批次有多久？一个批次有多少条数据？

关于批次间隔需要结合业务来确定的，如果实时性要求高，批次间隔需要调小。

每个批次的数据量是和每天产生的数据量有直接关系，在计算的时候需要考虑峰值的情况。需要注意的是，批次间隔越长，每个批次计算的数据量会越多。



### 22、SparkStreaming消费速度赶不上生产速度怎么办？

在默认情况下，Spark Streaming 通过receiver或者Direct方式以生产者生产数据的速率接收数据。当 batch processing time > batch interval 的时候，也就是每个批次数据处理的时间要比 Spark Streaming 批处理间隔时间长。越来越多的数据被接收，但是数据的处理速度没有跟上，导致系统开始出现数据堆积，可能进一步导致 Executor 端出现 OOM 问题而出现失败的情况。

Spark Streaming 1.5 之后的体系结构：

![](img/ratecontroller1.png)

● 为了实现自动调节数据的传输速率，在原有的架构上新增了一个名为 RateController 的组件，这个组件继承自 StreamingListener，其监听所有作业的 onBatchCompleted 事件，并且基于 processingDelay、schedulingDelay 、当前 Batch 处理的记录条数以及处理完成事件来估算出一个速率；这个速率主要用于更新流每秒能够处理的最大记录的条数。速率估算器（RateEstimator）可以又多种实现，不过目前的 Spark 2.2 只实现了基于 PID 的速率估算器。
● InputDStreams 内部的 RateController 里面会存下计算好的最大速率，这个速率会在处理完 onBatchCompleted 事件之后将计算好的速率推送到 ReceiverSupervisorImpl，这样接收器就知道下一步应该接收多少数据了。
● 如果用户配置了 spark.streaming.receiver.maxRate 或 spark.streaming.kafka.maxRatePerPartition，那么最后到底接收多少数据取决于三者的最小值。也就是说每个接收器或者每个 Kafka 分区每秒处理的数据不会超过 spark.streaming.receiver.maxRate 或 spark.streaming.kafka.maxRatePerPartition 的值。
详细的过程如下图所示：
![](img/receiverratecontroller.png)

如何启用？
在 Spark 启用反压机制很简单，只需要将 spark.streaming.backpressure.enabled 设置为 true 即可，这个参数的默认值为 false。反压机制还涉及以下几个参数，包括文档中没有列出来的：
● spark.streaming.backpressure.initialRate： 启用反压机制时每个接收器接收第一批数据的初始最大速率。默认值没有设置。
● spark.streaming.backpressure.rateEstimator：速率估算器类，默认值为 pid ，目前 Spark 只支持这个，大家可以根据自己的需要实现。
● spark.streaming.backpressure.pid.proportional：用于响应错误的权重（最后批次和当前批次之间的更改）。默认值为1，只能设置成非负值。weight for response to “error” (change between last batch and this batch)
● spark.streaming.backpressure.pid.integral：错误积累的响应权重，具有抑制作用（有效阻尼）。默认值为 0.2 ，只能设置成非负值。weight for the response to the accumulation of error. This has a dampening effect.
● spark.streaming.backpressure.pid.derived：对错误趋势的响应权重。 这可能会引起 batch size 的波动，可以帮助快速增加/减少容量。默认值为0，只能设置成非负值。weight for the response to the trend in error. This can cause arbitrary/noise-induced fluctuations in batch size, but can also help react quickly to increased/reduced capacity.
● spark.streaming.backpressure.pid.minRate：可以估算的最低费率是多少。默认值为 100，只能设置成非负值。

以上为Spark的反压机制，再结合Spark资源的动态调整（在下面的题中有详细解释），就是该问题的完整解决方案。



### 23、SparkStreaming的批次间隔，处理完的的数据存在哪里

批次间隔为SparkStreaming处理实时需求的时间间隔，需要根据业务需求来确定批次间隔。

实时需求的处理结果一般是保存在能快速读取的数据库中来提高效率，比如Redis、MongoDB、HBase。



### 24、Spark Shuffle和MR Shuffle的区别

#### MR的Shuffle 

Map Shuffle：
数据存到hdfs中是以块进行存储的，每一个块对应一个分片，maptask就是从分片中获取数据的 
在某个节点上启动了map Task,map Task读取是通过k-v来读取的,读取的数据会放到环形缓存区，这样做的目的是为了防止IO的访问次数,然后环形缓存区的内存达到一定的阀值的 
时候会把文件益写到磁盘，溢出的各种小文件会合并成一个大文件，这个合并的过程中会进行排序，这个排序叫做归并排序 
map阶段会涉及到 
1.sort排序(默认按字典排序) 
2.合并(combiner合并) 
3.文件合并(merage 合并 总共有三种，默认是内存到磁盘) 
4.压缩（设置压缩就会执行） 
Reduce Shuffle：
归并排序完成后reduce端会拉取map端的数据，拉取的这个过程叫做copy过程，拉取的数据合并成一个文件，GroupComparator(默认,这个我们也可以自定义)是专门对文件夹里面的key进行分组 
然后就形成k-List(v1,v2,v3)的形式，然后reduce经过业务处理，最终输出到hdfs，如果设置压缩就会执行，不设置则不执行 reduce阶段会涉及到： 
1.sort排序 
2.分组（将相同的key的value放到一个容器的过程） 
3.merge文件合并
4.压缩

#### Spark shuffle

Spark的Shuffle过程从2.0版本开始用Sorted-Based Shuffle，优点是通过排序建立索引，相比较2.0之前的Shuffle方式，它只有一个临时文件，不管有多少个ResultTask都只有一个临时文件， 缺点是这个排序操作是一个消耗CPU的操作，代价是会消耗很多的cpu 。

Spark Shuffle是跨Stage的，MapTask阶段属于Shuffle Write过程，ReduceTask阶段属于Shuffle Read过程。

shuffle的读流程 ：
1.有一个类BlockManager，封装了临时文件的位置信息，ResultTask先通过BlockManager,就知道我从哪个节点去拿数据 。如果是远程，它就是发起一次socket请求，创建一个socket链接。然后发起一次远程调用，告诉远程的读取程序，读取哪些数据。读到的内容再通过socket传过来。 
2.一条条读数据和一块块读数据的优缺点？ 
如果是一条条读取的话，实时性好，性能低下，一块块读取的话性能高，但是实时性不好。

Shuffle读由reduce这边发起，它需要先到临时文件中读，一般这个临时文件和reduce不在一台节点上，它需要跨网络去读。但也不排除在一台服务器。不论如何它需要知道临时文件的位置， 这个是谁来告诉它的呢？它有一个BlockManager的类。这里就知道将来是从本地文件中读取，还是需要从远程服务器上读取。 读进来后再做join或者combine的运算。 这些临时文件的位置就记录在Map结构中。 
可以这样理解分区partition是RDD存储数据的地方，实际是个逻辑单位，真正要取数据时，它就调BlockManage去读，它是以数据块的方式来读。 比如一次读取32k还是64k。它不是一条一条读，一条一条读肯定性能低。它读时首先是看本地还是远程，如果是本地就直接读这个文件了， 如果是远程，它就是发起一次socket请求，创建一个socket链接。然后发起一次远程调用，告诉远程的读取程序，读取哪些数据。读到的内容再通过socket传过来。



### 25、宽依赖和窄依赖

Spark的RDD之间是有依赖关系的，分宽依赖和窄依赖。

宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition。

窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。

RDD有了依赖关系，才使得RDD才有容错性，才可以进行Stage划分。



### 26、Spark-Submit底层源码了解多少

Spark的作业提交过程，需要按照四个阶段进行描述，即DAG的构建、Stage划分、TaskSet的调度、Task的执行。

详细描述情况按照课堂上讲的过程结合源码进行里的重要的对象和方法进行描述。

![](img/Spark任务生成和提交过程.png)



### 27、Spark里kyro序列化了解多少

Spark的序列化 默认为org.apache.spark.serializer.JavaSerializer,可选org.apache.spark.serializer.KryoSerializer，实际上只要是org.apache.spark.serializer的子类就可以了，不过如果只是应用，大概你不会自己去实现一个的。

序列化对于spark应用的性能来说,还是有很大影响的，在特定的数据格式的情况下，KryoSerializer的性能可以达到JavaSerializer的10倍以上，当然放到整个Spark程序中来考量，比重就没有那么大了，但是以Wordcount为例，通常也很容易达到30%以上的性能提升。而对于一些Int之类的基本类型数据，性能的提升就几乎可以忽略了。KryoSerializer依赖Twitter的Chill库来实现，相对于JavaSerializer，主要的问题在于不是所有的Java Serializable对象都能支持。

需要注意的是，这里可配的Serializer针对的对象是Shuffle数据，以及RDD Cache等场合，而Spark Task的序列化是通过spark.closure.serializer来配置，但是目前只支持JavaSerializer。



### 28、Spark Streaming的窗口大小，每个窗口处理的数据量大小。

该问题一定要根据业务需求来确定，比如要实现的需求为：统计每分钟的前一个小时的在线人数。

上面需求的窗口大小（窗口长度）为1小时，然后再统计每个窗口需要处理的数据量。

窗口处理的数据量 = 每个批次处理的平均数据量 * 窗口的批次数量

### 29、Spark中Job，Stage，Task，Executor的理解

**Job：**一个应用程序中可能有一个或多个Job，一个Action算子会生成一个Job，一个Job会提交一次作业。而且一个应用程序中后面的Job可以拿取前面Job的RDD的结果进行下一步的统计。

**Stage：**为了生成最终的Task，Spark需要将整个依赖链条按照是否发生宽依赖来确定一个或多个范围，即Stage。Stage分两种，即ShuffleMapStage和ResultStage。

**Task：**Spark最终的执行单元，也就是最终计算的过程是执行的Task，一个线程对应一个Task，每个Task对应一个分区进行计算。

**Executor：**全名叫CoarseGrainedExecutorBackend，属于Worker的子进程，真正参与计算的进程。Executor需要一定的资源进行计算，Standalone模式是由Worker来分配的资源，On Yarn模式是由AppMaster申请的资源。每个Worker可以启动一个或多个Executor进行计算。



### 30、Spark Streaming中的updateStateByKey和mapWithState的区别和使用。

**UpdateStateByKey：**统计全局的key的状态，但是就算没有数据输入，他也会在每一个批次的时候返回之前的key的状态。这样的缺点就是，如果数据量太大的话，而且我们需要checkpoint数据，这样会占用较大的存储。

如果要使用updateStateByKey,就需要设置一个checkpoint目录（updateStateByKey自己是无法保存key的状态的），开启checkpoint机制。因为key的state是在内存维护的，如果宕机，则重启之后之前维护的状态就没有了，所以要长期保存它的话需要启用checkpoint，以便恢复数据。

实现案例：

```scala
object LoadKafkaDataDemo {
  def main(args: Array[String]): Unit = {
    val checkpointDir = "d://cp-20190810-2"
    val ssc = StreamingContext.getOrCreate(checkpointDir, () => createContext)
    ssc.start()
    ssc.awaitTermination()
  }

  /**
    * 该方法包含主要计算逻辑，返回StreamingContext
    */
  def createContext = {
    // 创建上下文
    val conf = new SparkConf().setAppName("LoadKafkaDataDemo").setMaster("local[2]")
    val ssc = new StreamingContext(conf, Milliseconds(5000))
    ssc.checkpoint("hdfs://out-20190810-3")

    // 指定请求kafka的配置信息
    val kafkaParam = Map[String, Object](
      "bootstrap.servers" -> "node01:9092,node02:9092,node03:9092",
      // 指定key的反序列化方式
      "key.deserializer" -> classOf[StringDeserializer],
      // 指定value的反序列化方式
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "group1",
      // 指定消费位置
      "auto.offset.reset" -> "latest",
      // 如果value合法，自动提交offset
      "enable.auto.commit" -> (true: java.lang.Boolean)
    )
    val topics = Array("test1")
    // 消费数据
    val logs: InputDStream[ConsumerRecord[String, String]] =
      KafkaUtils.createDirectStream(
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe(topics, kafkaParam)
    )

    // 对消费的数据做单词计数
    // 其中key的数据不需要，仅仅留下value，因为value是实际的log日志数据
    val lines: DStream[String] = logs.map(_.value())
    val tups: DStream[(String, Int)] = lines.flatMap(_.split(" ")).map((_, 1))
    val res = tups.updateStateByKey(func, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)

    res.print
    ssc
  }
  val func = (it: Iterator[(String, Seq[Int], Option[Int])]) => {
    it.map{
      case (a,b,c) => {
        (a, b.sum + c.getOrElse(0))
      }
    }
  }
}
```

**MapWithState：**也是用于全局统计key的状态，但是它如果没有数据输入，便不会返回之前的key的状态，有一点增量的感觉。

这样做的好处是，我们可以只关心那些已经发生变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储。

实现案例：

```scala
object MapWithState {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
      .setAppName("StreamingMapWithState")
      .setMaster("local[2]")
    val sc = SparkContext.getOrCreate(conf)
    val ssc = new StreamingContext(sc, Seconds(2))
    // 当调用updateStateByKey函数API的时候，必须给定checkpoint dir
    // 路径对应的文件夹不能存在
    ssc.checkpoint("hdfs://out-20190810-3")

    /**
      * key    DStream的key数据类型
      * values DStream的value数据类型
      * state  是StreamingContext中之前该key的状态值
      */
    val mappingFunction = (key: String, values: Option[Int], state: State[Long]) => {
      // 获取之前状态的值
      val preStateValue = state.getOption().getOrElse(0L)
      // 计算出当前值
      val currentStateValue = preStateValue + values.getOrElse(0)
      // 更新状态值
      state.update(currentStateValue)
      // 返回结果
      (key, currentStateValue)
    }
    val spec = StateSpec.function[String, Int, Long, (String, Long)](mappingFunction)

    // 指定请求kafka的配置信息
    val kafkaParam = Map[String, Object](
      "bootstrap.servers" -> "node01:9092,node02:9092,node03:9092",
      // 指定key的反序列化方式
      "key.deserializer" -> classOf[StringDeserializer],
      // 指定value的反序列化方式
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "group1",
      // 指定消费位置
      "auto.offset.reset" -> "latest",
      // 如果value合法，自动提交offset
      "enable.auto.commit" -> (true: java.lang.Boolean)
    )
    // 指定topic
    val topics = Array("test1")

    // 消费数据
    val logs: DStream[String] = KafkaUtils.createDirectStream(
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe(topics, kafkaParam)
    ).map(_.value)

    val resultWordCount: DStream[(String, Long)] = logs
      .filter(line => line.nonEmpty)
      .flatMap(line => line.split(" ").map((_, 1)))
      .reduceByKey(_ + _)
      .mapWithState(spec)

    resultWordCount.print()

    // 启动开始处理
    ssc.start()
    ssc.awaitTermination()
  }
}
```



### 31、线上Spark Job如何停止？

1. SparkContext提供了一个取消job的api

```scala
class SparkContext(config: SparkConf) extends Logging with ExecutorAllocationClient {
/** Cancel a given job if it's scheduled or running */
private[spark] def cancelJob(jobId: Int) {
  dagScheduler.cancelJob(jobId)
}
```

2. 那么如何获取jobId呢？

Spark提供了一个叫SparkListener的对象，它提供了对spark事件的监听功能

```scala
trait SparkListener {
  /**
   * Called when a job starts
   */
  def onJobStart(jobStart: SparkListenerJobStart) { }
 
  /**
   * Called when a job ends
   */
  def onJobEnd(jobEnd: SparkListenerJobEnd) { }
}
```

因此需要自定义一个类，继承自SparkListener，即：

```scala
public class DHSparkListener implements SparkListener {
private static Logger logger = Logger.getLogger(DHSparkListener.class);
//存储了提交job的线程局部变量和job的映射关系
    private static ConcurrentHashMap<String, Integer> jobInfoMap;
    public DHSparkListener() {
        jobInfoMap = new ConcurrentHashMap<String, Integer>();
    }
    @Override
    public void onJobEnd(SparkListenerJobEnd jobEnd) {
        logger.info("DHSparkListener Job End:" + jobEnd.jobResult().getClass() + ",Id:" + jobEnd.jobId());
        for (String key : jobInfoMap.keySet()) {
            if (jobInfoMap.get(key) == jobEnd.jobId()) {
                jobInfoMap.remove(key);
                logger.info(key+" request has been returned. because "+jobEnd.jobResult().getClass());
            }
        }
    }
    @Override
    public void onJobStart(SparkListenerJobStart jobStart) {
        logger.info("DHSparkListener Job Start: JobId->" + jobStart.jobId());
//根据线程变量属性找到该job是哪个线程提交的
        logger.info("DHSparkListener Job Start: Thread->" + jobStart.properties().getProperty("thread", "default"));
        jobInfoMap.put(jobStart.properties().getProperty("thread", "default"), jobStart.jobId());
    }
……
}
```

那么用户如何知道该job是哪个线程提交的呢？需要在提交job的时候设置线程局部变量属性，如下：

```scala
SparkConf conf = new SparkConf().setAppName("SparkListenerTest application in Java");
        String sparkMaster = Configure.instance.get("SparkMaster");
        String sparkExecutorMemory = "16g";
        String sparkCoresMax = "4";
        String sparkJarAddress = "/tmp/cuckoo-core-1.0-SNAPSHOT-allinone.jar";
        conf.setMaster(sparkMaster);
        conf.set("spark.executor.memory", sparkExecutorMemory);
        conf.set("spark.cores.max", sparkCoresMax);
        JavaSparkContext jsc = new JavaSparkContext(conf);
        jsc.addJar(sparkJarAddress);
        DHSparkListener dHSparkListener = new DHSparkListener();
        jsc.sc().addSparkListener(dHSparkListener);
        List<Integer> listData = new ArrayList<Integer>();
        listData = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);
        JavaRDD<Integer> rdd1 = jsc.parallelize(listData, 1);
JavaRDD<Integer> rdd2 = rdd1.map(new Function<Integer, Integer>() {
            public Integer call(Integer v1) throws Exception {
              //do something then return
            }
        });
<pre name="code" class="plain">       //在触发action提交job之前设置提交线程的局部属性，供SparkListener获取
       jsc.setLocalProperty("thread", "client");
       rdd2.count();
```

这样在jobInfoMap中记录了job和job提交者的映射关系，当发现某个job迟迟没有结束的时候，可以调用SparkContext的cancelJob取消，但是仅仅到这里就够了吗？接着往下看，excutor取消job最终调用的是：

```scala
def kill(interruptThread: Boolean) {
  _killed = true
  if (context != null) {
    context.markInterrupted()
  }
  if (interruptThread && taskThread != null) {
    taskThread.interrupt()
  }
}
```

最终调用到Thread.interrupt函数，给启动task的线程设置interrupt标记位，因此在长时间允许的task中，需要针对Thread的interrupt标记位进行判断，当被置位的时候，需要退出，并且做一些清理，即存在类似的代码段：

```scala
if(Thread.interrupted()){
    //……线程被中断，清理资源
}
// 或者调用sleep，wait函数时会抛出InterruptedException异常，需要进行捕获，然后做对应的处理
```

3. 最后一步，配置job kill的动作

除了以上操作之外，还需要再配置针对每个job调用kill的动作，即spark.job.interruptOnCancel属性为true 

```scala
//在触发action提交job之前设置提交线程的局部属性，供SparkListener获取
jsc.setLocalProperty("thread", "client");
//配置该job接受到kill之后的动作，即task线程收到interrupt信号
jsc.setLocalProperty("spark.job.interruptOnCancel", "true");
rdd2.count();
```



### 32、工作中对Spark Job 7*24持续稳定运行的优化方案，至少说五点。

参照Spark调优文档，按照该文档的调优顺序进行叙述。



### 33、MySQL的数据如何被Spark Streaming消费，假如：MySQL中用户名为张三，Spark已经消费了，但是此时我的名字改为了张小三，怎么办？如何同步？

Spark Streaming是批处理，每个批次的计算方式都是从MySQL中消费到数据进行统计，得到结果后会紧接着将结果持久化到对应的数据库，此时如果MySQL的某个字段值更新了，更新的值是无法影响以前批次的Streaming的结果的，只能影响以后批次的结果。除非是将之前的结果覆盖操作。



### 34、Spark的数据放到Redis中代码是什么?

```scala
object SaveData2RedisDemo {
  def main(args: Array[String]): Unit = {
	val conf = new SparkConf().setMaster("local[*]")
      .setAppName("SaveData2RedisDemo")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    val sc = new SparkContext(conf)

    // 此处为业务逻辑 。。。。
      
    resRDD.foreachPartition(it => {
        val jedis = JedisConnectionPool.getConnection()
        it.foreach(t => {
          jedis.set(t._1, t._2)
        })
        jedis.close()
      })

    sc.stop()
  }
}

/**
  * 获取Redis连接
  */
object JedisConnectionPool{
  val conf = new JedisPoolConfig()
  //最大连接数,
  conf.setMaxTotal(20)
  //最大空闲连接数
  conf.setMaxIdle(10)
  //当调用borrow Object方法时，是否进行有效性检查 -->
  conf.setTestOnBorrow(true)
  //10000代表超时时间（10秒）
  val pool = new JedisPool(conf, "node02", 6379, 10000)

  def getConnection(): Jedis = {
    pool.getResource
  }
}
```



### 35、Spark如何连接到HBase中,代码是什么?

```scala
    // 配置HBASE的基本信息
    val load = ConfigFactory.load()
    val hbaseTableName = load.getString("hbase.table.name")
    // 配置HBASE的连接
    val configuration = sc.hadoopConfiguration
    configuration.set("hbase.zookeeper.quorum",
      load.getString("hbase.zookeeper.host"))
    val hbConn = ConnectionFactory.createConnection(configuration)
    //获得操作对象
    val hbadmin = hbConn.getAdmin
    if (!hbadmin.tableExists(TableName.valueOf(hbaseTableName))) {
      // 创建表对象
      val tableDescriptor = new HTableDescriptor(TableName.valueOf(hbaseTableName))
      //创建一个列簇
      val columnDescriptor = new HColumnDescriptor("tags")
      //将列簇放入到表中
      tableDescriptor.addFamily(columnDescriptor)
      hbadmin.createTable(tableDescriptor)
      hbadmin.close()
      hbConn.close()
    }
    // 创建Job对象
    val jobConf = new JobConf(configuration)
    // 指定输出类型
    jobConf.setOutputFormat(classOf[TableOutputFormat])
    // 指定输出到哪张表中
    jobConf.set(TableOutputFormat.OUTPUT_TABLE, hbaseTableName)

	// 此处是app的业务逻辑 。。。。

	// 写入到HBASE中
    res.map {
      case (userid, userTags) => {
        val put = new Put(Bytes.toBytes(userid))
        val tags = userTags.map(t => t._1 + ":" + t._2).mkString(",")
        put.addImmutable(Bytes.toBytes("tags"),
          Bytes.toBytes(s"$day"), Bytes.toBytes(tags))
        (new ImmutableBytesWritable(), put)
      }
    }.saveAsHadoopDataset(jobConf)
```



### 36、RDD与DataFrame区别，什么场景用RDD什么场景用DataFrame？

**区别：**

RDD是分布式的不可变的抽象的数据集，比如，RDD[Person]是以Person为类型参数，但是，Person类的内部结构对于RDD而言却是不可知的。DataFrame是以RDD为基础的分布式的抽象数据集，也就是分布式的Row类型的集合（每个Row对象代表一行记录），提供了详细的结构信息，即Schema信息。Spark SQL可以清楚地知道该数据集中包含哪些列、每列的名称和类型。

**应用场景：**

RDD的使用场景：

```
你需要使用low-level的transformation和action来控制你的数据集；
你得数据集非结构化，比如，流媒体或者文本流；
你想使用函数式编程来操作你得数据，而不是用特定领域语言（DSL）表达；
你不在乎schema，比如，当通过名字或者列处理（或访问）数据属性不在意列式存储格式；
你放弃使用DataFrame和Dataset来优化结构化和半结构化数据集；
```

DataFrame的使用场景：

```
你想使用丰富的语义，high-level抽象，和特定领域语言API，那你可DataFrame或者Dataset；
你处理的半结构化数据集需要high-level表达， filter，map，aggregation，average，sum ，SQL 查询，列式访问和使用lambda函数，那你可DataFrame或者Dataset；
你想利用编译时高度的type-safety，Catalyst优化和Tungsten的code生成，那你可DataFrame或者Dataset；
你想统一和简化API使用跨Spark的Library，那你可DataFrame或者Dataset；
如果你是一个R使用者，那你可DataFrame或者Dataset；
如果你是一个Python使用者，那你可DataFrame或者Dataset；
```



### 37、Spark可以用什么作业提交方式？

提交方式有：

Local模式（一般测试用）

Standalone模式（Spark自带的集群模式）

Spark-On-Yarn模式（将Spark App提交到Yarn上执行，较常用）

Mesos（Apache的一个独立的资源调度，不常用）



### 38、Spark作业提交时如果出现依赖冲突怎么解决？

当用户应用与 Spark 本身依赖同一个库时可能会发生依赖冲突，导致程序崩溃。这种情况 不是很常见，但是出现的时候也让人很头疼。

通常，依赖冲突表现为 Spark 作业执行过 程中抛出 NoSuchMethodError、ClassNotFoundException，或其他与类加载相关的 JVM 异 常。

对于这种问题，主要有两种解决方式:

一是修改你的应用，使其使用的依赖库的版本 与 Spark 所使用的相同。

二是使用通常被称为“shading”的方式打包你的应用。

Maven 构 建工具通过使用shading插件(事实上，shading 的功能也是这个插件取名为 maven- shade-plugin 的原因)进行高级配置来支持这种打包方式。shading 可以让你以另一个命名 空间保留冲突的包，并自动重写应用的代码使得它们使用重命名后的版本。这种技术有些 简单粗暴，不过对于解决运行时依赖冲突的问题非常有效。

```scala
<build>
    <plugins>
		<!-- 用来创建超级JAR包的Maven shade插件 -->
		<plugin>
			<groupId>org.apache.maven.plugins</groupId>
			<artifactId>maven-shade-plugin</artifactId>        
			<version>2.3</version>
			<executions>
				<execution>
					<phase>package</phase>
					<goals>
						<goal>shade</goal>
					</goals>
				</execution>
			</executions>
		</plugin>
	</plugins> 
</build>
```



### 39、叙述Spark的资源动态调整方式。

**为什么要动态资源分配和动态控制速率呢？**

默认情况下，Spark是先分配好资源，然后在进行计算，也就是粗粒度的资源分配 。
粗粒度的好处：资源是提前给分配好的，所以计算任务的时候，直接使用这些资源 。
粗粒度的缺点：从Spark Streaming的角度讲，有高峰值和低峰值，高峰和低峰的时候，需要的资源是不一样的；如果按照高峰值的角度去分配，低峰值的时候，有大量的资源的浪费。

资源的动态分配是由一个定时器，不断的扫描Executor的情况，例如：有段时间之内，Executor没收到任何任务，所以会把这个Executor移除掉。

动态资源调整的时候，最好不要设置太多的Core，Core设置的太多，假如资源调整太过频繁的话，是比较麻烦的(Core 的个数一般设置为奇数：3、5、7)。

Spark Streaming要进行处理资源的动态调整，就是Executor的动态调整 。
Spark Streaming是Batch Duration的方式执行的，这个Batch Duration里需要很多资源，下一个Batch Duration里不需要那么多资源，可能想调整资源的时候，还没来得及调整完资源，当前的这个Batch Duration的运行已过期的情况，这个时候的资源调整就是浪费的。

**资源动态申请**

spark streaming是基于spark core的，spark core也支持资源动态分配。可以在spark.dynamicAllocation.enabled中进行配置是否开启动态分配。如果开启就new ExecutorAllocationManager。

通过配置参数：spark.dynamicAllocation.enabled看是否需要开启Executor的动态分配。

你可以在程序运行时不断设置spark.dynamicAllocation.enabled参数的值，如果支持资源动态分配的话就使用ExecutorAllocationManager类。

### 40、叙述Spark的参数配置。

Spark应用程序的运行是通过外部参数来控制的，参数的设置是否合适直接影响应用程序的性能，最终影响应用程序的运行效率。

#### Spark的参数配置方式有3种：

（1）直接设置在SparkConf，通过参数的形式传递给SparkContext，达到控制目的。

（2）动态加载Spark属性。为了应用程序的属性不硬编码，灵活应用。可以这样处理：val sc = new SparkContext(new SparkConf())，通过 spark-submit时添加必要的参数。

（3）在conf/spark-defaults.conf 定义必要的属性参数，Spark在启动时，SparkContext会自动加载此配置文件的属性。

注：Spark加载属性参数的优先顺序也是以上的顺序。

#### Spark的参数配置

##### schedule调度相关

调度相关的参数设置，大多数内容都很直白，其实无须过多的额外解释，不过基于这些参数的常用性（大概会是你针对自己的集群第一步就会配置的参数），这里多少就其内部机制做一些解释。

**spark.cores.max**

一个集群最重要的参数之一，当然就是CPU计算资源的数量。spark.cores.max这个参数决定了在Standalone和Mesos模式下，一个Spark应用程序所能申请的CPU Core的数量。如果你没有并发跑多个Spark应用程序的需求，那么可以不需要设置这个参数，默认会使用spark.deploy.defaultCores的值（而spark.deploy.defaultCores的值默认为Int.Max，也就是不限制的意思）从而应用程序可以使用所有当前可以获得的CPU资源。

针对这个参数需要注意的是，这个参数对Yarn模式不起作用，YARN模式下，资源由Yarn统一调度管理，一个应用启动时所申请的CPU资源的数量由另外两个直接配置Executor的数量和每个Executor中core数量的参数决定。（历史原因造成，不同运行模式下的一些启动参数个人认为还有待进一步整合）

此外，在Standalone模式等后台分配CPU资源时，目前的实现中，在spark.cores.max允许的范围内，基本上是优先从每个Worker中申请所能得到的最大数量的CPU core给每个Executor，因此如果人工限制了所申请的Max Core的数量小于Standalone和Mesos模式所管理的CPU数量，可能发生应用只运行在集群中部分节点上的情况（因为部分节点所能提供的最大CPU资源数量已经满足应用的要求），而不是平均分布在集群中。通常这不会是太大的问题，但是如果涉及数据本地性的场合，有可能就会带来一定的必须进行远程数据读取的情况发生。理论上，这个问题可以通过两种途径解决：一是Standalone和Mesos的资源管理模块自动根据节点资源情况，均匀分配和启动Executor，二是和Yarn模式一样，允许用户指定和限制每个Executor的Core的数量。社区中有一个PR试图走第二种途径来解决类似的问题，不过截至我写下这篇文档为止（2014.8），还没有被Merge。

**spark.task.cpus**

这个参数在字面上的意思就是分配给每个任务的CPU的数量，默认为1。实际上，这个参数并不能真的控制每个任务实际运行时所使用的CPU的数量，比如你可以通过在任务内部创建额外的工作线程来使用更多的CPU（至少目前为止，将来任务的执行环境是否能通过LXC等技术来控制还不好说）。它所发挥的作用，只是在作业调度时，每分配出一个任务时，对已使用的CPU资源进行计数。也就是说只是理论上用来统计资源的使用情况，便于安排调度。因此，如果你期望通过修改这个参数来加快任务的运行，那还是赶紧换个思路吧。这个参数的意义，个人觉得还是在你真的在任务内部自己通过任何手段，占用了更多的CPU资源时，让调度行为更加准确的一个辅助手段。

**spark.scheduler.mode**

这个参数决定了单个Spark应用内部调度的时候使用FIFO模式还是Fair模式。是的，你没有看错，这个参数只管理一个Spark应用内部的多个没有依赖关系的Job作业的调度策略。

如果你需要的是多个Spark应用之间的调度策略，那么在Standalone模式下，这取决于每个应用所申请和获得的CPU资源的数量（暂时没有获得资源的应用就Pending在那里了），基本上就是FIFO形式的，谁先申请和获得资源，谁就占用资源直到完成。而在Yarn模式下，则多个Spark应用间的调度策略由Yarn自己的策略配置文件所决定。

那么这个内部的调度逻辑有什么用呢？如果你的Spark应用是通过服务的形式，为多个用户提交作业的话，那么可以通过配置Fair模式相关参数来调整不同用户作业的调度和资源分配优先级。

**spark.locality.wait**

spark.locality.wait和spark.locality.wait.process，spark.locality.wait.node,spark.locality.wait.rack这几个参数影响了任务分配时的本地性策略的相关细节。

Spark中任务的处理需要考虑所涉及的数据的本地性的场合，基本就两种，一是数据的来源是HadoopRDD;二是RDD的数据来源来自于RDD Cache（即由CacheManager从BlockManager中读取，或者Streaming数据源RDD）。其它情况下，如果不涉及shuffle操作的RDD，不构成划分Stage和Task的基准，不存在判断Locality本地性的问题，而如果是ShuffleRDD，其本地性始终为No Prefer，因此其实也无所谓Locality。

在理想的情况下，任务当然是分配在可以从本地读取数据的节点上时（同一个JVM内部或同一台物理机器内部）的运行时性能最佳。但是每个任务的执行速度无法准确估计，所以很难在事先获得全局最优的执行策略，当Spark应用得到一个计算资源的时候，如果没有可以满足最佳本地性需求的任务可以运行时，是退而求其次，运行一个本地性条件稍差一点的任务呢，还是继续等待下一个可用的计算资源已期望它能更好的匹配任务的本地性呢？

这几个参数一起决定了Spark任务调度在得到分配任务时，选择暂时不分配任务，而是等待获得满足进程内部/节点内部/机架内部这样的不同层次的本地性资源的最长等待时间。默认都是3000毫秒。

基本上，如果你的任务数量较大和单个任务运行时间比较长的情况下，单个任务是否在数据本地运行，代价区别可能比较显著，如果数据本地性不理想，那么调大这些参数对于性能优化可能会有一定的好处。反之如果等待的代价超过带来的收益，那就不要考虑了。

特别值得注意的是：在处理应用刚启动后提交的第一批任务时，由于当作业调度模块开始工作时，处理任务的Executors可能还没有完全注册完毕，因此一部分的任务会被放置到No Prefer的队列中，这部分任务的优先级仅次于数据本地性满足Process级别的任务，从而被优先分配到非本地节点执行，如果的确没有Executors在对应的节点上运行，或者的确是No Prefer的任务（如shuffleRDD），这样做确实是比较优化的选择，但是这里的实际情况只是这部分Executors还没来得及注册上而已。这种情况下，即使加大本节中这几个参数的数值也没有帮助。针对这个情况，有一些已经完成的和正在进行中的PR通过例如动态调整No Prefer队列，监控节点注册比例等等方式试图来给出更加智能的解决方案。不过，你也可以根据自身集群的启动情况，通过在创建SparkContext之后，主动Sleep几秒的方式来简单的解决这个问题。

**spark.speculation**

spark.speculation以及spark.speculation.interval,spark.speculation.quantile, spark.speculation.multiplier等参数调整Speculation行为的具体细节，Speculation是在任务调度的时候，如果没有适合当前本地性要求的任务可供运行，将跑得慢的任务在空闲计算资源上再度调度的行为，这些参数调整这些行为的频率和判断指标，默认是不使用Speculation的。

通常来说很难正确的判断是否需要Speculation，能真正发挥Speculation用处的场合，往往是某些节点由于运行环境原因，比如CPU资源由于某种原因被占用，磁盘损坏导致IO缓慢造成任务执行速度异常的情况，当然前提是你的分区任务不存在仅能被执行一次，或者不能同时执行多个拷贝等情况。Speculation任务参照的指标通常是其它任务的执行时间，而实际的任务可能由于分区数据尺寸不均匀，本来就会有时间差异，加上一定的调度和IO的随机性，所以如果一致性指标定得过严，Speculation可能并不能真的发现问题，反而增加了不必要的任务开销，定得过宽，大概又基本相当于没用。

个人觉得，如果你的集群规模比较大，运行环境复杂，的确可能经常发生执行异常，加上数据分区尺寸差异不大，为了程序运行时间的稳定性，那么可以考虑仔细调整这些参数。否则还是考虑如何排除造成任务执行速度异常的因数比较靠铺一些。

当然，我没有实际在很大规模的集群上运行过Spark，所以如果看法有些偏颇，还请有实际经验的XD指正。

##### 压缩和序列化相关

**spark.serializer**

默认为org.apache.spark.serializer.JavaSerializer,可选org.apache.spark.serializer.KryoSerializer,实际上只要是org.apache.spark.serializer的子类就可以了,不过如果只是应用,大概你不会自己去实现一个的。

序列化对于spark应用的性能来说,还是有很大影响的,在特定的数据格式的情况下,KryoSerializer的性能可以达到JavaSerializer的10倍以上,当然放到整个Spark程序中来考量,比重就没有那么大了,但是以Wordcount为例，通常也很容易达到30%以上的性能提升。而对于一些Int之类的基本类型数据，性能的提升就几乎可以忽略了。KryoSerializer依赖Twitter的Chill库来实现，相对于JavaSerializer，主要的问题在于不是所有的Java Serializable对象都能支持。

需要注意的是，这里可配的Serializer针对的对象是Shuffle数据，以及RDD Cache等场合，而Spark Task的序列化是通过spark.closure.serializer来配置，但是目前只支持JavaSerializer，所以等于没法配置啦。



### 41、Spark-On-Yarn分Cluster模式和Client模式，叙述每种模式的应用场景。

#### Cluster模式

Driver程序在YARN中运行，应用的运行结果不能在客户端显示，所以最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql）而非stdout输出的应用程序，客户端的终端显示的仅是作为YARN的job的简单运行状况。所以Cluster模式一般是运用在生产环境中。

#### Client模式

Driver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）。所以Client模式一般运用在测试环境中。



### 42、flink和spark streaming的区别

##### 运行角色对比

```
Spark Streaming 运行时的角色(standalone 模式)主要有：
	Master:主要负责整体集群资源的管理和应用程序调度；
	Worker:负责单个节点的资源管理，driver 和 executor 的启动等；
	Driver:用户入口程序执行的地方，即 SparkContext 执行的地方，主要是 DAG 生成、stage 划分、task 生成及调度；
	Executor:负责执行 task，反馈执行状态和执行结果。

Flink 运行时的角色(standalone 模式)主要有:
	Jobmanager: 协调分布式执行，他们调度任务、协调 checkpoints、协调故障恢复等。至少有一个 JobManager。高可用情况下可以启动多个 JobManager，其中一个选举为 leader，其余为 standby；
	Taskmanager: 负责执行具体的 tasks、缓存、交换数据流，至少有一个 TaskManager；
	Slot: 每个 task slot 代表 TaskManager 的一个固定部分资源，Slot 的个数代表着 taskmanager 可并行执行的 task 数。
```

##### 运行模型对比

Spark Streaming 是微批处理，运行的时候需要指定批处理的时间，每次运行 job 时处理一个批次的数据，流程如图 3 所示：

![img](img/model.jpg)

Flink 是基于事件驱动的，事件可以理解为消息。事件驱动的应用程序是一种状态应用程序，它会从一个或者多个流中注入事件，通过触发计算更新状态，或外部动作对注入的事件作出反应。

![img](img/flinkmodel.jpg)

##### 编程模型对比

**Spark Streaming**

Spark Streaming 与 kafka 的结合主要是两种模型：

- 基于 receiver dstream；
- 基于 direct dstream。

以上两种模型编程机构近似，只是在 api 和内部数据获取有些区别，新版本的已经取消了基于 receiver 这种模式，企业中通常采用基于 direct Dstream 的模式。

```scala
val Array(brokers, topics) = args // 创建一个批处理时间是2s的context
val sparkConf = new SparkConf().setAppName("DirectKafkaWordCount")
val ssc = new StreamingContext(sparkConf, Seconds(2))
// 使用broker和topic创建DirectStream
val topicsSet = topics.split(",").toSet
val kafkaParams = Map[String, String]("metadata.broker.list" -> brokers)
val messages = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams))
// 实现单词计数并打印
val lines = messages.map(_.value)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1L)).reduceByKey(_ + _)
wordCounts.print() // 启动流    
ssc.start()
ssc.awaitTermination()
```

通过以上代码我们可以 get 到：

- 设置批处理时间
- 创建数据流
- 编写transform
- 编写action
- 启动执行

**Flink**

Flink 与 kafka 结合是事件驱动，大家可能对此会有疑问，消费 kafka 的数据调用 poll 的时候是批量获取数据的(可以设置批处理大小和超时时间)，这就不能叫做事件触发了。而实际上，flink 内部对 poll 出来的数据进行了整理，然后逐条 emit，形成了事件触发的机制。 下面的代码是 flink 整合 kafka 作为 data source 和 data sink：

```scala
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.getConfig().disableSysoutLogging();
env.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(4, 10000));
// create a checkpoint every 5 seconds
env.enableCheckpointing(5000); 
// make parameters available in the web interface
env.getConfig().setGlobalJobParameters(parameterTool); 
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
//  ExecutionConfig.GlobalJobParameters
env.getConfig().setGlobalJobParameters(null);    
DataStream<KafkaEvent> input = env
           .addSource(new FlinkKafkaConsumer010<>(
                   parameterTool.getRequired("input-topic"),  new KafkaEventSchema(),
                   parameterTool.getProperties())
           .assignTimestampsAndWatermarks(new CustomWatermarkExtractor()))
           .setParallelism(1).rebalance()
           .keyBy("word")
           .map(new RollingAdditionMapper()).setParallelism(0);
input.addSink(new FlinkKafkaProducer010<>(parameterTool.getRequired("output-topic"), new KafkaEventSchema(),
        parameterTool.getProperties()));
env.execute("Kafka 0.10 Example");
```

从 Flink 与 kafka 结合的代码可以 get 到：

- 注册数据 source
- 编写运行逻辑
- 注册数据 sink
- 调用 env.execute 相比于 Spark Streaming 少了设置批处理时间，还有一个显著的区别是 flink 的所有算子都是 lazy 形式的，调用 env.execute 会构建 jobgraph。client 端负责 Jobgraph 生成并提交它到集群运行；而 Spark Streaming的操作算子分 action 和 transform，其中仅有 transform 是 lazy 形式，而且 DGA 生成、stage 划分、任务调度是在 driver 端进行的，在 client 模式下 driver 运行于客户端处。

##### 任务调度对比

**Spark Streaming任务调度**

Spark Streaming 任务如上文提到的是基于微批处理的，实际上每个批次都是一个 Spark Core 的任务。对于编码完成的 Spark Core 任务在生成到最终执行结束主要包括以下几个部分：

- 构建 DGA 图；
- 划分 stage；
- 生成 taskset；
- 调度 task。

![img](img/sparksubmit.png)

**Flink 任务调度**

对于 flink 的流任务客户端首先会生成 StreamGraph，接着生成 JobGraph，然后将 jobGraph 提交给 Jobmanager 由它完成 jobGraph 到 ExecutionGraph 的转变，最后由 jobManager 调度执行。

![img](img/flinksubmit.jpg)

如上图所示有一个由 data source、MapFunction和 ReduceFunction 组成的程序，data source 和 MapFunction 的并发度都为 4，而 ReduceFunction 的并发度为 3。一个数据流由 Source-Map-Reduce 的顺序组成，在具有 2 个TaskManager、每个 TaskManager 都有 3 个 Task Slot 的集群上运行。

可以看出 flink 的拓扑生成提交执行之后，除非故障，否则拓扑部件执行位置不变，并行度由每一个算子并行度决定，类似于 storm。而 spark Streaming 是每个批次都会根据数据本地性和资源情况进行调度，无固定的执行拓扑结构。 flink 是数据在拓扑结构里流动执行，而 Spark Streaming 则是对数据缓存批次并行处理。

##### 时间机制对比

流处理程序在时间概念上总共有三个时间概念：

**处理时间**

处理时间是指每台机器的系统时间，当流程序采用处理时间时将使用运行各个运算符实例的机器时间。处理时间是最简单的时间概念，不需要流和机器之间的协调，它能提供最好的性能和最低延迟。然而在分布式和异步环境中，处理时间不能提供消息事件的时序性保证，因为它受到消息传输延迟，消息在算子之间流动的速度等方面制约。

**事件时间**

事件时间是指事件在其设备上发生的时间，这个时间在事件进入 flink 之前已经嵌入事件，然后 flink 可以提取该时间。基于事件时间进行处理的流程序可以保证事件在处理的时候的顺序性，但是基于事件时间的应用程序必须要结合 watermark 机制。基于事件时间的处理往往有一定的滞后性，因为它需要等待后续事件和处理无序事件，对于时间敏感的应用使用的时候要慎重考虑。

**注入时间**

注入时间是事件注入到 flink 的时间。事件在 source 算子处获取 source 的当前时间作为事件注入时间，后续的基于时间的处理算子会使用该时间处理数据。

相比于事件时间，注入时间不能够处理无序事件或者滞后事件，但是应用程序无序指定如何生成 watermark。在内部注入时间程序的处理和事件时间类似，但是时间戳分配和 watermark 生成都是自动的。

![img](img/time.jpg)

**Spark 时间机制**

Spark Streaming 只支持处理时间，Structured streaming 支持处理时间和事件时间，同时支持 watermark 机制处理滞后数据。

**Flink 时间机制**

flink 支持三种时间机制：事件时间，注入时间，处理时间，同时支持 watermark 机制处理滞后数据。

##### 容错机制及处理语义

**Spark Streaming 保证仅一次处理**

对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。

对于 Spark Streaming 与 kafka 结合的 direct Stream 可以自己维护 offset 到 zookeeper、kafka 或任何其它外部系统，每次提交完结果之后再提交 offset，这样故障恢复重启可以利用上次提交的 offset 恢复，保证数据不丢失。但是假如故障发生在提交结果之后、提交 offset 之前会导致数据多次处理，这个时候我们需要保证处理结果多次输出不影响正常的业务。

由此可以分析，假设要保证数据恰一次处理语义，那么结果输出和 offset 提交必须在一个事务内完成。在这里有以下三种做法：

- repartition(1) Spark Streaming 输出的 action 变成仅一个 partition，这样可以利用事务去做
- 用幂等写入的方式实现
- 将结果和 offset绑定到 一起提交

也就是结果数据包含 offset。这样提交结果和提交 offset 就是一个操作完成，不会数据丢失，也不会重复处理。故障恢复的时候可以利用上次提交结果带的 offset。

**Flink 与 kafka 0.11 保证仅一次处理**

若要 sink 支持仅一次语义，必须以事务的方式写数据到 Kafka，这样当提交事务时两次 checkpoint 间的所有写入操作作为一个事务被提交。这确保了出现故障或崩溃时这些写入操作能够被回滚。

在一个分布式且含有多个并发执行 sink 的应用中，仅仅执行单次提交或回滚是不够的，因为所有组件都必须对这些提交或回滚达成共识，这样才能保证得到一致性的结果。Flink 使用两阶段提交协议以及预提交(pre-commit)阶段来解决这个问题。



### 43、斐波那契数列能用spark做出来吗？

Spark本来就是将海量数据进行批量处理的，如果用Spark实现兔子数列就是个伪命题，用Scala实现会简洁很多：

```scala
  /**
    * 递归实现兔子数列
    * @param n
    * @return
    */
  def m1(n: Int): Int = {
    if (n == 0) 0
    if (n == 1 || n == 2) 1
    else m2(n - 1) + m2(n - 2)
  }

  /**
    * 采用动态规划思想实现兔子数列 ，比递归实现要高效
    * @param n
    * @return
    */  
  def m2(n: Int): Int = {
    var n1 = 0
    var n2 = 1
    var res = 0
    if (n <= 0) 0
    else if (n == 1) return 1
    else {
      for (i <- 2 to n) {
        res = n1 + n2
        n1 = n2
        n2 = res
      }
      res
    }
  }
```



### 44、spark有几种task？

有两种task，ShuffleMapTask和ResultTask。

ShuffleMapTask：

```scala
override def runTask(context: TaskContext): U = {
	// Deserialize the RDD and the func using the broadcast variables.
	val deserializeStartTime = System.currentTimeMillis()
	val ser = SparkEnv.get.closureSerializer.newInstance()
	// 反序列化得到rdd及func
	val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
	_executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime

	// 对rdd指定partition的迭代器执行func函数
	func(context, rdd.iterator(partition, context))
}
```

实现代码如上，主要做了两件事：

1. 反序列化得到 rdd 及 func

2. 对rdd指定partition的迭代器执行func函数并返回结果

ShuffleMapTask:

```scala
    var writer: ShuffleWriter[Any, Any] = null
    try {
      val manager = SparkEnv.get.shuffleManager
      // 拿到写入流
      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)
      // 将maptask端的数据写入磁盘的过程----shufflewrite
      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
      writer.stop(success = true).get
    }
```

与 ResultTask 对 partition 数据进行计算得到计算结果并汇报给 driver 不同，ShuffleMapTask 的职责是为下游的 RDD 计算而输入数据。更具体的说，ShuffleMapTask 要计算出 partition 数据并通过 shuffle write 写入磁盘（由 BlockManager 来管理）来等待下游的 RDD 通过 shuffle read 读取。



### 45、对比一下SparkStreaming、Flink和Strom，如下图###

![](img/80004.jpg)


# 第九章 kafka #
### 1、kafka扩容 ###
将服务器添加到Kafka集群非常简单，只需为其分配唯一的 broker ID并在您的新服务器上启动Kafka即可。但是，这些新的服务器不会自动分配到任何数据分区，除非将分区移动到这些分区，否则直到创建新 topic 时才会提供服务。所以通常当您将机器添加到群集中时，您会希望将一些现有数据迁移到这些机器上。迁移数据的过程是手动启动的，但是完全自动化。在迁移数据时，Kafka会将新服务器添加为正在迁移的分区的 follower，并允许它完全复制该分区中的现有数据。当新服务器完全复制了此分区的内容并加入了同步副本时，其中一个现有副本将删除其分区的数据。分区重新分配工具可用于跨 broker 移动分区。理想的分区分布将确保所有 broker 的数据负载和分区大小比较均衡。分区重新分配工具不具备自动分析Kafka集群中的数据分布并移动分区以获得均匀负载的功能。因此，管理员必须找出哪些 topic 或分区应该移动。

分区重新分配工具可以以3种互斥方式运行：

--generate: 在此模式下，给定一个 topic 列表和一个 broker 列表，该工具会生成一个候选重新分配，以将指定的 topic 的所有分区移动到新的broker。此选项仅提供了一种便捷的方式，可以根据 tpoc 和目标 broker 列表生成分区重新分配计划。

--execute: 在此模式下，该工具基于用户提供的重新分配计划启动分区重新分配。（使用--reassignment-json-file选项）。这可以是由管理员制作的自定义重新分配计划，也可以是使用--generate选项提供的自定义重新分配计划。

--verify: 在此模式下，该工具将验证最近用 --execute 模式执行间的所有分区的重新分配状态。状态可以是成功完成，失败或正在进行 

自动将数据迁移到新机器分区重新分配工具可用于将当前一组 broker 的一些 topic 移至新增的topic。这在扩展现有群集时通常很有用，因为将整个 topic 移动到新 broker 集比移动一个分区更容易。当这样做的时候，用户应该提供需要移动到新的 broker 集合的 topic 列表和新的目标broker列表。该工具然后会均匀分配新 broker 集中 topic 的所有分区。在此过程中，topic 的复制因子保持不变。实际上，所有输入 topic 的所有分区副本都将从旧的broker 组转移到新 broker中。例如，以下示例将把名叫foo1，foo2的 topic 的所有分区移动到新的 broker 集5,6。最后，foo1和foo2的所有分区将只在<5,6> broker 上存在。由于该工具接受由 topic 组成的输入列表作为json文件，因此首先需要确定要移动的 topic 并创建 json 文件，如下所示： 

    > cat topics-to-move.json
    {"topics": [{"topic": "foo1"},
                {"topic": "foo2"}],
    "version":1
    }

一旦json文件准备就绪，就可以使用分区重新分配工具来生成候选分配： 

    > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list "5,6" --generate
    //当前分区副本分配
     
    {"version":1,
    "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                  {"topic":"foo1","partition":0,"replicas":[3,4]},
                  {"topic":"foo2","partition":2,"replicas":[1,2]},
                  {"topic":"foo2","partition":0,"replicas":[3,4]},
                  {"topic":"foo1","partition":1,"replicas":[2,3]},
                  {"topic":"foo2","partition":1,"replicas":[2,3]}]
    }
     
    //建议的分区重新分配配置
     
    {"version":1,
    "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                  {"topic":"foo1","partition":0,"replicas":[5,6]},
                  {"topic":"foo2","partition":2,"replicas":[5,6]},
                  {"topic":"foo2","partition":0,"replicas":[5,6]},
                  {"topic":"foo1","partition":1,"replicas":[5,6]},
                  {"topic":"foo2","partition":1,"replicas":[5,6]}]
    }

该工具会生成一个候选分配，将所有分区从topic foo1，foo2移动到brokers 5,6。但是，请注意，这个时候，分区操作还没有开始，它只是告诉你当前的任务和建议的新任务。应该保存当前的分配，以防您想要回滚到它。新的任务应该保存在一个json文件（例如expand-cluster-reassignment.json）中，并用--execute选项输入到工具中，如下所示： 

    > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute
    //当前分区副本分配
     
    {"version":1,
    "partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
                  {"topic":"foo1","partition":0,"replicas":[3,4]},
                  {"topic":"foo2","partition":2,"replicas":[1,2]},
                  {"topic":"foo2","partition":0,"replicas":[3,4]},
                  {"topic":"foo1","partition":1,"replicas":[2,3]},
                  {"topic":"foo2","partition":1,"replicas":[2,3]}]
    }
     
    //保存这个以在回滚期间用作--reassignment-json-file选项
    //成功开始重新分配分区
    {"version":1,
    "partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
                  {"topic":"foo1","partition":0,"replicas":[5,6]},
                  {"topic":"foo2","partition":2,"replicas":[5,6]},
                  {"topic":"foo2","partition":0,"replicas":[5,6]},
                  {"topic":"foo1","partition":1,"replicas":[5,6]},
                  {"topic":"foo2","partition":1,"replicas":[5,6]}]
    }

最后，可以使用--verify选项来检查分区重新分配的状态。请注意，相同的expand-cluster-reassignment.json（与--execute选项一起使用）应与--verify选项一起使用： 

    > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify
    Status of partition reassignment:
    Reassignment of partition [foo1,0] completed successfully
    Reassignment of partition [foo1,1] is in progress
    Reassignment of partition [foo1,2] is in progress
    Reassignment of partition [foo2,0] completed successfully
    Reassignment of partition [foo2,1] completed successfully
    Reassignment of partition [foo2,2] completed successfully

自定义分区分配和迁移分区重新分配工具也可用于选择性地将分区的副本移动到特定的一组 broker。当以这种方式使用时，假定用户知道重新分配计划并且不需要该工具产生候选的重新分配，有效地跳过 --generate 步骤并直接到 --execute步骤例如，以下示例将 topic foo1的分区0 移到 broker 5,6中和将 topic foo2的分区1移到 broker 2,3中：第一步是在json文件中定义重新分配计划： 

    > cat custom-reassignment.json
    {"version":1,"partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},{"topic":"foo2","partition":1,"replicas":[2,3]}]}


然后，使用带有 --execute 选项的 json 文件来启动重新分配过程： 

     > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute
    当前分区副本分配情况
     
     {"version":1,
     "partitions":[{"topic":"foo1","partition":0,"replicas":[1,2]},
                   {"topic":"foo2","partition":1,"replicas":[3,4]}]
     }
     
     保存这个以在回滚期间用作 --reassignment-json-file 选项
     成功开始重新分配分区
     {"version":1,
     "partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},
                   {"topic":"foo2","partition":1,"replicas":[2,3]}]
     }

可以使用--verify选项来检查分区重新分配的状态。 请注意，相同的expand-cluster-reassignment.json（与--execute选项一起使用）应与--verify选项一起使用： 

    > bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify
    Status of partition reassignment:
    Reassignment of partition [foo1,0] completed successfully
    Reassignment of partition [foo2,1] completed successfully



### 2、kafka的数据堆积问题？是如何解决的？ ###
数据堆积是怎么产生的？
其一是消费者消费能力不足，这种情况，可以通过增加消费者，提升消费能力；
其二是使用Kafka时，消费者每次poll的数据业务处理时间不能超过kafka的max.poll.interval.ms，该参数在kafka0.10.2.1中的默认值是300s,所以要综合业务处理时间和每次poll的数据数量。 
### 3、kafka用的什么模式 ###
发布订阅模式
### 4、kafka的消费者组怎么理解,说了一个情景题:用两个同名的topic,两个消费者组去拉取数据的时候,会发生什么 ###
关于消费者消费数据的一些规则：
任何Consumer必须属于一个Consumer Group
同一Consumer Group中的多个Consumer实例，不同时消费同一个partition。
不同Consumer Group的Consumer实例可以同时消费同一个partition。
partition内消息是有序的，Consumer通过pull方式消费消息。
Kafka不删除已消费的消息
### 5、kafka了解多少，kafka的一致性怎么保证的，kafka消费者如何消费分区，分区分配策略 ###
一致性保证
一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到
1.HighWaterMark简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置，另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark <= leader. LogEndOffset
2.对于Leader新写入的msg，Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消息才能被Consumer消费，即Consumer最多只能消费到HW位置
这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。同时,Follower也会维护一份自己的HW,Folloer.HW = min(Leader.HW, Follower.offset)
kafka消费者如何消费分区
按照Kafka默认的消费逻辑设定，一个分区只能被同一个消费组（ConsumerGroup）内的一个消费者消费。
如果消费者过多，出现了消费者的数量大于分区的数量的情况，就会有消费者分配不到任何分区。
kafka分区分配策略
Kafka提供了消费者客户端参数partition.assignment.strategy用来设置消费者与订阅主题之间的分区分配策略。默认情况下，此参数的值为：org.apache.kafka.clients.consumer.RangeAssignor，即采用RangeAssignor分配策略。除此之外，Kafka中还提供了另外两种分配策略： RoundRobinAssignor和StickyAssignor。消费者客户端参数partition.asssignment.strategy可以配置多个分配策略，彼此之间以逗号分隔。
RangeAssignor分配策略
RangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。
假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。
为了更加通俗的讲解RangeAssignor策略，我们不妨再举一些示例。假设消费组内有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有4个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为：
    消费者C0：t0p0、t0p1、t1p0、t1p1
    消费者C1：t0p2、t0p3、t1p2、t1p3
这样分配的很均匀，那么此种分配策略能够一直保持这种良好的特性呢？我们再来看下另外一种情况。假设上面例子中2个主题都只有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为：
    消费者C0：t0p0、t0p1、t1p0、t1p1
    消费者C1：t0p2、t1p2
可以明显的看到这样的分配并不均匀，如果将类似的情形扩大，有可能会出现部分消费者过载的情况。对此我们再来看下另一种RoundRobinAssignor策略的分配效果如何。
RoundRobinAssignor分配策略
RoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。RoundRobinAssignor策略对应的partition.assignment.strategy参数值为：org.apache.kafka.clients.consumer.RoundRobinAssignor。
如果同一个消费组内所有的消费者的订阅信息都是相同的，那么RoundRobinAssignor策略的分区分配会是均匀的。举例，假设消费组中有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有3个分区，那么所订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为：
    消费者C0：t0p0、t0p2、t1p1
    消费者C1：t0p1、t1p0、t1p2
如果同一个消费组内的消费者所订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个topic，那么在分配分区的时候此消费者将分配不到这个topic的任何分区。
举例，假设消费组内有3个消费者C0、C1和C2，它们共订阅了3个主题：t0、t1、t2，这3个主题分别有1、2、3个分区，即整个消费组订阅了t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。具体而言，消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2，那么最终的分配结果为：
    消费者C0：t0p0
    消费者C1：t1p0
    消费者C2：t1p1、t2p0、t2p1、t2p2
可以看到RoundRobinAssignor策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区t1p1分配给消费者C1。
StickyAssignor分配策略
我们再来看一下StickyAssignor策略，“sticky”这个单词可以翻译为“粘性的”，Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的：
分区的分配要尽可能的均匀；
分区的分配尽可能的与上次分配的保持相同。
当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。我们举例来看一下StickyAssignor策略的实际效果。
假设消费组内有3个消费者：C0、C1和C2，它们都订阅了4个主题：t0、t1、t2、t3，并且每个主题有2个分区，也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下：
    消费者C0：t0p0、t1p1、t3p0
    消费者C1：t0p1、t2p0、t3p1
    消费者C2：t1p0、t2p1
这样初看上去似乎与采用RoundRobinAssignor策略所分配的结果相同，但事实是否真的如此呢？再假设此时消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下：
    消费者C0：t0p0、t1p0、t2p0、t3p0
    消费者C2：t0p1、t1p1、t2p1、t3p1
如分配结果所示，RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为：
    消费者C0：t0p0、t1p1、t3p0、t2p0
    消费者C2：t1p0、t2p1、t0p1、t3p1
可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配还保持了均衡。
如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。
到目前为止所分析的都是消费者的订阅信息都是相同的情况，我们来看一下订阅信息不同的情况下的处理。
举例，同样消费组内有3个消费者：C0、C1和C2，集群中有3个主题：t0、t1和t2，这3个主题分别有1、2、3个分区，也就是说集群中有t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。消费者C0订阅了主题t0，消费者C1订阅了主题t0和t1，消费者C2订阅了主题t0、t1和t2。
如果此时采用RoundRobinAssignor策略，那么最终的分配结果如下所示（和讲述RoundRobinAssignor策略时的一样，这样不妨赘述一下）：
【分配结果集1】
消费者C0：t0p0
消费者C1：t1p0
消费者C2：t1p1、t2p0、t2p1、t2p2
    消费者C0：t0p0
    消费者C1：t1p0
    消费者C2：t1p1、t2p0、t2p1、t2p2
如果此时采用的是StickyAssignor策略，那么最终的分配结果为：
【分配结果集2】
    消费者C0：t0p0
    消费者C1：t1p0、t1p1
    消费者C2：t2p0、t2p1、t2p2
可以看到这是一个最优解（消费者C0没有订阅主题t1和t2，所以不能分配主题t1和t2中的任何分区给它，对于消费者C1也可同理推断）。
假如此时消费者C0脱离了消费组，那么RoundRobinAssignor策略的分配结果为：
    消费者C1：t0p0、t1p1
    消费者C2：t1p0、t2p0、t2p1、t2p2
可以看到RoundRobinAssignor策略保留了消费者C1和C2中原有的3个分区的分配：t2p0、t2p1和t2p2（针对结果集1）。而如果采用的是StickyAssignor策略，那么分配结果为：
    消费者C1：t1p0、t1p1、t0p0
    消费者C2：t2p0、t2p1、t2p2
可以看到StickyAssignor策略保留了消费者C1和C2中原有的5个分区的分配：t1p0、t1p1、t2p0、t2p1、t2p2。
从结果上看StickyAssignor策略比另外两者分配策略而言显得更加的优异，这个策略的代码实现也是异常复杂，如果读者没有接触过这种分配策略，不妨使用一下来尝尝鲜。

### 6、Kafka与其他消息队列的不同； ###
| 功能                     | 消息队列 RocketMQ                                    | Apache RocketMQ （开源）     | 消息队列 Kafka                                       | Apache Kafka （开源）  | RabbitMQ （开源）                     |
| ------------------------ | ---------------------------------------------------- | ---------------------------- | ---------------------------------------------------- | ---------------------- | ------------------------------------- |
| 安全防护                 | 支持                                                 | 不支持                       | 支持                                                 | 不支持                 | 支持                                  |
| 主子账号支持             | 支持                                                 | 不支持                       | 支持                                                 | 不支持                 | 不支持                                |
| 可靠性                   | – 同步刷盘 – 同步双写 – 超3份数据副本 – 99.99999999% | – 同步刷盘 – 异步刷盘        | – 同步刷盘 – 同步双写 – 超3份数据副本 – 99.99999999% | 异步刷盘，丢数据概率高 | 同步刷盘                              |
| 可用性                   | – 非常好，99.95% – Always Writable                   | 好                           | – 非常好，99.95% – Always Writable                   | 好                     | 好                                    |
| 横向扩展能力             | – 支持平滑扩展 – 支持百万级 QPS                      | 支持                         | – 支持平滑扩展 – 支持百万级 QPS                      | 支持                   | – 集群扩容依赖前端 – LVS 负载均衡调度 |
| Low Latency              | 支持                                                 | 不支持                       | 支持                                                 | 不支持                 | 不支持                                |
| 消费模型                 | Push / Pull                                          | Push / Pull                  | Push / Pull                                          | Pull                   | Push / Pull                           |
| 定时消息                 | 支持（可精确到秒级）                                 | 支持（只支持18个固定 Level） | 暂不支持                                             | 不支持                 | 支持                                  |
| 事务消息                 | 支持                                                 | 不支持                       | 不支持                                               | 不支持                 | 不支持                                |
| 顺序消息                 | 支持                                                 | 支持                         | 暂不支持                                             | 支持                   | 不支持                                |
| 全链路消息轨迹           | 支持                                                 | 不支持                       | 暂不支持                                             | 不支持                 | 不支持                                |
| 消息堆积能力             | 百亿级别 不影响性能                                  | 百亿级别 影响性能            | 百亿级别 不影响性能                                  | 影响性能               | 影响性能                              |
| 消息堆积查询             | 支持                                                 | 支持                         | 支持                                                 | 不支持                 | 不支持                                |
| 消息回溯                 | 支持                                                 | 支持                         | 支持                                                 | 不支持                 | 不支持                                |
| 消息重试                 | 支持                                                 | 支持                         | 暂不支持                                             | 不支持                 | 支持                                  |
| 死信队列                 | 支持                                                 | 支持                         | 不支持                                               | 不支持                 | 支持                                  |
| 性能（常规）             | 非常好 百万级 QPS                                    | 非常好 十万级 QPS            | 非常好 百万级 QPS                                    | 非常好 百万级 QPS      | 一般 万级 QPS                         |
| 性能（万级 Topic 场景）  | 非常好 百万级 QPS                                    | 非常好 十万级 QPS            | 非常好 百万级 QPS                                    | 低                     | 低                                    |
| 性能（海量消息堆积场景） | 非常好 百万级 QPS                                    | 非常好 十万级 QPS            | 非常好 百万级 QPS                                    | 低                     | 低                                    |

### 7、在一次leader failover过程中，kafka是如何协调各个broker的，以及处理partition和replication，这些对producer和consumer有何影响？ ###
1. Controller在Zookeeper的/brokers/ids节点上注册Watch。一旦有Broker宕机（本文用宕机代表任何让Kafka认为其Broker die的情景，包括但不限于机器断电，网络不可用，GC导致的Stop The World，进程crash等），其在Zookeeper对应的Znode会自动被删除，Zookeeper会fire Controller注册的Watch，Controller即可获取最新的幸存的Broker列表。
2. Controller决定set_p，该集合包含了宕机的所有Broker上的所有Partition。
3. 对set_p中的每一个Partition：
   　　3.1 从/brokers/topics/[topic]/partitions/[partition]/state读取该Partition当前的ISR。
      　　3.2 决定该Partition的新Leader。如果当前ISR中有至少一个Replica还幸存，则选择其中一个作为新Leader，新的ISR则包含当前ISR中所有幸存的Replica。否则选择该Partition中任意一个幸存的Replica作为新的Leader以及ISR（该场景下可能会有潜在的数据丢失）。如果该Partition的所有Replica都宕机了，则将新的Leader设置为-1。
      　　3.3 将新的Leader，ISR和新的leader_epoch及controller_epoch写入/brokers/topics/[topic]/partitions/[partition]/state。注意，该操作只有Controller版本在3.1至3.3的过程中无变化时才会执行，否则跳转到3.1。
4. 直接通过RPC向set_p相关的Broker发送LeaderAndISRRequest命令。Controller可以在一个RPC操作中发送多个命令从而提高效率。

### 8、kafka的producer、broker和consumer如何保持消息不重复不丢失。 ###
kafka支持3种消息投递语义：
- At most once——最多一次，消息可能会丢失，但不会重复
- At least once——最少一次，消息不会丢失，可能会重复
- Exactly once——只且一次，消息不丢失不重复，只且消费一次。
但是整体的消息投递语义需要Producer端和Consumer端两者来保证。
Producer 消息生产者端
一个场景例子：
当producer向broker发送一条消息，这时网络出错了，producer无法得知broker是否接受到了这条消息。
网络出错可能是发生在消息传递的过程中，也可能发生在broker已经接受到了消息，并返回ack给producer的过程中。
这时，producer只能进行重发，消息可能会重复，但是保证了at least once。
0.11.0的版本通过给每个producer一个唯一ID，并且在每条消息中生成一个sequence num，
这样就能对消息去重，达到producer端的exactly once。
这里还涉及到producer端的acks设置和broker端的副本数量，以及min.insync.replicas的设置。
比如producer端的acks设置如下：
acks=0 //消息发了就发了，不等任何响应就认为消息发送成功
acks=1 //leader分片写消息成功就返回响应给producer
acks=all（-1） //当acks=all， min.insync.replicas=2，就要求INSRNC列表中必须要有2个副本都写成功，才返回响应给producer，
如果INSRNC中已同步副本数量不足2，就会报异常，如果没有2个副本写成功，也会报异常，消息就会认为没有写成功。
Broker 消息接收端
上文说过acks=1，表示当leader分片副本写消息成功就返回响应给producer，此时认为消息发送成功。
如果leader写成功单马上挂了，还没有将这个写成功的消息同步给其他的分片副本，那么这个分片此时的ISR列表为空，
如果unclean.leader.election.enable=true，就会发生log truncation（日志截取），同样会发生消息丢失。
如果unclean.leader.election.enable=false，那么这个分片上的服务就不可用了，producer向这个分片发消息就会抛异常。
所以我们设置min.insync.replicas=2，unclean.leader.election.enable=false，producer端的acks=all，这样发送成功的消息就绝不会丢失。
Consumer 消息消费者端
所有分片的副本都有自己的log文件（保存消息）和相同的offset值。当consumer没挂的时候，offset直接保存在内存中，
如果挂了，就会发生负载均衡，需要consumer group中另外的consumer来接管并继续消费。
consumer消费消息的方式有以下2种;
1. consumer读取消息，保存offset，然后处理消息。
   现在假设一个场景：保存offset成功，但是消息处理失败，consumer又挂了，这时来接管的consumer
   就只能从上次保存的offset继续消费，这种情况下就有可能丢消息，但是保证了at most once语义。
2. consumer读取消息，处理消息，处理成功，保存offset。
   如果消息处理成功，但是在保存offset时，consumer挂了，这时来接管的consumer也只能
   从上一次保存的offset开始消费，这时消息就会被重复消费，也就是保证了at least once语义。
以上这些机制的保证都不是直接一个配置可以解决的，而是你的consumer代码来完成的，只是一个处理顺序先后问题。 
第一种对应的代码：
    List<String> messages = consumer.poll();
    consumer.commitOffset();
    processMsg(messages);
第二种对应的代码：
    List<String> messages = consumer.poll();
    processMsg(messages);
    consumer.commitOffset();
Exactly Once实现原理
下面详细说说exactly once的实现原理。
Producer端的消息幂等性保证
每个Producer在初始化的时候都会被分配一个唯一的PID，
Producer向指定的Topic的特定Partition发送的消息都携带一个sequence number（简称seqNum），从零开始的单调递增的。
Broker会将Topic-Partition对应的seqNum在内存中维护，每次接受到Producer的消息都会进行校验；
只有seqNum比上次提交的seqNum刚好大一，才被认为是合法的。比它大的，说明消息有丢失；比它小的，说明消息重复发送了。
以上说的这个只是针对单个Producer在一个session内的情况，假设Producer挂了，又重新启动一个Producer被而且分配了另外一个PID，
这样就不能达到防重的目的了，所以kafka又引进了Transactional Guarantees（事务性保证）。
Transactional Guarantees 事务性保证
kafka的事务性保证说的是：同时向多个TopicPartitions发送消息，要么都成功，要么都失败。
Consumer端
以上的事务性保证只是针对的producer端，对consumer端无法保证，有以下原因：
1. 压实类型的topics，有些事务消息可能被新版本的producer重写
2. 事务可能跨坐2个log segments，这时旧的segments可能被删除，就会丢消息
3. 消费者可能寻址到事务中任意一点，也会丢失一些初始化的消息
4. 消费者可能不会同时从所有的参与事务的TopicPartitions分片中消费消息
如果是消费kafka中的topic，并且将结果写回到kafka中另外的topic，
可以将消息处理后结果的保存和offset的保存绑定为一个事务，这时就能保证
消息的处理和offset的提交要么都成功，要么都失败。
如果是将处理消息后的结果保存到外部系统，这时就要用到两阶段提交（tow-phase commit），
但是这样做很麻烦，较好的方式是offset自己管理，将它和消息的结果保存到同一个地方，整体上进行绑定， 
可以参考Kafka Connect中HDFS的例子。

### 9、说一说kafka的选举机制 ###
Kafka中的选举大致可以分为三大类：控制器的选举、分区leader的选举以及消费者相关的选举，这里还可以具体细分为7个小类。我们一一来过一下，本文只是简单罗列下大致的内容，至于内部具体的细节逻辑就需要靠读者自己去探索啦。虐人还是被虐就靠你的自驱力了。
控制器的选举
在Kafka集群中会有一个或多个broker，其中有一个broker会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态等工作。比如当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。再比如当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。
Kafka Controller的选举是依赖Zookeeper来实现的，在Kafka集群中哪个broker能够成功创建/controller这个临时（EPHEMERAL）节点他就可以成为Kafka Controller。
分区leader的选举
分区leader副本的选举由Kafka Controller 负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的leader副本下线，此时分区需要选举一个新的leader上线来对外提供服务）的时候都需要执行leader的选举动作。
基本思路是按照AR集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。注意这里是根据AR的顺序而不是ISR的顺序进行选举的。这个说起来比较抽象，有兴趣的读者可以手动关闭/开启某个集群中的broker来观察一下具体的变化。
还有一些情况也会发生分区leader的选举，比如当分区进行重分配（reassign）的时候也需要执行leader的选举动作。这个思路比较简单：从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。
再比如当发生优先副本（preferred replica partition leader election）的选举时，直接将优先副本设置为leader即可，AR集合中的第一个副本即为优先副本。
组协调器GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有leader，那么第一个加入消费组的消费者即为消费组的leader。如果某一时刻leader消费者由于某些原因退出了消费组，那么会重新选举一个新的leader.
### 10、kafka的topic，partition，replica，message的理解 ###
Topic：Topic可以理解为一个队列，消息根据Topic进行归类。Topic也可以理解为一个命名的消息流
partition:
partition:一个topic可以分为多个partition，每个partition是一个有序的队；在磁盘上以文件夹的形式存在；消息最终以文件形式保存在partition文件夹下面，分段存储。
replica:replica指的是消息的备份，为了保证kafka的高可用（当leader节点挂了之后，kafka依然能提供服务）kafka提供了备份的功能。这个备份是针对partition的。可以通过 default.replication.factor 对replica的数目进行配置，默认值为1，表示不对topic进行备份。如果配置为2，表示除了leader节点，对于topic里的每一个partition，都会有一个额外的备份。
message:实际写入Kafka中并可以被读取的消息记录。每个record包含了key、value和timestamp。
### 11、kafka生产者的具体实现，怎么保证数据不丢不重 ###
当producer向broker发送一条消息，这时网络出错了，producer无法得知broker是否接受到了这条消息。
网络出错可能是发生在消息传递的过程中，也可能发生在broker已经接受到了消息，并返回ack给producer的过程中。
这时，producer只能进行重发，消息可能会重复，但是保证了at least once。
0.11.0的版本通过给每个producer一个唯一ID，并且在每条消息中生成一个sequence num，
这样就能对消息去重，达到producer端的exactly once。
这里还涉及到producer端的acks设置和broker端的副本数量，以及min.insync.replicas的设置。
比如producer端的acks设置如下：
acks=0 //消息发了就发了，不等任何响应就认为消息发送成功
acks=1 //leader分片写消息成功就返回响应给producer
acks=all（-1） //当acks=all， min.insync.replicas=2，就要求INSRNC列表中必须要有2个副本都写成功，才返回响应给producer，
如果INSRNC中已同步副本数量不足2，就会报异常，如果没有2个副本写成功，也会报异常，消息就会认为没有写成功。
### 12、kafka的官网看过吗?简单说一下 ###

### 13、kafka的partition是如何分的，topic中一个partition发生数据堆积，如何处理 ###
Kafka消费者是消费组的一部分，当多个消费者形成一个消费组来消费主题时，每个消费者会收到不同分区的消息。 
可以通过增加消费组的消费者来进行水平扩展提升消费能力 
### 14、kafka如何实现高吞吐 ###

### 15、kafka自己写api管理的情况下，ack失败，导致重复消费，你自己怎么设计解决？ ###
幂等或者每消费一条消息都记录offset，对于少数严格的场景可能需要把offset或唯一ID,例如订单ID和下游状态更新放在同一个数据库里面做事务来保证精确的一次更新或者在下游数据表里面同时记录消费offset，然后更新下游数据的时候用消费位点做乐观锁拒绝掉旧位点的数据更新。 
### 16、kafka集群配置 ###



# 第十章	flink #
### 你觉得flink的优点是什么 ###




# 第十一章	数据结构与算法 #
### 1、git是什么，介绍一下 ###
### 2、数据挖掘算法了解哪些？有那些应用 ###
### 3、常用的推荐算法有哪些？你们在推荐系统中采用的推荐策略是什么 ###
### 4、GeoHash算法原理了解吗; ###
### 5、实现将10进制数字字符串转换成为16进制数字字符串，如"155",输出"0x9B",要求不得使用任何第三方提供字符串与数字转换方法，如c语言的atol，Java的String.ParsetInt等等 ###
### 6、设计一个简易链表类SimpleList,要求给出如：构造(SimpleList)、读取(Get)、插入(insert),删除(earse)等基本方法的定义，鼓励使用模板(c++)或泛型(java).只要求写出声明，不要求实现，请写上必要注释 ###
### 7、随机给定两个超大证书，实现计算两数之乘积，如"111111111111111"和"222222222222222",输出乘积"24691358024691308641975308642",要求不使用java中的BigInteger类型 ###
### 8、给一个字符串，找出他的最长回文字符串 ###
### 9、给一个数，求他的全排列 ###
### 10、 ###
![](./img/110001.jpg)
![](./img/110002.jpg)
![](./img/110003.jpg)
![](./img/110004.jpg)
### 11、两个超大的矩阵，用MR求乘积 ###
### 12、任意字符串（有重复字母），找出最大子串 ###
### 13、概率论：一个班有50个人，求至少两个人同天生日的概率？ ###
### 14、计算线性代数的特征值，特征向量 ###
### 15、查找二度好友，手写 ###
### 16、网络Ip分段，如何查找指定Ip在这个段里，高效？写程序 ###
### 17、用spark 或者MR实现矩阵乘积 ###
### 18、计算了一个贝叶斯概率题 有两张牌，一个是两面红，另一个是一面红一面黑，求第一次取出的牌一面是红色，背面也为红色的概率 ###
### 19、流量走势是怎么分析的 ###
### 20、写一下ip转换int 的算法 ###
### 21、写一下ip的二分查找 ###
### 22、给你一个链表，只知道第一个node，判断后面的链表是否存在环状结构 ###
### 23、给你一个整数123，实现全排序，需要计算时间复杂度和空间复杂度 ###
### 24、数据量贼大，大到内存放不下的两个有序数组，求交集 ###
### 25、 ###
![](./img/110005.jpg)
![](./img/110006.jpg)
![](./img/110007.jpg)

### 26、 ###
![](./img/110008.jpg)
### 27、用户画像体系是怎么建的？ ###
### 28、 ###
![](./img/110009.jpg)
### 29、分层聚类算法 ###
### 30、乱序100万条数据，进行排序 ###
### 31、一个数组A[1,5,6,7,8,4,8,3,2,4,8] B[8,4,8]找出A中是否有连续的B字母 不要用封装的函数直接写手写代码  ###
### 32、 ###
![](./img/110010.jpg)
### 33、写一个算法  一个楼层,你可以走一步或者两步,走完所有的台阶,总共有多少方案? ###
### 34、字典树: 定义一个字典树,实现插入和删除 ###

### 35、对机器学习的看法




# 第十二章 scala #
### 1、scala介绍一下 ###

​	Scala 是一种多范式语言，它一方面吸收继承了多种语言中的优秀特性，一方面又没有抛弃 Java 这个强大的平台，它运行在 Java 虚拟机 (Java Virtual Machine) 之上，轻松实现和丰富的 Java 类库互联互通。它既支持面向对象的编程方式，又支持函数式编程。它写出的程序像动态语言一样简洁，但事实上它确是严格意义上的静态语言，相对于Java而言，Scala的代码更为精简（减低犯错），而且功能更为广泛（Scala其实是Scalable Language 的简称，意为可扩展的语言），许多Scala的特性和语法都是针对Java的不足和弱点来设计的。 

### 2、关于scala，你有什么想说的吗？scala可以用在哪些方面呢？ ###

​	Scala的特点是有很多函数程式语言的特性（例如ML，Miranda, Scheme，Haskell），譬如惰性求值，list comprehension, type inference, anonymous function, pattern matching 等等，同时也包含 Object-Oriented 的特性（OO 能与 FP 混合使用是 Scala 的亮点）。此外，许多相似于高级编程语言的语法也渗入其中（例如 Python），不仅提高了 Scala 代码的可读性，维护、修改起来也较为省时省力。

​	scala语言主要在于Spark开发中进行使用，代码编写简洁方便，并且执行效率很高，相比较Java语言来说，代码可以减少几倍，还很灵活。

### 3、scala懒加载问题怎么处理？ ###

​	使用Lazy关键字进行懒加载操作

​	在一些情况中我们经常希望某些变量的初始化要延迟，并且表达式不会被重复计算。就像我们用Java实现一个懒汉式的单例。如：

    打开一个数据库连接。这对于程序来说，执行该操作，代价式昂贵的，所以我们一般希望只有在使用其的引用时才初始化。（当然实际开发中用的是连接池技术）
    为了缩短模块启动时间，可以将当前不需要的某些工作推迟执行。
    保证对象中其他字段的初始化能优先执行。
​	

### 4、Scala有break吗，Case class了解吗，哪里用到过？ ###

​	Scala没有break操作，但是可以实现break原理，需要创建Breaks对象实现内部的break方法就可以像java一样跳出语句，但是在模式匹配过程中不需要跳出匹配模式，因为模式匹配只能匹配其中一个结果值。

​	case class代表样例类，它和class类比较来说，可以不需要序列化，而class需要序列化操作，和object很类似，但是不同的是object不能传入参数，而case class可以带入参数，一般在做转换操作传参使用，比如DataSet操作的时候，转换RDD或者DataFream操作时候，可以使用case class进行参数的传递。

### 5、元组

1. 元组的创建

~~~
val tuple1 = (1, 2, 3, "heiheihei")

println(tuple1)

~~~

2. 元组数据的访问，注意元组元素的访问有下划线，并且访问下标从1开始，而不是0

~~~
val value1 = tuple1._4

println(value1)

~~~

3. 元组的遍历

~~~
方式1：
for (elem <- tuple1.productIterator  ) {
   print(elem)
}
方式2：
tuple1.productIterator.foreach(i => println(i))
tuple1.produIterator.foreach(print(_))
~~~

### 6、隐式转换

​	隐式转换函数是以implicit关键字声明的带有单个参数的函数。这种函数将会自动应用，将值从一种类型转换为另一种类型。

~~~
implicit def a(d: Double) = d.toInt
// 当执行这句代码的时候，内部会自动调用我们自己编写好的隐式转换方法
val i1: Int = 3.5
println(i1)
~~~

### 7、隐式转换应用场景

​	在scala语言中，隐式转换一般用于类型的隐式调用，亦或者是某个方法内的局部变量，想要让另一个方法进行直接调用，那么需要导入implicit关键字，进行隐式的转换操作，同时，在Spark Sql中，这种隐式转换大量的应用到了我们的DSL风格语法中，并且在Spark2.0版本以后，DataSet里面如果进行转换RDD或者DF的时候，那么都需要导入必要的隐式转换操作。

### 8、什么叫闭包

​	一个函数把外部的那些不属于自己的对象也包含(闭合)进来。

​	通俗的来说就是局部变量当全局变量来使用！！！

~~~
案例1：

def minusxy(x: Int) = (y: Int) => x - y

这就是一个闭包：

1) 匿名函数(y: Int) => x -y嵌套在minusxy函数中。

2) 匿名函数(y: Int) => x -y使用了该匿名函数之外的变量x

3) 函数minusxy返回了引用了局部变量的匿名函数

案例2

def minusxy(x: Int) = (y: Int) => x - y

val f1 = minusxy(10)

val f2 = minusxy(10)

println(f1(3) + f2(3))

此处f1,f2这两个函数就叫闭包。
~~~

### 9、解释一下Scala内的Option类型

​	在Scala语言中，Option类型是一个特殊的类型，它是代表有值和无值的体现，内部有两个对象，一个是Some一个是None，Some代表有返回值，内部有值，而None恰恰相反，表示无值，比如，我们使用Map集合进行取值操作的时候，当我们通过get取值，返回的类型就是Option类型，而不是具体的值。

### 10、解释一下什么叫偏函数

​	偏函数表示用{}包含用case进行类型匹配的操作，这种操作一般用于匹配唯一的属性值，在Spark中的算子内经常会遇到，例

~~~
val rdd = sc.textFile(路径)
rdd.map{
    case (参数)=>{返回结果}
}
~~~

### 11、手写Scala单例模式

​	单例模式是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例的特殊类。通过单例模式可以保证系统中一个类只有一个实例。 

~~~
/**
  * scala中关于单例的模拟
  * object中的属性和方法都可以当做类似java中的静态成员，都可以通过
  * object.成员来进行调用
  */
object SingletonOps {
  def main(args: Array[String]): Unit = {
    val singleton1 = Singleton.getInstance
    val singleton2 = Singleton.getInstance
    println(singleton1 == singleton2)
    singleton1.index = 5
    println("singleton1.index： " + singleton1.index)
    println("singleton2.index： " + singleton2.index)
  }
}
 
object Singleton {
  private val singleton = Singleton;
  def getInstance = singleton
  var index = 1
} 
~~~

### 12、解释一下柯里化

​	定义：柯里化指的是将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有的第二个参数作为参数的函数　

　　例如：

~~~
　　def mul(x:Int,y:Int) = x * y  //该函数接受两个参数
　　def mulOneAtTime(x:Int) = (y:Int) => x * y  //该函数接受一个参数生成另外一个接受单个参数的函数
    这样的话，如果需要计算两个数的乘积的话只需要调用：
    mulOneAtTime(5)(4)
　　这就是函数的柯里化
~~~

### 13、Scala中的模式匹配和Java的匹配模式的区别

　　　　scala的模式匹配包括了一系列的备选项，每个替代项以关键字大小写为单位，每个替代方案包括一个模式或多个表达式，如果匹配将会进行计算，箭头符号=>将模式与表达式分离

　　　　例如:

~~~
obj match{
　　　　　　case 1 => "one"
　　　　　　case 2 => "two"
　　　　　　case 3 => "three"
　　　　　　case _ => default
　　　　}
~~~

​	而Java的匹配模式是switch case匹配方式，它内部匹配的类型有局限性，并且需要用Break跳出匹配模式，而Scala中只会匹配其中一个结果，同时匹配类型居多，如String、Array、List、Class等..

### 14、Scala中的伴生类和伴生对象是怎么一回事

　　在scala中，单例对象与类同名时，该对象被称为该类的伴生对象，该类被称为该对象的伴生类。

　　伴生类和伴生对象要处在同一个源文件中

　　伴生对象和伴生类可以互相访问其私有成员

　　不与伴生类同名的对象称之为孤立对象

### 15 、谈谈Scala的尾递归

​	正常得递归，每一次递归步骤，需要保存信息到堆栈中去，当递归步骤很多的时候，就会导致内存溢出

　　而尾递归，就是为了解决上述的问题，在尾递归中所有的计算都是在递归之前调用，编译器可以利用这个属性避免堆栈错误，尾递归的调用可以使信息不插入堆栈，从而优化尾递归

例如:

```
5 + sum(4) // 暂停计算 => 需要添加信息到堆栈
5 + (4 + sum(3))
5 + (4 + (3 + sum(2)))
5 + (4 + (3 + (2 + sum(1))))
5 + (4 + (3 + (2 + 1)))
15
tailSum(4, 5) // 不需要暂停计算
tailSum(3, 9)
tailSum(2, 12)
tailSum(1, 14)
tailSum(0, 15)
15
```

### 16、函数中 Unit是什么意思？ 

​	Scala中的Unit类型类似于java中的void，无返回值。主要的不同是在Scala中可以有一个Unit类型值，也就是（），然而java中是没有void类型的值的。除了这一点，Unit和void是等效的。一般来说每一个返回void的java方法对应一个返回Unit的Scala方法。

### 17、Scala中的to和until 有什么区别？

​	例如1to10，它会返回Range（1,2,3,4,5,6,7,8,9,10），而1until  10 ，它会返回Range（1,2,3,4,5,6,7,8,9）

也就是说to包头包尾，而until  包头不包尾！

### 18、**var，val和def三个关键字之间的区别？** 

​	var是变量声明关键字，类似于Java中的变量，变量值可以更改，但是变量类型不能更改。  val常量声明关键字。  def 关键字用于创建方法（注意方法和函数的区别）  还有一个lazy val（惰性val）声明，意思是当需要计算时才使用，避免重复计算 

```
代码示例：
var x = 3 //  x是Int类型
x = 4      // 
x = "error" // 类型变化，编译器报错'error: type mismatch'
val y = 3
y = 4        //常量值不可更改，报错 'error: reassignment to val'
def fun(name: String) = "Hey! My name is: " + name
fun("Scala") // "Hey! My name is: Scala"
//注意scala中函数式编程一切都是表达式
lazy val x = {
  println("computing x")
  3
}
val y = {
  println("computing y")
  10
}
x+x  //
y+y  // x 没有计算, 打印结果"computing y" 
```

### 19、trait（特质）和abstract class（抽象类）的区别？

 （1）一个类只能集成一个抽象类，但是可以通过with关键字继承多个特质； 

 （2）抽象类有带参数的构造函数，特质不行（如 trait t（i：Int）{} ，这种声明是错误的）

### 20、unapply 和apply方法的区别， 以及各自使用场景？

​	先讲一个概念——提取器，它实现了构造器相反的效果，构造器从给定的参数创建一个对象，然而提取器却从对象中提取出构造该对象的参数，scala标准库预定义了一些提取器，如上面提到的样本类中，会自动创建一个伴生对象（包含apply和unapply方法）。  为了成为一个提取器，unapply方法需要被伴生对象。  apply方法是为了自动实现样本类的对象，无需new关键字。 

### 21、Scala类型系统中Nil, Null, None, Nothing四个类型的区别？

​	Null是一个trait（特质），是所以引用类型AnyRef的一个子类型，null是Null唯一的实例。  

​	Nothing也是一个trait（特质），是所有类型Any（包括值类型和引用类型）的子类型，它不在有子类型，它也没有实例，实际上为了一个方法抛出异常，通常会设置一个默认返回类型。  

​	Nil代表一个List空类型，等同List[Nothing]  

​	None是Option monad的空标识 

### 22、call-by-value和call-by-name求值策略的区别？

（1）call-by-value是在调用函数之前计算；  

（2） call-by-name是在需要时计算 

```
示例代码
//声明第一个函数
def func(): Int = {
  println("computing stuff....")
  42 // return something
}
//声明第二个函数，scala默认的求值就是call-by-value
def callByValue(x: Int) = {
  println("1st x: " + x)
  println("2nd x: " + x)
}
//声明第三个函数，用=>表示call-by-name求值
def callByName(x: => Int) = {
  println("1st x: " + x)
  println("2nd x: " + x)
}
//开始调用
//call-by-value求值
callByValue(func())   
//输出结果
//computing stuff....  
//1st x: 42  
//2nd x: 42
//call-by-name求值
callByName(func())   
//输出结果
//computing stuff....  
//1st x: 42  
//computing stuff....
//2nd x: 42
```

### 23、yield如何工作？comprehension（推导式）的语法糖是什么操作？

​	yield用于循环迭代中生成新值，yield是comprehensions的一部分，是多个操作（foreach, map, flatMap, filter or withFilter）的composition语法糖。 

​	comprehension（推导式）是若干个操作组成的替代语法。如果不用yield关键字，comprehension（推导式）可以被forech操作替代，或者被map/flatMap，filter代替。 

```
示例代码：
// 三层循环嵌套
for {
  x <- c1
  y <- c2
  z <- c3 if z > 0
} yield {...}
//上面的可转换为
c1.flatMap(x => c2.flatMap(y => c3.withFilter(z => z > 0).map(z => {...})))
```

### 24、什么是高阶函数？

​	高阶函数指能接受或者返回其他函数的函数，scala中的filter map flatMap函数都能接受其他函数作为参数。 

### 25、scala全排序过滤字段，求 1 to 4 的全排序， 2不能在第一位， 3,4不能在一起 

~~~scala
import util.control.Breaks._ 

- 1 to 4 的全排序
- 2不能在第一位
- 3,4不能在一起
 object LocalSpark extends App{
    override def main(args: Array[String]): Unit = { 
    List(1,2,3,4).permutations.filter(list=>list(0) != 2).map(list=>{
    var num =0
    breakable{ 
    for(x<- 0 to (list.size-1)){ 
    if(list(x)==3 && x<3 && list(x+1)==4) break
    if(list(x)==3 && x>0 && list(x-1)==4) break 
    num +=1 
    } 
    } 
    if(num <4){ 
    List()
    }else{
    list 
    } 
    }).filter(list=>list.size>3).foreach(println(_))
    }
    }

结果
List(1, 3, 2, 4)
List(1, 4, 2, 3)
List(3, 1, 2, 4)
List(3, 1, 4, 2)
List(3, 2, 1, 4)
List(3, 2, 4, 1)
List(4, 1, 2, 3)
List(4, 1, 3, 2)
List(4, 2, 1, 3)
List(4, 2, 3, 1)
~~~

# 第十三章 其他题型 #

### 1、你最擅长的技术点是什么，详细的描述下 ###
```
①Hadoop生态相关的框架。
hadoop 生态概况Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。具有可靠、高效、可伸缩的特点。
Hadoop的核心是YARN,HDFS和Mapreduce。在未来一段时间内，hadoop将于spark共存，hadoop与spark都能部署在yarn、mesos的资源管理系统之上

a)HDFS（Hadoop分布式文件系统）源自于Google的GFS论文，发表于2003年10月，HDFS是GFS克隆版。HDFS是Hadoop体系中数据存储管理的基础。它是一个高度容错的系统，能检测和应对硬件故障，用于在低成本的通用硬件上运行。HDFS简化了文件的一致性模型，通过流式数据访问，提供高吞吐量应用程序数据访问功能，适合带有大型数据集的应用程序。它提供了一次写入多次读取的机制，数据以块的形式，同时分布在集群不同物理机器上。

b)Mapreduce（分布式计算框架）源自于google的MapReduce论文，发表于2004年12月，Hadoop MapReduce是google MapReduce 克隆版。MapReduce是一种分布式计算模型，用以进行大数据量的计算。它屏蔽了分布式计算框架细节，将计算抽象成map和reduce两部分，其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果。Reduce则对中间结果中相同“键”的所有“值”进行规约，以得到最终结果。MapReduce非常适合在大量计算机组成的分布式并行环境里进行数据处理。

c)HBASE（分布式列存数据库）源自Google的Bigtable论文，发表于2006年11月，HBase是Google Bigtable克隆版HBase是一个建立在HDFS之上，面向列的针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库。HBase采用了BigTable的数据模型：增强的稀疏排序映射表（Key/Value），其中，键由行关键字、列关键字和时间戳构成。HBase提供了对大规模数据的随机、实时读写访问，同时，HBase中保存的数据可以使用MapReduce来处理，它将数据存储和并行计算完美地结合在一起。

d)Zookeeper（分布式协作服务）源自Google的Chubby论文，发表于2006年11月，Zookeeper是Chubby克隆版解决分布式环境下的数据管理问题：统一命名，状态同步，集群管理，配置同步等。Hadoop的许多组件依赖于Zookeeper，它运行在计算机集群上面，用于管理Hadoop操作。

e)HIVE（数据仓库）由facebook开源，最初用于解决海量结构化的日志数据统计问题。Hive定义了一种类似SQL的查询语言(HQL),将SQL转化为MapReduce任务在Hadoop上执行。通常用于离线分析。HQL用于运行存储在Hadoop上的查询语句，Hive让不熟悉MapReduce开发人员也能编写数据查询语句，然后这些语句被翻译为Hadoop上面的MapReduce任务。

f)Sqoop(数据ETL/同步工具）Sqoop是SQL-to-Hadoop的缩写，主要用于传统数据库和Hadoop之前传输数据。数据的导入和导出本质上是Mapreduce程序，充分利用了MR的并行化和容错性。Sqoop利用数据库技术描述数据架构，用于在关系数据库、数据仓库和Hadoop之间转移数据。

g)Flume（日志收集工具）Cloudera开源的日志收集系统，具有分布式、高可靠、高容错、易于定制和扩展的特点。它将数据从产生、传输、处理并最终写入目标的路径的过程抽象为数据流，在具体的数据流中，数据源支持在Flume中定制数据发送方，从而支持收集各种不同协议数据。同时，Flume数据流提供对日志数据进行简单处理的能力，如过滤、格式转换等。此外，Flume还具有能够将日志写往各种数据目标（可定制）的能力。总的来说，Flume是一个可扩展、适合复杂环境的海量日志收集系统。当然也可以用于收集其他类型数据

h)Oozie(工作流调度器）Oozie是一个可扩展的工作体系，集成于Hadoop的堆栈，用于协调多个MapReduce作业的执行。它能够管理一个复杂的系统，基于外部事件来执行，外部事件包括数据的定时和数据的出现。Oozie工作流是放置在控制依赖DAG（有向无环图 Direct Acyclic Graph）中的一组动作（例如，Hadoop的Map/Reduce作业、Pig作业等），其中指定了动作执行的顺序。Oozie使用hPDL（一种XML流程定义语言）来描述这个图。

i)Yarn(分布式资源管理器）YARN是下一代MapReduce，即MRv2，是在第一代MapReduce基础上演变而来的，主要是为了解决原始Hadoop扩展性较差，不支持多计算框架而提出的。Yarn是下一代 Hadoop 计算平台，yarn是一个通用的运行时框架，用户可以编写自己的计算框架，在该运行环境中运行。用于自己编写的框架作为客户端的一个lib，在运用提交作业时打包即可。该框架为提供了以下几个组件：  - 资源管理：包括应用程序管理和机器资源管理  - 资源双层调度  - 容错性：各个组件均有考虑容错性  - 扩展性：可扩展到上万个节点

j)Phoenix（hbase sql接口）Apache Phoenix 是HBase的SQL驱动，Phoenix 使得Hbase 支持通过JDBC的方式进行访问，并将你的SQL查询转换成Hbase的扫描和相应的动作。

②Spark生态相关的框架。
a)Spark(内存DAG计算模型)Spark是一个Apache项目，它被标榜为“快如闪电的集群计算”。它拥有一个繁荣的开源社区，并且是目前最活跃的Apache项目。最早Spark是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用的并行计算框架。Spark提供了一个更快、更通用的数据处理平台。和Hadoop相比，Spark可以让你的程序在内存中运行时速度提升100倍，或者在磁盘上运行时速度提升10倍。

b)GraphX(图计算模型）Spark GraphX最先是伯克利AMPLAB的一个分布式图计算框架项目，目前整合在spark运行框架中，为其提供BSP大规模并行图计算能力。

c)MLib（机器学习库）Spark MLlib是一个机器学习库，它提供了各种各样的算法，这些算法用来在集群上针对分类、回归、聚类、协同过滤等。

d)Streaming（流计算模型）Spark Streaming支持对流数据的实时处理，以微批的方式对实时数据进行计算

e)Kafka（分布式消息队列）Kafka是Linkedin于2010年12月份开源的消息系统，它主要用于处理活跃的流式数据。活跃的流式数据在web网站应用中非常常见，这些数据包括网站的pv、用户访问了什么内容，搜索了什么内容等。这些数据通常以日志的形式记录下来，然后每隔一段时间进行一次统计处理。
```



### 2、你都会什么语言？python会多少呢？ ###

```
熟练掌握如下语言：
①Java → 
a)Java是一门面向对象编程语言，不仅吸收了C++语言的各种优点，还摒弃了C++里难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。Java语言作为静态面向对象编程语言的代表，极好地实现了面向对象理论，允许程序员以优雅的思维方式进行复杂的编程。
b)Java具有简单性、面向对象、分布式、健壮性、安全性、平台独立与可移植性、多线程、动态性等特点。Java可以编写桌面应用程序、Web应用程序、分布式系统和嵌入式系统应用程序等。

②Scala→
Scala是一门多范式的编程语言，一种类似java的编程语言，设计初衷是实现可伸缩的语言 、并集成面向对象编程和函数式编程的各种特性。

③python→
Python是一种计算机程序设计语言。是一种面向对象的动态类型语言，最初被设计用于编写自动化脚本(shell)，随着版本的不断更新和语言新功能的添加，越来越多被用于独立的、大型项目的开发。
```



### 3、介绍你项目中的数据流向，并说出在数据流向中遇到的主要问题有哪些？ ###

①数仓项目数据流向

![](./oimg/janson01.png)

②大数据项目数据流向

![](./oimg/janson02.png)

### 4、Dubbo是啥？你为啥要用dubbo？你们公司主要用的是啥？ ###

Dubbo 是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring 框架无缝集成。

 

主要核心部件：

Remoting: 网络通信框架，实现了 sync-over-async 和 request-response 消息机制

RPC: 一个远程过程调用的抽象，支持负载均衡、容灾和集群功能

Registry: 服务目录框架用于服务的注册和服务事件发布和订阅

 

Dubbo 工作原理

![](./oimg/janson03.png)

Provider

暴露服务方称之为“服务提供者”

 

Consumer

调用远程服务方称之为“服务消费者”

 

Registry

服务注册与发现的中心目录服务称之为“服务注册中心”

 

Monitor

统计服务的调用次调和调用时间的日志服务称之为“服务监控中心”

### 5、Dubbo可以直接链接redis吗? ###

可以。使用Redis注册中心

当使用Redis注册中心，需先把服务提供方和消费放的注册中心xml配置修改为下方

<dubbo:registry address="redis://localhost:6379"/>

### 6、两个数据量特别大的文件都在一台机器中，只提供1个G的内存，问如何查找到两个文件的交集部分？ ###

```
（1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。
（2）但上述方法有一个明显问题，加载一个文件的数据需要50亿*64bytes = 320G远远大于1G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。
（3）针对上述问题，我们分治算法的思想。
step1：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,...,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计）
step2：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,...,b999)（为什么要这样做? 文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中）
所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url）
step3：因为每个hash大约300M，所以我们再可以采用（1）中的想法
```



### 7、了解Canal吗,对数据库binlog提取还原数据这一方面 ###
```
canal是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析，提供增量数据订阅&消费，目前主要支持了MySQL（也支持mariaDB）。
binlog他是一个二进制的文件，要恢复需要通过转换成正常的sql脚本然后执行数据的恢复，或者采用管道流的方式执行（实际还是转换成脚本的形式执行）
（1）要读取binlog文件必须首先开启binlog日志查看方式如下：
```

![](./oimg/janson04.png)

![](./oimg/janson05.png)

![](./oimg/janson06.png)

### 8、给你一个用户信息包（包含用户的各种id），如何与你公司已有的用户之间进行匹配，使用高效的方式？

建议使用Spark Core来实现。具体步骤如下：

①将用户信息包装载到内存中，并封装到RDD中，collect之后，进而将其封装到广播变量中。、

②将公司已有的用户也装载到内存中，封装为新的RDD,分析每个元素，从广播变量中读取用户信息包，找出相同id对应的用户信息。

总结：借助广播变量，将Reduce端的join修改成了Map Join,避免了数据倾斜的产生。且使用Spark ,提高了运行的速度。

### 9、什么是留存，如何计算留存率 ###

①留存→

a）在互联网行业当中，因为拉新或推广的活动把客户引过来，用户开始访问公司的网站，但是经过一段时间可能就会有一部分客户逐渐流失了。那些留下来的人或者是经常回访我们公司网站的人就称为留存。

b)现在大家经常会用到所谓的“日活”(日活跃用户量，简称DAU)来监测我们的网站，有的时候会看到我们的“日活”在一段时期内都是逐渐增加的，这是一个非常好的现象。但是如果我们忽略了留存分析的话，这个结果很可能是一个错误。

c)比如某公司做了很多拉新、推广的活动，用户是带来了很多，但是留下来或经常返回来的客户不一定增长，他们有可能是在减少，只不过是拉新过来的人太多了而掩盖了流失率居高不下的问题，实际上客户的留存是在逐渐降低的。这个时候留存分析就很重要!

 

②计算留存→

a）与留存率相关的指标一般有：次日留存率、二日留存率、三日留存率、七日留存率等。

b）次日留存率=基准日注册的人数在第二天登录的人数/基准日注册的人数

二日留存率=基准日注册的人数在第三天登录的人数/基准日注册的人数

三日留存率=基准日注册的人数在第四天登录的人数/基准日注册的人数

四日留存率=基准日注册的人数在第五天登录的人数/基准日注册的人数

...

以此类推。

### 10、如何保证累加器的精准性？ ###

```
使用Accumulator时，为了保证准确性，只使用一次action操作。如果需要使用多次则使用cache或persist操作切断依赖。
```

### 11、使用图计算可以将web上的用户数据与app的用户数据进行聚合吗？ ###

①可以。

②GraphX是Spark中用于图和图计算的组件，GraphX通过扩展Spark RDD引入了一个新的图抽象数据结构，一个将有效信息放入顶点和边的有向多重图。如同Spark的每一个模块一样，它们都有一个基于RDD的便于自己计算的抽象数据结构（如SQL的DataFrame，Streaming的DStream）。为了方便与图计算，GraphX公开了一系列基本运算（InDegress,OutDegress,subgraph等等 ），也有许多用于图计算算法工具包（PageRank,TriangleCount,ConnectedComponents等等算法）。相对于其他分布式图计算框架，Graphx最大的贡献，也是大多数开发喜欢它的原因是，在Spark之上提供了一站式解决方案，可以方便且高效地完成图计算的一整套流水作业；即在实际开发中，可以使用核心模块来完成海量数据的清洗与与分析阶段，SQL模块来打通与数据仓库的通道，Streaming打造实时流处理通道，基于GraphX图计算算法来对网页中复杂的业务关系进行计算，最后使用MLLib以及SparkR来完成数据挖掘算法处理。

### 12、sqoop增量还是全量 ###

```
①Sqoop支持两种方式的全量数据导入和增量数据导入，同时可以指定数据是否以并发形式导入。
②增量导入案例（对事实表进行增量导入）：
sqoop job --create bap_code_category -- import \
--connect jdbc:mysql://NODE03:3306/mtbap \
--driver com.mysql.jdbc.Driver \
--username root \
--password-file /sqoop/pwd/mysql.pwd \
--table code_category \
--delete-target-dir \
--target-dir /mtbap/ods/ods_code_category \
--fields-terminated-by '\001' \

③全量导入案例（对维度表进行全量导入）：
sqoop job --create bap_user_order -- import \
--connect jdbc:mysql://NODE03:3306/mtbap?dontTrackOpenResources=true\&defaultFetchSize=1000\&useCursorFetch=true \
--driver com.mysql.jdbc.Driver \
--username root \
--password-file /sqoop/pwd/mysql.pwd \
--table user_order \
--target-dir /mtbap/ods_tmp/ods_user_order/ \
--fields-terminated-by '\001' \
--check-column order_id \
--incremental append \
--last-value 0 \
```



### 13、kylin用的什么软件做可视化界面，生成cube模型过程中，遇到过什么问题 ###

①kylin与superset集成实现数据可视化。

②Apache kylin是一个开源分布式引擎，提供Hadoop之上的SQL查询接口及多维分析(OLAP)能力以支持超大规模数据。而superset是airbnb开源的一款数据可视化工具。

 

③kylin在超大数据规模下仍然可以提供秒级甚至毫秒级sql响应的OLAP多维分析查询服务。而且对服务器内存的要求也不像spark sql那么高，经过多方面的优化，数据膨胀率甚至可以控制在100%以内。它利用hive做预计算，然后建立多维的数据立方体，并存在hbase中，从而提供了实时查询的能力。

 

④superset也就是早先的caravel，提供了丰富的图表供用户配置。只要连上数据源，勾几个简单的配置，或者写点sql。用户就可以轻易的构建基于d3、nvd3、mapbox-gl等的炫酷图表。

 

Kylin作为一个OLAP引擎，需要Cube模型支撑，在我们的工作过程中，在和用户以及相关的开发人员、测试、产品等介绍Kylin的过程中，他们总是会对Cube的模型有一些疑惑，作为经常接触这个概念的我来说这是再明了不过的了，而他们还是会在我讲解多次之后表示还在云里雾里，所以就希望通过一篇关于Cube和Kylin创建Cube的过程来聊一下Cube是什么，以及Kylin的一些高级设置。

### 14、用户画像系统,打了什么标签,具体说一个怎么打的 ###

①行为标签

②基于行为的兴趣标签

③社会属性标签

就是用户的年龄、性别，等，直接用户填写的表单，或其他方式收集到的数据，存上

④模型类标签

基于基础数据，通过模型计算出来，然后打标签

### 15、某人乘车，给个数组里面是乘车日期，日卡2元，周卡7元，月卡15元，求某天怎么买最划算，手写代码（说是用动态规划比较简单） ###

有关动态规划（Dynamic Programming）算法：

→ 动态规划算法的核心，由下面的图片和小故事可以知道动态规划算法的核心就是记住已经解决过的子问题的解。

A * "1+1+1+1+1+1+1+1 =？" *

 

A : "上面等式的值是多少"

B : *计算* "8!"

 

A *在上面等式的左边写上 "1+" *

A : "此时等式的值为多少"

B : *quickly* "9!"

A : "你怎么这么快就知道答案了"

A : "只要在8的基础上加1就行了"

A : "所以你不用重新计算因为你记住了第一个等式的值为8!动态规划算法也可以说是 '记住求过的解来节省时间'"

 

→ 动态规划算法的两种形式：

上面已经知道动态规划算法的核心是记住已经求过的解，记住求解的方式有两种：

​	①自顶向下的备忘录法 

​	②自底向上

→ 一般来说由于备忘录方式的动态规划方法使用了递归，递归的时候会产生额外的开销，使用自底向上的动态规划方法要比备忘录方法好。 

→ 动态规划原理

虽然已经用动态规划方法解决了上面两个问题，但是大家可能还跟我一样并不知道什么时候要用到动态规划。总结一下上面的斐波拉契数列和钢条切割问题，发现两个问题都涉及到了重叠子问题，和最优子结构。

①最优子结构

用动态规划求解最优化问题的第一步就是刻画最优解的结构，如果一个问题的解结构包含其子问题的最优解，就称此问题具有最优子结构性质。因此，某个问题是否适合应用动态规划算法，它是否具有最优子结构性质是一个很好的线索。使用动态规划算法时，用子问题的最优解来构造原问题的最优解。因此必须考查最优解中用到的所有子问题。

②重叠子问题

在斐波拉契数列和钢条切割结构图中，可以看到大量的重叠子问题，比如说在求fib（6）的时候，fib（2）被调用了5次，在求cut（4）的时候cut（0）被调用了4次。如果使用递归算法的时候会反复的求解相同的子问题，不停的调用函数，而不是生成新的子问题。如果递归算法反复求解相同的子问题，就称为具有重叠子问题（overlapping subproblems）性质。在动态规划算法中使用数组来保存子问题的解，这样子问题多次求解的时候可以直接查表不用调用函数递归。

### 16、手写快排，讲一下原理 ###

快速排序（Quicksort）是对冒泡排序的一种改进。

快速排序由C. A. R. Hoare在1960年提出。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。

```java
public static int[] qsort(int arr[], int start, int end) {
    int pivot = arr[start];
    int i = start;
    int j = end;
    while (i < j) {
        while ((i < j) && (arr[j] > pivot)) {
            j--;
        }
        while ((i < j) && (arr[i] < pivot)) {
            i++;
        }
        if ((arr[i] == arr[j]) && (i < j)) {
            i++;
        } else {
            int temp = arr[i];
            arr[i] = arr[j];
            arr[j] = temp;
        }
    }
    if (i - 1 > start) {
        arr = qsort(arr, start, i - 1);
    }

    if (j + 1 < end) {
        arr = qsort(arr, j + 1, end);
    }
    return (arr);
}
public static void main(String[] args) {
    int arr[] = new int[]{3, 3, 3, 7, 9, 122344, 4656, 34, 34, 4656, 5, 6, 7, 8, 9, 343, 57765, 23, 12321};
    int len = arr.length - 1;
    arr = qsort(arr, 0, len);
    for (int i : arr) {
        System.out.print(i + "\t");
    }
}
运行结果：
```

![](./oimg/janson07.png)

### 17、算法:1.数组1{node1，node2，node3 ...} 数组2{[node1,node2],[node1,node3],[node2,node3]}数组一可以被看为一个点，数组而看为一个有向边，求出数组二中具有环形的数组 2.单项链表 node1-》node2-》node3 ... 把倒数第N个节点删除，并让上一节点指向N的下一个节点 ###

题解：给的 n 始终是有效的。尝试一次遍历实现。

例子：给定一个链表: 1->2->3->4->5, 并且 n = 2。当删除了倒数第二个节点后链表变成了 1->2->3->5。

思路： 既然只允许遍历一次，且N一直有效，那么可以利用双指针解法（前指针、后指针），让前指针先走N步，再让两个在指针同时后移，直到前指针到达尾部，此时，后指针的下一个节点就是要被删除的节点了。

 

Java代码参考：

```java
@Test
public void test() {
    ListNode head = new ListNode(1);
    ListNode listNode = removeNthFromEnd(head, 1);
    System.out.println(listNode);

}

public ListNode removeNthFromEnd(ListNode head, int n) {

    ListNode preNode = head;
    ListNode curNode = head;

    for (int i = 0; i < n; i++) {
        curNode = curNode.next;
    }

    if (curNode == null) {
        return preNode.next;
    }

    while (curNode.next != null) {
        preNode = preNode.next;
        curNode = curNode.next;
    }

    preNode.next = preNode.next.next;

    return head;
}


class ListNode {
    int val;
    ListNode next;

    ListNode(int x) {
        val = x;
    }
}
```



### 18、说一下怎么实现实时统计每天的uv量； ###

### 19、说一下怎么实现实时统计每天的uv量； ###
![](./oimg/janson08.png)

→流程：

日志数据从flume采集过来，落到hdfs供其它离线业务使用，也会sink到kafka，SparkStreaming从kafka拉数据过来，计算pv,uv，uv是用的redis的set集合去重，最后把结果写入mysql数据库，供前端展示使用。

 

→uv的计算：

uv是要全天去重的，每次进来一个batch的数据，如果用原生的reduceByKey或者groupByKey对配置要求太高，在配置较低情况下，我们申请了一个93G的redis用来去重，原理是每进来一条数据，将date作为key，guid加入set集合，20秒刷新一次，也就是将set集合的尺寸取出来，更新一下数据库即可。

### 20、100W条数据求TOPN，只能用java、scala、python或C实现。 ###

思路（可以使用scala高效地予以实现）：

可以局部排序(局部数据是全局数据的某一个范围)，最后合并到全同一个文件，保证全局有序，这样可以设置一个reduce任务实现。然后使用RDD的算子take(N)可以取出前几个值。

 

具体实现（伪代码）：

数据处理中, 遇到取TopN的问题. 在Spark中,取TopN有如下的方法:

 

→ 生成rdd 

读取数据源的数据并转为rdd。val rdd = sc.textFile()

 

→ 分区

将rdd划分分区,分区的个数根据实际的数据量和计算集群机器的数量以及核心数确定。val partitionedRDD = rdd.coalesce(partitions)

 

→ kv变换

把每条数据转换为(k,v)形式, k为键,v为值且类型为数字类型。val kv = partitionedRDD.map(line => .. (k,v))

 

→ 去重

如果键值不唯一,则需要根据键值去重,并把key相同的value值进行累加。val uniqueKeys = kv.reduceByKey((a,b) => a + b)

 

→  每个分区本地TopN

 

利用分布式计算的优势,在各个分区生成一个本地TopN.这里有几种方法可以选择.

```java
方法1: mapParititions + SortedMap(TreeMap)
val partitions = uniqueKeys.mapPartitions(mp => {
 
    val localTopN = new TreeMap[Integer, String]()
    for (item <- mp){
        localTopN.put(item._2, item._1)
        // 只保留top N
        if (localTopN.size > N){
            // 删除频度最小的元素
            localTopN.remove(localTopN.first)
        }
    }
    Collections.singletonList(localTopN)
 
})

→  收集每个分区的TopN，取最终的TopN

val allTopN = partitions.collect()
val finalTopN = new TreeMap[Integer, String]()
for (item <- allTopN){
    finalTopN.put(item._1, item._2)
    // 只保留top N
    if (finalTopN.size > N){
        finalTopN.remove(finalTopN.first)
    }
}

方法2:Spark提供了topN的算子, 使用takeOrdered()
val topNResult = uniqueKeys.takeOrdered(N)(implicit Ordering[T])
```



### 21、了解ClickHouse吗 ###

```
Clickhouse是一个用于联机分析处理（OLAP）的列式数据库管理系统（columnar DBMS）。

传统数据库在数据大小比较小，索引大小适合内存，数据缓存命中率足够高的情形下能正常提供服务。但残酷的是，这种理想情形最终会随着业务的增长走到尽头，查询会变得越来越慢。你可能通过增加更多的内存，订购更快的磁盘等等来解决问题（纵向扩展），但这只是拖延解决本质问题。如果你的需求是解决怎样快速查询出结果，那么ClickHouse也许可以解决你的问题。
```

应用场景：

a）.绝大多数请求都是用于读访问的

b）.数据需要以大批次（大于1000行）进行更新，而不是单行更新；或者根本没有更新操作

c）.数据只是添加到数据库，没有必要修改

d）.读取数据时，会从数据库中提取出大量的行，但只用到一小部分列

e）.表很“宽”，即表中包含大量的列

f）.查询频率相对较低（通常每台服务器每秒查询数百次或更少）

g）.对于简单查询，允许大约50毫秒的延迟

h）.列的值是比较小的数值和短字符串（例如，每个URL只有60个字节）

i）.在处理单个查询时需要高吞吐量（每台服务器每秒高达数十亿行）

j）.不需要事务

k）.数据一致性要求较低

l）.每次查询中只会查询一个大表。除了一个大表，其余都是小表

m）.查询结果显著小于数据源。即数据有过滤或聚合。返回结果不超过单个服务器内存大小

 

相应地，使用ClickHouse也有其本身的限制：

a）不支持真正的删除/更新支持 不支持事务（期待后续版本支持）

b）不支持二级索引

c）有限的SQL支持，join实现与众不同

d）不支持窗口功能

e）元数据管理需要人工干预维护scala,python

### 22、了解分布式协议吗 ###

```
几种常见的分布式一致性协议介绍：
→Zab
把节点分两种，Leader（主）和Follower（从）。 有一个主节点，所有写操作全部通过节点进行处理，如果一个从节点收到了一个写操作请求，就会转给主节点处理。 其他节点都是从节点，可以通过从节点进行读操作。 主节点通过选举得出，主节点失踪后，其他从节点自动开始选举新的主节点。
使用Zookeeper
→Raft
Raft将一致性的问题分解为了三个问题：
Leader Election：在Leader故障的时候，选出一个新的Leader。
Log Replication：Leader需要让日志完整地复制到集群内的所有服务器
Safety：如果某个服务器在特定的index提交了一个日志，那么不能有其它的服务器在相同的index提交日志，同一时刻只能保证有一个Leader。
发现主节点失踪一段时间后，向所有从节点向其他从节点发消息，让他们选自己为新的主节点；
还没参加选举的节点如果收到其他节点的选举请求，就选举自己收到的第一个节点，后面谁再请求自己支持选举，就告诉他们我已经支持另一个节点了
如果一个节点发现另一个节点得到的支持比自己多，也就开始无条件支持那个节点选举，同时让支持自己的节点也去支持它
如果一轮没选出来得到大多数节点支持的主节点，就开始下一轮选举，直到一个节点得到了大部分节点支持，成为新的主节点；
使用Redis使用了类Raft的算法
→ Gossip
Gossip协议如其名，流行病协议，一个节点有状态需要更新到网络的其它节点的时候，它会随机的选择周围的几个节点散播消息，收到消息的节点会重复这个过程，直到网络中所有的节点都收到了消息。这个过程需要一定的时间，消息之间的传递具有一定的延迟性。但是理论上所有的节点都会收到所有的消息，因此它是一个最终一致性消息。
使用ElasticSearch在寻找Node时候使用了类Gossip的协议
→额外Lease
Lease 是由颁发者授予的在某一有效期内的承诺。颁发者一旦发 出 lease，则无论接受方是否收到，也无论后续接收方处于何种状态，只要 lease 不过期，颁发者一 定严守承诺；另一方面，接收方在 lease 的有效期内可以使用颁发者的承诺，但一旦 lease 过期，接 收方一定不能继续使用颁发者的承诺。
```



### 23、怎么测试网络io的速度 ###

```
网络的吞吐量是把单位时间内所有类型的包的数量都加起来。得到的结果既是输入输出的吞吐量。
可以下载软件，如：聚生网管等等。最简单也是最常用和有效的吞吐量测试方法就是将测试接入点选在链路两端的以太网网络上的测试。测试时在发送端在指定发送速度，在接收器上计算收到的帧的速度。吞吐量是接收器收到的好帧数量，时间，测试通过改变帧长度，重复以上测试得到不同速率下的测试结果。建议你测试吞吐量时反复测试下。目的是来确定在不同的传输速度时的吞吐量。
```



### 24、怎么理解插件？ ###

插件(Plug-in,又称addin、add-in、addon或add-on,又译外挂)是一种遵循一定规范的应用程序接口编写出来的程序。其只能运行在程序规定的系统平台下（可能同时支持多个平台），而不能脱离指定的平台单独运行。因为插件需要调用原纯净系统提供的函数库或者数据。很多软件都有插件，插件有无数种。例如在IE中，安装相关的插件后，WEB浏览器能够直接调用插件程序，用于处理特定类型的文件。插件的定位是开发实现原纯净系统平台、应用软件平台不具备的功能的程序，其只能运行在程序规定的系统平台下（可能同时支持多个平台），而不能脱离指定的平台单独运行。因为插件需要调用原纯净系统提供的函数库或者数据。

### 25、parquet格式的原理说一下？ ###

```
![janson09](E:\☆QF☆\GitLab\my-gitlab\qf\2019千锋面试宝典\./oimg\janson09.png)Parquet 是面向分析型业务的列式存储格式，由 Twitter 和 Cloudera 合作开发，2015 年 5 月从 Apache 的孵化器里毕业成为 Apache 顶级项目，最新的版本是 1.8.0。
列式存储
列式存储和行式存储相比有哪些优势呢？
可以跳过不符合条件的数据，只读取需要的数据，降低 IO 数据量。
压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如 Run Length Encoding 和 Delta Encoding）进一步节约存储空间。
只读取需要的列，支持向量运算，能够获取更好的扫描性能。
当时 Twitter 的日增数据量达到压缩之后的 100TB+，存储在 HDFS 上，工程师会使用多种计算框架（例如 MapReduce, Hive, Pig 等）对这些数据做分析和挖掘；日志结构是复杂的嵌套数据类型，例如一个典型的日志的 schema 有 87 列，嵌套了 7 层。所以需要设计一种列式存储格式，既能支持关系型数据（简单数据类型），又能支持复杂的嵌套类型的数据，同时能够适配多种数据处理框架。
关系型数据的列式存储，可以将每一列的值直接排列下来，不用引入其他的概念，也不会丢失数据。关系型数据的列式存储比较好理解，而嵌套类型数据的列存储则会遇到一些麻烦。如图 1 所示，我们把嵌套数据类型的一行叫做一个记录（record)，嵌套数据类型的特点是一个 record 中的 column 除了可以是 Int, Long, String 这样的原语（primitive）类型以外，还可以是 List, Map, Set 这样的复杂类型。在行式存储中一行的多列是连续的写在一起的，在列式存储中数据按列分开存储，例如可以只读取 A.B.C 这一列的数据而不去读 A.E 和 A.B.D，那么如何根据读取出来的各个列的数据重构出一行记录呢？
```

![](./oimg/janson09.png)

### 26、ES为什么查询那么快呢 ###

```
Elasticsearch是通过Lucene的倒排索引技术实现比关系型数据库更快的过滤。特别是它对多条件的过滤支持非常好，比如年龄在18和30之间，性别为女性这样的组合查询。倒排索引很多地方都有介绍，但是其比关系型数据库的b-tree索引快在哪里？到底为什么快呢？笼统的来说，b-tree索引是为写入优化的索引结构。当我们不需要支持快速的更新的时候，可以用预先排序等方式换取更小的存储空间，更快的检索速度等好处，其代价就是更新慢。
```



### 27、ES是怎么建立索引的 ###

```
→ 案例①创建一个索引库 (客户端向远程es服务器发送请求的方式包括：get,post,put,delete等等)
 curl -XPUT ‘http://JANSON01:9200/bigdata’
 返回值 {"acknowledged":true,"shards_acknowledged":true,"index":"bigdata"}  
```

### 28、ES添加数据时，是怎么添加索引的。 ###

```
→案例：在索引库中添加若干索引信息   (类比：在mysql db server名为bigdata的db中的表product中插入多条记录）
curl -H 'Content-Type: application/json'  -XPOST  'http://JANSON01:9200/bigdata/product/1?pretty' -d '{"name":"hadoop","author":"Doc Culting","version":"3.0.1" }'
curl -H 'Content-Type: application/json'  -XPOST  'http://JANSON01:9200/bigdata/product/2?pretty' -d '{"name":"hive","author":"刘德华","version":"2.5.1" }'
curl -H 'Content-Type: application/json'  -XPOST  'http://JANSON01:9200/bigdata/product?pretty' -d '{"name":"sqoop","author":"楚留香","version":"1.9.8" }'
curl -H 'Content-Type: application/json'  -XPOST  'http://JANSON01:9200/bigdata/product?pretty' -d '{"name":"flume","author":"lesie","version":"1.5.1" }'
curl -H 'Content-Type: application/json'  -XPOST  'http://JANSON01:9200/bigdata/product?pretty' -d '{"name":"hbase","author":"jack","version":"3.9.8" }'


说明： 
a)与传统的rdbms不一样，可以事先不创建表，而是在插入记录的时候指定表名即可。
那么，type中的字段名和类型由字段名和字段值来决定。（字段值）
				
b)  参数名              参数值                             含义
————————————————————————————————————--------------------------------------——————————
 -H 		Content-Type: application/json	用来指定要插入到type中的document的数据格	 
 -XPOST   'http://JANSON01:9200/xxx'       用来向指定的索引库指定的type中新增索引信息（document）
 -d        '{"name":"hbase","..}       用来指定document的详情; -d参数后的值外层使用单引号括起来，里面使用双引号
```



### 29、phoenix用的多吗? ###

```
Phonenix：
Phoenix是构建在HBase上的一个SQL层，能让我们用标准的JDBC APIs而不是HBase客户端APIs来创建表，插入数据和对HBase数据进行查询。 
Phoenix完全使用Java编写，作为HBase内嵌的JDBC驱动。Phoenix查询引擎会将SQL查询转换为一个或多个HBase扫描，并编排执行以生成标准的JDBC结果集。直接使用HBase API、协同处理器与自定义过滤器，对于简单查询来说，其性能量级是毫秒，对于百万级别的行数来说，其性能量级是秒。 
Phoenix通过以下方式使我们可以少写代码，并且性能比我们自己写代码更好：
将SQL编译成原生的HBase scans。 确定scan关键字的最佳开始和结束。让scan并行执行。
```

### 30、es中quary和filter的区别,使用的时候用什么注意点 ###

```
查询虽然包含这两种，但是查询在不同的执行环境下，操作还是不一样的。
Query与Filter
	查询在Query查询上下文和Filter过滤器上下文中，执行的操作是不一样的：
Query查询上下文：
	在查询上下文中，查询会回答这个问题——“这个文档匹不匹配这个查询，它的相关度高么？”
	如何验证匹配很好理解，如何计算相关度呢？之前说过，ES中索引的数据都会存储一个_score分值，分值越高就代表越匹配。另外关于某个搜索的分值计算还是很复杂的，因此也需要一定的时间。
	查询上下文 是在 使用query进行查询时的执行环境，比如使用search的时候。
Filter过滤器上下文：
	在过滤器上下文中，查询会回答这个问题——“这个文档匹不匹配？”
	答案很简单，是或者不是。它不会去计算任何分值，也不会关心返回的排序问题，因此效率会高一点。
	过滤上下文 是在使用filter参数时候的执行环境，比如在bool查询中使用Must_not或者filter。
另外，经常使用过滤器，ES会自动的缓存过滤器的内容，这对于查询来说，会提高很多性能。
总结
	1 查询上下文中，查询操作不仅仅会进行查询，还会计算分值，用于确定相关度；在过滤器上下文中，查询操作仅判断是否满足查询条件
	2 过滤器上下文中，查询的结果可以被缓存。
```



### 31、 ###

![](./img/130001.jpg)

![](./oimg/janson10.png)

### 32、Es了解吗？脑裂啥意思？如何解决 ###

```
ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。ElasticSearch用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。官方客户端在Java、.NET（C#）、PHP、Python、Apache Groovy、Ruby和许多其他语言中都是可用的。根据DB-Engines的排名显示，Elasticsearch是最受欢迎的企业搜索引擎，其次是Apache Solr，也是基于Lucene。
脑裂现象的产生
由于某些节点的失效，部分节点的网络连接会断开，并形成一个与原集群一样名字的集群，这种情况称为集群脑裂（split-brain）现象。这个问题非常危险，因为两个新形成的集群会同时索引和修改集群的数据，这个时候就会出现数据冲突了。
可能产生“脑裂”的原因？
网络原因 
内网一般不会出现此问题，可以监控内网流量状态。外网的网络出现问题的可能性大些。
节点负载 
由于master节点与data节点都是混合在一起的，所以当工作节点的负载较大（确实也较大）时，导致对应的ES实例停止响应，而这台服务器如果正充当着master节点的身份，那么一部分节点就会认为这个master节点失效了，故重新选举新的节点，这时就出现了脑裂； 
这里最好是master节点和数据节点分开。
回收内存 
由于data节点上ES进程占用的内存较大，较大规模的内存回收操作也能造成ES进程失去响应。
脑裂现象的解决办法
推测出原因应该是由于节点负载导致了master进程停止响应，继而导致了部分节点对于master的选择出现了分歧。为此，一个直观的解决方案便是将master节点与data节点分离。
可以用上面说到的配置来限制其角色：
node.master: true 
node.data: false 
其他节点设置，这样master节点和data节点就分开了
node.master: false 
node.data:  true
discovery.zen.ping_timeout（默认值是3秒，可以设置为120s）：默认情况下，一个节点会认为，如果master节点在3秒之内没有应答，那么这个节点就是死掉了，而增加这个值，会增加节点等待响应的时间，从一定程度上会减少误判。
discovery.zen.minimum_master_nodes（默认是1）：这个参数控制的是，一个节点需要看到的具有master节点资格的最小数量，然后才能在集群中做操作。官方的推荐值是(N/2)+1（向下取整），其中N是具有master资格的节点的数量。
```



### 33、为什么用Kryo序列化 ###

```
前言：kryo是个高效的java序列化/反序列化库，目前Twitter、yahoo、Apache、strom等等在使用该技术，比如Apache的spark、hive等大数据领域用的较多。
为什么使用kryo而不是其他？
因为性能足够好。比kyro更高效的序列化库就只有google的protobuf了（而且两者性能很接近），protobuf有个缺点就是要传输的每一个类的结构都要生成对应的proto文件（也可以都放在同一个proto文件中，如果考虑到扩展性的话，不建议放在一个proto文件中），如果某个类发生修改，还得重新生成该类对应的proto文件；另外考虑到项目中用的全部是java技术栈，不存在不同编程语言间的兼容性问题，因此最终采用了kryo作为序列化库。
使用场景：（数据交换或数据持久化）比如使用kryo把对象序列化成字节数组发送给消息队列或者放到redis等nosql中等等应用场景。
注意：由于kryo不是线程安全的，针对多线程情况下的使用，要对kryo进行一个简单的封装设计，从而可以多线程安全的使用序列化和反序列化
```



### 34、为什么维护offset到Redis中 ###

```
Spark Streaming消费Kafka Direct保存offset到Redis，实现数据零丢失和exactly once
一、概述
   上次写这篇文章文章的时候，Spark还是1.x，kafka还是0.8x版本，转眼间spark到了2.x，kafka也到了2.x，存储offset的方式也发生了改变，笔者根据上篇文章和网上文章，将offset存储到Redis，既保证了并发也保证了数据不丢失，经过测试，有效。

二、使用场景
Spark Streaming实时消费kafka数据的时候，程序停止或者Kafka节点挂掉会导致数据丢失，Spark Streaming也没有设置CheckPoint（据说比较鸡肋，虽然可以保存Direct方式的offset，但是可能会导致频繁写HDFS占用IO），所以每次出现问题的时候，重启程序，而程序的消费方式是Direct，所以在程序down掉的这段时间Kafka上的数据是消费不到的，虽然可以设置offset为smallest，但是会导致重复消费，重新overwrite hive上的数据，但是不允许重复消费的场景就不能这样做。

三、原理阐述
在Spark Streaming中消费 Kafka 数据的时候，有两种方式分别是 ：

1.基于 Receiver-based 的 createStream 方法。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。本文对此方式不研究，有兴趣的可以自己实现，个人不喜欢这个方式。KafkaUtils.createStream

2.Direct Approach (No Receivers) 方式的 createDirectStream 方法，但是第二种使用方式中  kafka 的 offset 是保存在 checkpoint 中的，如果程序重启的话，会丢失一部分数据，我使用的是这种方式。KafkaUtils.createDirectStream。本文将用代码说明如何将 kafka 中的 offset 保存到 Redis 中，以及如何从 Redis 中读取已存在的 offset。参数auto.offset.reset为latest的时候程序才会读取redis的offset。

四、总结
根据不同的groupid来保存不同的offset，支持多个topic

五、exactly once方案
准确的说也不是严格的方案，要根据实际的业务场景来配合。

现在的方案是保存rdd的最后一个offset，我们可以考虑在处理完一个消息之后就更新offset，保存offset和业务处理做成一个事务，当出现Exception的时候，都进行回退，或者将出现问题的offset和消息发送到另一个kafka或者保存到数据库，另行处理错误的消息。
```



### 35、es中query和filter区别，哪个能缓存。 ###

```
ES中的查询操作分为2种：查询（query）和过滤（filter）。
查询即是之前提到的query查询，它（查询）默认会计算每个返回文档的得分，然后根据得分排序。
而过滤（filter）只会筛选出符合的文档，并不计算得分，且它可以缓存文档。所以，单从性能考虑，过滤比查询更快。
所以我们在组合使用这两种查询操作的时候，我们优先考虑先过滤后查询的组合。
就是说：过滤适合在大范围筛选数据，而查询则适合精确匹配数据。所以，一般应用时，应先使用过滤操作过滤数据，然后使用查询匹配数据。
```


### 36、大数据技术栈还用过哪些 ###

![](./oimg/janson11.png)

###  37、nginx的字段都有哪些

```
1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。
2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。
3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。
4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。
5、location块：配置请求的路由，以及各种页面的处理情况。
下面给大家上一个配置文件，作为理解，同时也配入我搭建的一台测试机中，给大家示例。
	########### 每个指令必须有分号结束。#################
	#user administrator administrators; #配置用户或者组，默认为nobody nobody。
	#worker_processes 2; #允许生成的进程数，默认为1
	#pid /nginx/pid/nginx.pid;  #指定nginx进程运行文件存放地址
	error_log log/error.log debug; #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emerg
	events {
	  accept_mutex on;  #设置网路连接序列化，防止惊群现象发生，默认为on
	  multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off
	  #use epoll;   #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport
	  worker_connections 1024;  #最大连接数，默认为512
	}
	http {
	  include    mime.types;  #文件扩展名与文件类型映射表
	  default_type application/octet-stream; #默认文件类型，默认为text/plain
	  #access_log off; #取消服务日志  
	  log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式
	  access_log log/access.log myFormat; #combined为日志格式的默认值
	  sendfile on;  #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。
	  sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。
	  keepalive_timeout 65; #连接超时时间，默认为75s，可以在http，server，location块。
	 
	  upstream mysvr {  
	   server 127.0.0.1:7878;
	   server 192.168.10.121:3333 backup; #热备
	  }
	  error_page 404 https://www.baidu.com; #错误页
	  server {
		keepalive_requests 120; #单连接请求上限次数。
		listen    4545;  #监听端口
		server_name 127.0.0.1;  #监听地址    
		location ~*^.+$ {    #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。
		  #root path; #根目录
		  #index vv.txt; #设置默认页
		  proxy_pass http://mysvr; #请求转向mysvr 定义的服务器列表
		  deny 127.0.0.1; #拒绝的ip
		  allow 172.18.5.54; #允许的ip      
		} 
	  }
	｝
上面是nginx的基本配置，需要注意的有以下几点：
	1.$remote_addr 与$http_x_forwarded_for 用以记录客户端的ip地址；
	2.$remote_user ：用来记录客户端用户名称；
	3.$time_local ： 用来记录访问时间与时区；
	4.$request ： 用来记录请求的url与http协议；
	5.$status ： 用来记录请求状态；成功是200，
	6.$body_bytes_s ent ：记录发送给客户端文件主体内容大小；
	7.$http_referer ：用来记录从那个页面链接访问过来的；
	8.$http_user_agent ：记录客户端浏览器的相关信息；
```